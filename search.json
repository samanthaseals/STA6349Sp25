[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Note: this is an abbreviated syllabus. The full syllabus is available through Classmate and/or Canvas.\n\nInstructor Information\n\nDr. Samantha Seals\nOffice: 4/344\n\n\n\nMeeting Times and Location\n\nDay/time: MW 6:00-7:15 pm (Central)\nClassroom: 4/406\n\n\n\nOffice Hours\n\nMonday: 2:30–4:30 pm (Central)\nWednesday: 2:30–4:30 pm (Central)\nThursday: 10:00 am–12:00 pm (Central)\nOther times by appointment\n\n\n\nGrading and Evaluation\nThe course grade will be determined as follows:\n\nActivities (20%): Every few lectures, we will use the class time to work on an activity reflecting homework question(s) from the learning module. The resulting .html file will be submitted to the appropriate dropbox on Canvas. To allow for flexibility with life/work scheduling, activities will be due the following Sunday at 11:59 pm (Central).\nProjects - 2 (25% each): Both projects will be completed using R and Quarto. Projects will be presented orally on the assigned date(s). Students are expected to be present for and participate in their group’s presentation, but to allow for flexibility with life/work scheduling, students may watch the other groups’ presentations through the class recording. Evaluations of other groups’ projects will be due the following Sunday at 11:59 pm (Central).\nFinal Exam (30%): The final exam will be a written, closed book, proctored exam focused on your conceptual understanding of the course. While there may be some basic calculations needed, you will not be processing raw data on the proctored portion of the final exam or dealing with R code in any form. A scientific (not graphing) calculator will be allowed and is recommended. The proctored final exam will be on Monday, April 28, 2025. Exams may not be taken late. Students will have 2 hours and 30 minutes to complete the exam.\n\nFor those local to the Pensacola area, you may opt to come to the classroom for your final exam. When MathStat Proctoring reaches out, please reply: let them know that you will be testing with me on campus and cc me on your response. Our exam is 6:00-8:30 pm in Building 4, Room 404.\nFor those requiring external proctoring, you must follow guidelines from MathStat Proctoring. You will receive an email from Proctoring the first week of the semester. Due to the number of students in our graduate programs and form processing time, you must complete the form by the deadline specified by Proctoring. Otherwise, you will receive a 0 on the exam.\n\n\n\n\nLate Policy\nAssigments have due dates, however, the dropboxes will not close until the end of the semester. All students are automatically granted “extensions” without question.\nNote that if there is not a submission when I go to grade (after the initial deadline), I will assign a zero (0) and request that you submit the assignment when you are able to. This is only for record keeping purposes. There is no penalty for submitting late and a full grade will be given upon review of your submission.\nExtensions are not available for the projects or final exam."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#introduction",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#introduction",
    "title": "Introduction to Probability",
    "section": "Introduction",
    "text": "Introduction\n\nThe first few lectures come from Mathematical Statistics with Applications, by Wackerly.\n\nWe must understand the underlying probability and random variable theory before moving into the Bayesian world.\n\nWe will be covering the following chapters:\n\nChapter 2: probability theory\nChapter 3: discrete random variables\nChapter 4: continuous random variables"
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#conditional-prob.-and-independence-of-events",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#conditional-prob.-and-independence-of-events",
    "title": "Introduction to Probability",
    "section": "2.7: Conditional Prob. and Independence of Events",
    "text": "2.7: Conditional Prob. and Independence of Events\n\nConditional probability of an event A given that an event B has occurred is as follows\n\n\nP[A|B] = \\frac{P[A \\cap B]}{P[B]},\n\n\nso long as P[B] &gt; 0.\nNotation: P[A|B] is the probability of A given B."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#conditional-prob.-and-independence-of-events-1",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#conditional-prob.-and-independence-of-events-1",
    "title": "Introduction to Probability",
    "section": "2.7: Conditional Prob. and Independence of Events",
    "text": "2.7: Conditional Prob. and Independence of Events\n\nSuppose that a balanced die is tossed once. Find the probability of rolling a 1, given that an odd number was obtained."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#conditional-prob.-and-independence-of-events-2",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#conditional-prob.-and-independence-of-events-2",
    "title": "Introduction to Probability",
    "section": "2.7: Conditional Prob. and Independence of Events",
    "text": "2.7: Conditional Prob. and Independence of Events\n\nTwo events A and B are said to be independent events if any one of the following holds:\n\n\n\\begin{align*}\nP[A|B] &= P[A] \\\\\nP[B|A] &= P[B] \\\\\nP[A \\cap B] &= P[A] P[B]\n\\end{align*}\n - Otherwise, we say that A and B are dependent events."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#conditional-prob.-and-independence-of-events-3",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#conditional-prob.-and-independence-of-events-3",
    "title": "Introduction to Probability",
    "section": "2.7: Conditional Prob. and Independence of Events",
    "text": "2.7: Conditional Prob. and Independence of Events\n\nConsider the following events in the toss of a single die:\n\nA: Observe an odd number.\nB: Observe an even number.\nC: Observe a 1 or 2.\n\nAre A and B independent events? \nAre A and C independent events?"
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#two-laws-of-probability",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#two-laws-of-probability",
    "title": "Introduction to Probability",
    "section": "2.8: Two Laws of Probability",
    "text": "2.8: Two Laws of Probability\n\nTheorem: The Multiplicative Law of Probability\n\nThe probability of the intersection of two events A and B is\n\n\n\\begin{align*}P[A\\cap B] &= P[A] P[B|A] \\\\ &= P[B] P[A|B]\\end{align*}\n\nNote that if A and B are independent, then\n\nP[A \\cap B] = P[A] P[B]"
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#two-laws-of-probability-1",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#two-laws-of-probability-1",
    "title": "Introduction to Probability",
    "section": "2.8: Two Laws of Probability",
    "text": "2.8: Two Laws of Probability\n\nTheorem: The Additive Law of Probability\n\nThe probability of the union of two events A and B is\n\n\nP[A \\cup B] = P[A] + P[B] - P[A \\cap B] \n\nNote that if A and B are mutually exclusive, then P[A \\cap B] = 0 and\n\nP[A \\cup B] = P[A] + P[B]"
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#two-laws-of-probability-2",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#two-laws-of-probability-2",
    "title": "Introduction to Probability",
    "section": "2.8: Two Laws of Probability",
    "text": "2.8: Two Laws of Probability\n\nTheorem: The Complement Rule\n\nIf A is an event, then\n\n\n\n\\begin{align*}\nP[A] &= 1 - P[\\bar{A}] \\\\\nP[\\bar{A}] &= 1 - P[A] \\\\\n1 &= P[A] + P[\\bar{A}]\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#two-laws-of-probability-3",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#two-laws-of-probability-3",
    "title": "Introduction to Probability",
    "section": "2.8: Two Laws of Probability",
    "text": "2.8: Two Laws of Probability\n\nSuppose A_1, A_2, and A_3 are three events and P[A_1 \\cap A_2] = P[A_1 \\cap A_3] \\ne 0 but P[A_2 \\cap A_3] = 0. Show that\n\n\nP[\\text{at least one } A_i] = P[A_1] + P[A_2] + P[A_3] - 2 P[A_1 \\cap A_2]."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#calculating-the-probability-of-an-event",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#calculating-the-probability-of-an-event",
    "title": "Introduction to Probability",
    "section": "2.9: Calculating the Probability of an Event",
    "text": "2.9: Calculating the Probability of an Event\n\nThe steps used to define the probability of an event:\n\nDefine the experiment.\nVisualize the nature of the sample points. Identify a few to clarify your thinking.\nWrite an equation expressing the event of interest (A) as a composition of two or more events, using usions, intersections, and/or complements. Make certain that event A and the event implied by the compsotion represnt the sameset of sample points.\nApply the additive and multiplicative laws of probability in the compositions obtained in step 3 to find P[A]."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#calculating-the-probability-of-an-event-1",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#calculating-the-probability-of-an-event-1",
    "title": "Introduction to Probability",
    "section": "2.9: Calculating the Probability of an Event",
    "text": "2.9: Calculating the Probability of an Event\n\nIt is known that a patient with a disease with respond to treatment with probability equal to 0.9. If three patients with the disease are treated independently, find the probability that at least one will respond."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#the-law-of-total-probability-and-bayes-rule",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#the-law-of-total-probability-and-bayes-rule",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes’ Rule",
    "text": "2.10: The Law of Total Probability and Bayes’ Rule\n\nPartition:\n\nFor some positive integer k, let the sets B_1, B_2, ..., B_k be such that\n\nS = B_1 \\cup B_2 \\cup ... \\cup B_k\nB_1 \\cap B_j = \\emptyset, for i \\ne j\n\nThen the collection of sets \\{B_1, B_2, ..., B_k\\} is said to be a partition of S.\n\nWe also know that if A is any subset of S and \\{B_1, B_2, ..., B_k\\} is a partition of S, A can be decomposed:\n\n\nA = (A \\cap B_1) \\cup (A \\cap B_2) \\cup \\ ... \\ \\cup (A \\cap B_k)"
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#the-law-of-total-probability-and-bayes-rule-1",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#the-law-of-total-probability-and-bayes-rule-1",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes’ Rule",
    "text": "2.10: The Law of Total Probability and Bayes’ Rule\n\nTheorem:\n\nAssume that \\{ B_1, B_2, ..., B_k \\} is a partition of S such that P[B_i] &gt; 0 for i = 1, 2, ..., k. Then for any event A,\n\n\n\nP[A] = \\sum_{i=1}^k P[A|B_i] P[B_i]\n\n\nTheorem: Bayes’ Rule\n\nAssume that \\{ B_1, B_2, ..., B_k \\} is a partition of S such that P[B_i] &gt; 0 for i = 1, 2, ..., k. Then\n\n\n\nP[B_j | A] = \\frac{P[A|B_j] P[B_j]}{\\sum_{i=1}^k P[A|B_i] P[B_i]}"
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#the-law-of-total-probability-and-bayes-rule-2",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#the-law-of-total-probability-and-bayes-rule-2",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes’ Rule",
    "text": "2.10: The Law of Total Probability and Bayes’ Rule\n\nAn electronic fuse is produced by five production lines in a manufacturing operation. The fuses are costly, are quite reliable, and are shipped to suppliers in 100-unit lots. Because testing is destructive, most buyers of the fuses test only a small number of fuses before deciding to accept or reject lots of incoming fuses. All five production lines produce fuses at the same rate and normally produce only 2% defective fuses, which are dispersed randomly in the output. Unfortunately, production line 1 suffered mechanical difficulty and produced 5% defectives during the month of March. This situation became known to the manufacturer after the fuses had been shipped.\nA customer received a lot produced in March and tested three fuses. One failed.\n\nWhat is the probability that the lot was produced on line 1? \nWhat is the probability that the lot came from one of the four other lines?"
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#the-law-of-total-probability-and-bayes-rule-3",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#the-law-of-total-probability-and-bayes-rule-3",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes’ Rule",
    "text": "2.10: The Law of Total Probability and Bayes’ Rule\n\nOf the travelers arriving at a small airport, 60% fly on major airlines, 30% fly on privately owned planes, and the remainder fly on commercially owned planes not belonging to a major airline. Of those traveling on major airlines, 50% are traveling for business reasons, whereas 60% of those arriving on private planes and 90% of those arriving on other commercially owned planes are traveling for business reasons.\nSuppose that we randomly select one person arriving at this airport. What is the probability that the person:\n\nis traveling on business? \nis traveling for business on a privately owned plane? \narrived on a privately owned plane, given that the person is traveling for business reasons? \nis traveling on business, given that the person is flying on a commercially owned plane?"
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#numerical-events-and-random-variables",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#numerical-events-and-random-variables",
    "title": "Introduction to Probability",
    "section": "2.11: Numerical Events and Random Variables",
    "text": "2.11: Numerical Events and Random Variables\n\nRandom variable:\n\nA real-valued function for which the domain is a sample space.\n\nLet Y denote a variable to be measured in an experiment.\n\nBecause the value of Y will vary depending on the outcome of the experiment, it is called a random variable.\nEach point in the sample space will be assigned a real number denoting the value of Y.\n\nThat is, it may vary from one sample point to another."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#numerical-events-and-random-variables-1",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#numerical-events-and-random-variables-1",
    "title": "Introduction to Probability",
    "section": "2.11: Numerical Events and Random Variables",
    "text": "2.11: Numerical Events and Random Variables\n\nDefine an experiment as tossing two coins and observing the results.\nLet Y equal the number of heads obtained.\n\nIdentify the sample points in S. \nAssign a value of Y to each sample point \nIdentify the sample points associated with each value of the random variable Y. \nCompute the probabilities for each value of Y."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#random-sampling",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#random-sampling",
    "title": "Introduction to Probability",
    "section": "2.12: Random Sampling",
    "text": "2.12: Random Sampling\n\nPopulation: collection of all elements of interest.\n\nParameter: numeric characteristic of the population\n\nSample: subset of the population.\n\nStatistic: numeric characteristic of the sample\n\nThe method of sampling will affect the probability of a particular sample outcome.\n\nSimple random sample.\nStratified random sample.\nCluster sample.\nSystematic sample."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#random-sampling-1",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#random-sampling-1",
    "title": "Introduction to Probability",
    "section": "2.12: Random Sampling",
    "text": "2.12: Random Sampling"
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#random-sampling-2",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#random-sampling-2",
    "title": "Introduction to Probability",
    "section": "2.12: Random Sampling",
    "text": "2.12: Random Sampling\n\nSampling with replacement: elements can be chosen more than once for inclusion in a sample.\n\nThis means every time we select an element for the sample, the possible choices from the population stay the same.\n\nSampling without replacement: elements cannot be chosen more than once for inclusion in a sample.\n\nThis means every time we select an element for the sample, the possible choices from the population decrease.\n\nRandom sample: each of the {N \\choose n} possible samples have equal probability of being selected.\n\nN is the population size\nn is the sample size\n\nIf we need to randomize, we will use a random number generator to assign a random number, then reorder the dataset and take the first n observations."
  },
  {
    "objectID": "files/lecture/W02-L2-intro-prob-pt2.html#homework",
    "href": "files/lecture/W02-L2-intro-prob-pt2.html#homework",
    "title": "Introduction to Probability",
    "section": "Homework",
    "text": "Homework\n\n2.73\n2.77\n2.94\n2.106\n2.107\n2.114\n2.120\n2.128\n2.140\n2.141"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#introduction",
    "href": "files/lecture/W05-L1-discrete-rv.html#introduction",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "Introduction",
    "text": "Introduction\n\nThe first few lectures come from Mathematical Statistics with Applications, by Wackerly.\n\nWe must understand the underlying probability and random variable theory before moving into the Bayesian world.\n\nWe will be covering the following chapters:\n\nChapter 2: probability theory\nChapter 3: discrete random variables\nChapter 4: continuous random variables"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#basic-definitions",
    "href": "files/lecture/W05-L1-discrete-rv.html#basic-definitions",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.1: Basic Definitions",
    "text": "3.1: Basic Definitions\n\nDiscrete random variable: a variable that can assume only a finite or countably infinite number of distinct values.\nProbability distribution of a random variable: collection of probabilities for each value of the random variable.\nNotation:\n\nUppercase letter (e.g., Y) denotes a random variable.\nLowercase letter (e.g., y) denotes a particular value that the random variable may assume.\n\nThe specific observed value, y, is not random."
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#probability-distributions-for-discrete-rv",
    "href": "files/lecture/W05-L1-discrete-rv.html#probability-distributions-for-discrete-rv",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.2: Probability Distributions for Discrete RV",
    "text": "3.2: Probability Distributions for Discrete RV\n\nprobability function for Y: sum of the the probabilities of all sample points in S that are assigned the value y\n\nP[Y = y] = p(y): the probability that Y takes on the value y.\n\nprobability distribution for Y: a formula, table, or graph that provides p(y) = P[Y = y] \\forall y.\nTheorem:\n\nFor any discrete probability distribution, the following must be true:\n\n0 \\le p(y) \\le 1 \\  \\forall \\ y\n\\sum_y p(y) = 1 \\ \\forall \\ p(y) &gt; 0."
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#probability-distributions-for-discrete-rv-1",
    "href": "files/lecture/W05-L1-discrete-rv.html#probability-distributions-for-discrete-rv-1",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.2: Probability Distributions for Discrete RV",
    "text": "3.2: Probability Distributions for Discrete RV\n\nA supervisor in a manufacturing plant has three men and three women working for them. The supervisor wants to choose two workers for a special job. Not wishing to show any biases in their selection, they decides to select the two workers at random.\nLet Y denote the number of women in his selection. Find the probability distribution for Y."
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#probability-distributions-for-discrete-rv-2",
    "href": "files/lecture/W05-L1-discrete-rv.html#probability-distributions-for-discrete-rv-2",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.2: Probability Distributions for Discrete RV",
    "text": "3.2: Probability Distributions for Discrete RV\n\nWhen the health department tested private wells in a county for two impurities commonly found in drinking water, it found that:\n\n20% of the wells had neither impurity,\n40% had impurity A, and\n50% had impurity B.\n\nIf a well is randomly chosen from those in the county, find the probability distribution for Y, the number of impurities found in the well.\n\nHint: some wells had both impurities…"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#expected-values",
    "href": "files/lecture/W05-L1-discrete-rv.html#expected-values",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nExpected value: Let Y be a discrete random variable with the probability function, p(y). Then, the expected value of Y, E[Y], is defined to be\n\n\nE(Y) = \\sum_{y} y p(y)\n\n\nWhen p(y) is an accurate characterization of the population frequency distribution, then the expected value is the population mean.\n\n\nE[Y] = \\mu\n\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and g(Y) be a real-valued function of Y (i.e., a transformed variable). Then the expected value of g(Y) is given by\n\n\n\nE[g(Y)] = \\sum_{y} g(y) p(y)"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#expected-values-1",
    "href": "files/lecture/W05-L1-discrete-rv.html#expected-values-1",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nVariance: if Y is a random variable with mean E[Y] = \\mu, the variance of a random variable Y is defined to be the expected value of (Y-\\mu)^2.\n\n\nV[Y] = E\\left[ (Y-\\mu)^2 \\right]\n\n\nIf p(y) is an accurate characterization of the population frequency distribution, then V(Y) is the population variance,\n\n\nV[Y] = \\sigma^2\n\n\nStandard deviation: the positive square root of V[Y]."
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#expected-values-2",
    "href": "files/lecture/W05-L1-discrete-rv.html#expected-values-2",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nThe probability distribution for a random variable Y is given below.\n\n\n\nFind the mean, variance, and standard deviation of Y."
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#expected-values-3",
    "href": "files/lecture/W05-L1-discrete-rv.html#expected-values-3",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and c be a constant. Then,\n\n\nE(c) = c\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y), g(Y) be a function of Y, and c be a constant. Then,\n\n\nE[cg(Y)] = cE[g(Y)]\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y), and g_1(Y), g_2(Y), ..., g_k(Y) be k functions of Y. Then,\n\n\nE[g_1(Y) + g_2(Y) + ... + g_k(Y)] = E[g_1(Y)] + E[g_2(Y)] + ... + E[g_k(Y)]"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#expected-values-4",
    "href": "files/lecture/W05-L1-discrete-rv.html#expected-values-4",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nPutting the previous theorems into one:\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and mean E[Y] = \\mu. Then,\n\n\nV[Y] = \\sigma^2 = E\\left[(Y-\\mu)^2\\right] = E\\left[Y^2\\right] - \\mu^2"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#expected-values-5",
    "href": "files/lecture/W05-L1-discrete-rv.html#expected-values-5",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nThe probability distribution for a random variable Y is given below.\n\n\n\nUse the previous theorem to find V[Y] and compare to our previous answer.\n\nRecall that \\mu=1.75."
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#expected-values-6",
    "href": "files/lecture/W05-L1-discrete-rv.html#expected-values-6",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nLet Y be a random variable with p(y) in the table below.\n\n\n\nFind\n\nE[Y] \nE[1/Y] \nE\\left[Y^2-1\\right] \nV[Y]"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#expected-values-7",
    "href": "files/lecture/W05-L1-discrete-rv.html#expected-values-7",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nE[Y] \nE[1/Y] \nE\\left[Y^2-1\\right] \nV[Y]"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution",
    "href": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\n\nBinomial experiment:\n\nThe experiment consists of a fixed number, n, of identical trials.\nEach trial results in one of two outcomes: success (S) or failure (F).\nThe probability of success on a single trial is equal to some value p and remains the same from trial to trial.\n\nThe probability of failure is equal to q = (1-p).\n\nThe trials are independent.\nThe random variable of interest is Y, the number of successes observed during the n trials."
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-1",
    "href": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-1",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\n\nBinomial Distribution\n\nA random variable Y is said to have a binomial distribution based on n trials with success probability p iff\n\n\n\np(y) = {n \\choose y}p^y q^{n-y}, \\text{ where } y = 0, 1, 2, ..., n, \\text{ and } 0 \\le p \\le1\n\n\nTheorem:\n\nLet Y be a binomial random variable based on n trials and success probability p. Then\n\n\n\nE[Y] = \\mu = np \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = npq\n\n\nSee Wackerly pg. 107 for derivation."
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-2",
    "href": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-2",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\np=0.10"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-3",
    "href": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-3",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\np=0.25"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-4",
    "href": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-4",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\np=0.50"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-5",
    "href": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-5",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\np=0.75"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-6",
    "href": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-6",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\np=0.90"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-7",
    "href": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-7",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\n\nWhat do you notice when comparing distributions under p vs. 1-p?"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-8",
    "href": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-8",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\n\nWhat do you notice as n increases?"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-9",
    "href": "files/lecture/W05-L1-discrete-rv.html#binomial-probability-distribution-9",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\n\nThe manufacturer of a low-calorie dairy drink wishes to compare the taste appeal of a new formula (formula B) with that of the standard formula (formula A). Each of four judges is given three glasses in random order, two containing formula A and the other containing formula B. Each judge is asked to state which glass he or she most enjoyed. Suppose that the two formulas are equally attractive. Let Y be the number of judges stating a preference for the new formula.\n\nFind the probability function for Y. \nWhat is the probability that at least three of the four judges state a preference for the new formula?\nFind the expected value of Y.\nFind the variance of Y."
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#poisson-probability-distribution",
    "href": "files/lecture/W05-L1-discrete-rv.html#poisson-probability-distribution",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.8: Poisson Probability Distribution",
    "text": "3.8: Poisson Probability Distribution\n\nThe Poisson probability distribution often provides a good model for the probability distribution of the number Y of rare events that occur in space, time, volume, or any other dimension.\nPoisson Distribution:\n\nA random variable Y is said to have a Poisson probability distribution iff\n\n\n\np(y) = \\frac{\\lambda^y}{y!}e^{-\\lambda}, \\text{ where } y=0,1,2,..., \\text{ and } \\lambda &gt; 0\n\n\nTheorem\n\nIf Y is a random variable with a Poisson distribution with parameter \\lambda, then\n\n\n\nE[Y] = \\mu = \\lambda \\text{ and } V[Y] = \\sigma^2 = \\lambda\n\n\nSee Wackerly pg. 134 for derivation."
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#poisson-probability-distribution-1",
    "href": "files/lecture/W05-L1-discrete-rv.html#poisson-probability-distribution-1",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "3.8: Poisson Probability Distribution",
    "text": "3.8: Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. During a given hour, what are the probabilities that\n\nno more than three customers arrive? \nat least two customers arrive? \nexactly five customers arrive?"
  },
  {
    "objectID": "files/lecture/W05-L1-discrete-rv.html#homework",
    "href": "files/lecture/W05-L1-discrete-rv.html#homework",
    "title": "Discrete Random Variables and Their Probability Distributions",
    "section": "Homework",
    "text": "Homework\n\n3.6\n3.10\n3.15\n3.22\n3.34\n3.60\n3.128\n3.129\n3.136"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#introduction",
    "href": "files/lecture/W08-L1-beta-binomial.html#introduction",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Introduction",
    "text": "Introduction\n\nLast week, we talked about creating posterior models for discrete priors (non-named distributions).\nThis week, we will now introduce having a named distribution as a prior.\nWe will start with analyzing a binomial outcome.\n\nRecall that the binomial distribution depends on \\pi."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example",
    "href": "files/lecture/W08-L1-beta-binomial.html#example",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nConsider the following scenario.\n\n“Michelle” has decided to run for president and you’re her campaign manager for the state of Florida.\nAs such, you’ve conducted 30 different polls throughout the election season.\nThough Michelle’s support has hovered around 45%, she polled at around 35% in the dreariest days and around 55% in the best days on the campaign trail."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-1",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-1",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nPast polls provide prior information about \\pi, the proportion of Floridians that currently support Michelle.\n\nIn fact, we can reorganize this information into a formal prior probability model of \\pi.\n\nIn a previous problem, we assumed that \\pi could only be 0.2, 0.5, or 0.8, the corresponding chances of which were defined by a discrete probability model.\n\nHowever, in the reality of Michelle’s election support, \\pi \\in [0, 1].\n\nWe can reflect this reality and conduct a Bayesian analysis by constructing a continuous prior probability model of \\pi."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-2",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-2",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\n\n\n\nA reasonable prior is represented by the curve on the right.\n\nNotice that this curve preserves the overall information and variability in the past polls, i.e., Michelle’s support, \\pi can be anywhere between 0 and 1, but is most likely around 0.45."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-3",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-3",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nIncorporating this more nuanced, continuous view of Michelle’s support, \\pi, will require some new tools.\n\nNo matter if our parameter \\pi is continuous or discrete, the posterior model of \\pi will combine insights from the prior and data.\n\\pi isn’t the only variable of interest that lives on [0,1].\n\nMaybe we’re interested in modeling the proportion of people that use public transit, the proportion of trains that are delayed, the proportion of people that prefer cats to dogs, etc.\n\nThe Beta-Binomial model provides the tools we need to study the proportion of interest, \\pi, in each of these settings."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#beta-prior",
    "href": "files/lecture/W08-L1-beta-binomial.html#beta-prior",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\n\n\n\nIn building the Bayesian election model of Michelle’s election support among Floridians, \\pi, we begin with the prior.\n\nOur continuous prior probability model of \\pi is specified by the probability density function (pdf).\n\nWhat values can \\pi take and which are more plausible than others?"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#beta-prior-1",
    "href": "files/lecture/W08-L1-beta-binomial.html#beta-prior-1",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\nLet \\pi be a random variable, where \\pi \\in [0, 1].\nThe variability in \\pi may be captured by a Beta model with shape hyperparameters \\alpha &gt; 0 and \\beta &gt; 0,\n\nhyperparameter: a parameter used in a prior model.\n\n\n \\pi \\sim \\text{Beta}(\\alpha, \\beta), \n\nLet’s explore the shape of the Beta:\n\n\nlibrary(bayesrules)\nlibrary(tidyverse)\n\nplot_beta(1, 5) + theme_bw()\nplot_beta(1, 2) + theme_bw()\nplot_beta(3, 7) + theme_bw()\nplot_beta(1, 1) + theme_bw()"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#beta-prior-2",
    "href": "files/lecture/W08-L1-beta-binomial.html#beta-prior-2",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\n\nplot_beta(1, 5) + theme_bw() + ggtitle(\"Beta(1, 5)\")"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#beta-prior-3",
    "href": "files/lecture/W08-L1-beta-binomial.html#beta-prior-3",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\n\nplot_beta(1, 2) + theme_bw() + ggtitle(\"Beta(1, 2)\")"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#beta-prior-4",
    "href": "files/lecture/W08-L1-beta-binomial.html#beta-prior-4",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\n\nplot_beta(3, 7) + theme_bw() + ggtitle(\"Beta(3, 7)\")"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#beta-prior-5",
    "href": "files/lecture/W08-L1-beta-binomial.html#beta-prior-5",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\n\nplot_beta(1, 1) + theme_bw() + ggtitle(\"Beta(1, 1)\")"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#beta-prior-6",
    "href": "files/lecture/W08-L1-beta-binomial.html#beta-prior-6",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\nYour turn!\nExplore the following and report back:\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha=\\beta?\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha&gt;\\beta?\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha&lt;\\beta?\nFor which model is there greater variability in the plausible values of \\pi, Beta(20, 20) or Beta(5, 5)?"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior",
    "href": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nWe can tune the shape hyperparameters (\\alpha and \\beta) to reflect our prior information about Michelle’s election support, \\pi.\nIn our example, we saw that she polled between 25 and 65 percentage points, with an average of 45 percentage points.\n\nWe want our Beta(\\alpha, \\beta) to have similar patterns.\nWe want to pick \\alpha and \\beta such that \\pi is around 0.45.\n\n\n\nE[\\pi] = \\frac{\\alpha}{\\alpha+\\beta} \\approx 0.45\n\n\nUsing algebra, we can tune, and find\n\n\\alpha \\approx \\frac{9}{11} \\beta"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior-1",
    "href": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior-1",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nYour turn!\n\nGraph the following and determine which is best for the example.\n\n\n\nplot_beta(9, 11) + theme_bw()\nplot_beta(27, 33) + theme_bw()\nplot_beta(45, 55) + theme_bw()\n\n\nRecall, this is what we are going for:"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior-2",
    "href": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior-2",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(9, 11) + theme_bw() + ggtitle(\"Beta(9, 11)\")"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior-3",
    "href": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior-3",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(27, 33) + theme_bw() + ggtitle(\"Beta(27, 33)\")"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior-4",
    "href": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior-4",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(45, 55) + theme_bw() + ggtitle(\"Beta(45, 55)\")"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior-5",
    "href": "files/lecture/W08-L1-beta-binomial.html#tuning-the-beta-prior-5",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nNow that we have a prior, we “know” some things.\n\n\\pi \\sim \\text{Beta}(45, 55)\n\nFrom the properties of the beta distribution,\n\n\n\\begin{equation*}\n\\begin{aligned}\nE[\\pi] &= \\frac{\\alpha}{\\alpha + \\beta} & \\text{ and } & \\text{ } & \\text{ }  \\\\\n&=\\frac{45}{45+55} \\\\\n&= 0.45\n\\end{aligned}\n\\begin{aligned}\n\\text{var}[\\pi] &= \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} \\\\\n&= \\frac{(45)(55)}{(45+55)^2(45+55+1)} \\\\\n&= 0.0025\n\\end{aligned}\n\\end{equation*}"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function\n\nNow we are ready to think about the data collection.\nA new poll of n = 50 Floridians recorded Y, the number that support Michelle.\n\nThe results depend upon \\pi – the greater Michelle’s actual support, the greater Y will tend to be.\n\nTo model the dependence of Y on \\pi, we assume\n\nvoters answer the poll independently of one another;\nthe probability that any polled voter supports your candidate Michelle is \\pi\n\nThis is a binomial event, Y|\\pi \\sim \\text{Bin}(50, \\pi), with conditional pmf, f(y|\\pi) defined for y \\in \\{0, 1, ..., 50\\}\n\nf(y|\\pi) = P[Y = y|\\pi] = {50 \\choose y} \\pi^y (1-\\pi)^{50-y}"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-1",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-1",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function\n\nThe conditional pmf, f(y|\\pi), gives us answers to a hypothetical question:\n\nIf Michelle’s support were given some value of \\pi, then how many of the 50 polled voters (Y=y) might we expect to suppport her?\n\nLet’s look at this graphically:\n\n\nn &lt;- 50\npi &lt;- value of pi\n\nbinom_prob &lt;- tibble(n_success = 1:n,\n                     prob = dbinom(n_success, size=n, prob=pi))\n\nbinom_prob %&gt;%\n  ggplot(aes(x=n_success,y=prob))+\n  geom_col(width=0.2)+\n  labs(x= \"Number of Successes\",\n       y= \"Probability\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-2",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-2",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-3",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-3",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-4",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-4",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-5",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-5",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function\n\nIt is observed that Y=30 of the n=50 polled voters support Michelle.\nWe now want to find the likelihood function – remember that we treat Y=30 as the observed data and \\pi as unknown,\n\n\n\\begin{align*}\nf(y|\\pi) &= {50 \\choose y} \\pi^y (1-\\pi)^{50-y} \\\\\nL(\\pi|y=30) &= {50 \\choose 30} \\pi^{30} (1-\\pi)^{20}\n\\end{align*}\n\n\nThis is valid for \\pi \\in [0, 1]."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-6",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-6",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function\n\nWhat is the likelihood of 30/50 voters supporting Michelle?\n\n\ndbinom(30, 50, pi)\n\n\nYou try this for \\pi = \\{0.25, 0.50, 0.75\\}."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-7",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-7",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function\n\nWhat is the likelihood of 30/50 voters supporting Michelle?\n\n\ndbinom(30, 50, pi)\n\n\nFor \\pi = 0.25,\n\n\ndbinom(30, 50, 0.25)\n\n[1] 1.29633e-07\n\n\n\nFor \\pi = 0.5,\n\n\ndbinom(30, 50, 0.5)\n\n[1] 0.04185915\n\n\n\nFor \\pi = 0.75,\n\n\ndbinom(30, 50, 0.75)\n\n[1] 0.007654701"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-8",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-8",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function\n\nChallenge!\nCreate a graph showing what happens to the likelihood for different values of \\pi.\n\ni.e., have \\pi on the x-axis and likelihood on the y-axis.\n\nTo get you started,\n\n\ngraph &lt;- tibble(pi = seq(0, 1, 0.001)) %&gt;%\n  mutate(likelihood = dbinom(30, 50, pi))"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-9",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-9",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function\n\nCreate a graph showing what happens to the likelihood for different values of \\pi.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere is the maximum?"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-10",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-binomial-data-model-and-likelihood-function-10",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Binomial Data Model and Likelihood Function",
    "text": "The Binomial Data Model and Likelihood Function\n\nWhere is the maximum?"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-beta-posterior-model",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-beta-posterior-model",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(50, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(45, 55)\n\\end{align*}\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prior is a bit more pessimistic about Michelle’s election support than the data obtained from the latest poll."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-beta-posterior-model-1",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-beta-posterior-model-1",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nLet’s graph the posterior,\n\n\n\nplot_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nWe can see that the posterior model of \\pi is continuous and \\in [0, 1].\nThe shape of the posterior appears to also have a Beta(\\alpha, \\beta) model.\n\nThe shape parameters (\\alpha and \\beta) have been updated."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-beta-posterior-model-2",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-beta-posterior-model-2",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nIf we were to collect more information about Michelle’s support, we would use the current posterior as the new prior, then update our posterior.\n\nHow do we know what the updated parameters are?\n\n\n\nsummarize_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50)\n\n      model alpha beta mean      mode         var         sd\n1     prior    45   55 0.45 0.4489796 0.002450495 0.04950248\n2 posterior    75   75 0.50 0.5000000 0.001655629 0.04068942"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-beta-binomial-model",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-beta-binomial-model",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Beta-Binomial Model",
    "text": "The Beta-Binomial Model\n\nWe used Michelle’s election support to understand the Beta-Binomial model.\nLet’s now generalize it for any appropriate situation.\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n\\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n\\end{align*}\n\n\nWe can see that the posterior distribution reveals the influence of the prior (\\alpha and \\beta) and data (y and n)."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-beta-binomial-model-1",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-beta-binomial-model-1",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Beta-Binomial Model",
    "text": "The Beta-Binomial Model\n\nUnder this updated distribution,\n\n\n\\pi | (Y=y) \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n\n\nwe have updated moments:\n\n\n\\begin{align*}\nE[\\pi | Y = y] &= \\frac{\\alpha + y}{\\alpha + \\beta + n} \\\\\n\\text{Var}[\\pi|Y=y] &= \\frac{(\\alpha+y)(\\beta+n-y)}{(\\alpha+\\beta+n)^2(\\alpha+\\beta+1)}\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-beta-binomial-model-2",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-beta-binomial-model-2",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Beta-Binomial Model",
    "text": "The Beta-Binomial Model\n\nLet’s pause and think about this from a theoretical standpoint.\nThe Beta distribution is a conjugate prior for the likelihood.\n\nConjugate prior: the posterior is from the same model family as the prior.\n\nRecall the Beta prior, f(\\pi),\n\n L(\\pi|y) = {n \\choose y} \\pi^y (1-\\pi)^{n-y} \n\nand the likelihood function, L(\\pi|y).\n\n f(\\pi) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1}(1-\\alpha)^{\\beta-1}"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#the-beta-binomial-model-3",
    "href": "files/lecture/W08-L1-beta-binomial.html#the-beta-binomial-model-3",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "The Beta-Binomial Model",
    "text": "The Beta-Binomial Model\n\nWe can put the prior and likelihood together to create the posterior,\n\n\n\\begin{align*}\nf(\\pi|y) &\\propto f(\\pi)L(\\pi|y) \\\\\n&= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\times {n \\choose y} \\pi^y (1-\\pi)^{n-1} \\\\\n&\\propto \\pi^{(\\alpha+y)-1} (1-\\pi)^{(\\beta+n-y)-1}\n\\end{align*}\n\n\nThis is the same structure as the normalized Beta(\\alpha+y, \\beta+n-y),\n\nf(\\pi|y) = \\frac{\\Gamma(\\alpha+\\beta+n)}{\\Gamma(\\alpha+y) \\Gamma(\\beta+n-y)} \\pi^{(\\alpha+y)-1} (1-\\pi)^{(\\beta+n-y)-1}"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-4",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-4",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nIn a 1963 issue of The Journal of Abnormal and Social Psychology, Stanley Milgram described a study in which he investigated the propensity of people to obey orders from authority figures, even when those orders may harm other people (Milgram 1963).\nStudy participants were given the task of testing another participant (who was a trained actor) on their ability to memorize facts.\nIf the actor didn’t remember a fact, the participant was ordered to administer a shock on the actor and to increase the shock level with every subsequent failure.\nUnbeknownst to the participant, the shocks were fake and the actor was only pretending to register pain from the shock."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-5",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-5",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nThe parameter of interest here is \\pi, the chance that a person would obey authority (in this case, administering the most severe shock), even if it meant bringing harm to others.\n\nSince Milgram passed away in 1984, we don’t have the opportunity to ask him about his understanding of \\pi prior to conducting the study.\nSuppose another psychologist helped carry out this work. Prior to collecting data, they indicated that a Beta(1,10) model accurately reflected their understanding about \\pi.\n\nThe outcome of interest is Y, the number of the n=40 study participants that would inflict the most severe shock.\nWhat model is appropriate?"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-6",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-6",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nWhat model is appropriate?\nAssuming that each participant behaves independently of the others, we can model the dependence of Y on \\pi using the Binomial.\nThus, we have a Beta-Binomial Bayesian model.\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(40, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(1, 10)\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-7",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-7",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nWhat do you think this prior reveals about the psychologist’s prior understanding of \\pi?\n\n\nplot_beta(alpha = 1, beta = 10)\n\n\nThey don’t have an informed opinion.\nThey’re fairly certain that a large proportion of people will do what authority tells them.\nThey’re fairly certain that only a small proportion of people will do what authority tells them."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-8",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-8",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nWhat do you think this prior reveals about the psychologist’s prior understanding of \\pi?\n\n\n\n\n\n\n\n\n\n\n\n\nc. They’re fairly certain that only a small proportion of people will do what authority tells them."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-9",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-9",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nAfter data collection, Y = 26 of the n=40 study participants inflected what they understood to be the maximum shock.\nFrom the problem set up,\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(40, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(1, 10)\n\\end{align*}\n\n\nUse what you know to find the posterior model of \\pi,\n\n\\pi|(Y=26) \\sim \\text{Beta}(\\text{??}, \\text{??})"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-10",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-10",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nAfter data collection, Y = 26 of the n=40 study participants inflected what they understood to be the maximum shock.\nUse what you know to find the posterior model of \\pi,\n\n\\pi|(Y=26) \\sim \\text{Beta}(\\text{??}, \\text{??})\n\nWith \\alpha = 1 and \\beta = 10, we know the posterior distribution will be as follows,\n\n\n\\begin{align*}\n\\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y) \\\\\n&\\sim \\text{Beta}(1+26, 10+40-26) \\\\\n&\\sim \\text{Beta}(27, 24)\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-11",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-11",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nLet’s find the summary,\n\n\nsummarize_beta_binomial(alpha = 1, beta = 10, y = 26, n = 40)\n\n\nand graph the distributions involved,\n\n\nplot_beta_binomial(alpha = 1, beta = 10, y = 26, n = 40)\n\n\nWhat belief did we have for \\pi before considering the data?\nWhat belief do we have for \\pi after considering the prior and the observed data?"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-12",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-12",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\nLet’s find the summary,\n\n\nsummarize_beta_binomial(alpha = 1, beta = 10, y = 26, n = 40)\n\n      model alpha beta       mean      mode         var         sd\n1     prior     1   10 0.09090909 0.0000000 0.006887052 0.08298827\n2 posterior    27   24 0.52941176 0.5306122 0.004791057 0.06921746\n\n\n\nand graph the distributions involved,\n\n\n\nplot_beta_binomial(alpha = 1, beta = 10, y = 26, n = 40) + theme_bw()"
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#example-13",
    "href": "files/lecture/W08-L1-beta-binomial.html#example-13",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\n\n\n\n\nWhat belief did we have for \\pi before considering the data?\n\nFewer than 25% of people would inflict the most severe shock.\n\nWhat belief do we have for \\pi after considering the prior and the observed data?\n\nSomewhere between 30% and 70% of people would inflict the most severe shock."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#wrap-up",
    "href": "files/lecture/W08-L1-beta-binomial.html#wrap-up",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have built the Beta-Binomial model for \\pi, an unknown proportion.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY|\\pi &\\sim \\text{Bin}(n,\\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha,\\beta) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\pi), is given by Beta(\\alpha,\\beta).\nThe data model, f(Y|\\pi), is given by Bin(n,\\pi).\nThe likelihood function, L(\\pi|y), is obtained by plugging y into the Binomial pmf.\nThe posterior model is a Beta distribution with updated parameters \\alpha+y and \\beta+n-y."
  },
  {
    "objectID": "files/lecture/W08-L1-beta-binomial.html#homework",
    "href": "files/lecture/W08-L1-beta-binomial.html#homework",
    "title": "The Beta-Binomial Bayesian Model",
    "section": "Homework",
    "text": "Homework\n\n3.3\n3.9\n3.10\n3.18"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#introduction",
    "href": "files/lecture/W01-L1-intro-to-R.html#introduction",
    "title": "Review of Technology",
    "section": "Introduction",
    "text": "Introduction\n\nWelcome to Applied Bayesian Analysis - Spring 2025!\n\nCanvas set up\nSyllabus\nDiscord\nR/RStudio\nQuarto\nGitHub\nResources"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#introduction-1",
    "href": "files/lecture/W01-L1-intro-to-R.html#introduction-1",
    "title": "Review of Technology",
    "section": "Introduction",
    "text": "Introduction\n\nGeneral topics:\n\nProbability rules and distributions\nBayes Theorem\nPrior distributions\nPosterior distributions\nConjugate families\nBeta-Binomial, Normal-Normal, and Gamma-Poisson models\nPosterior simulation\nPosterior inference\nLinear regression\n\nThis is an applied class."
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#github",
    "href": "files/lecture/W01-L1-intro-to-R.html#github",
    "title": "Review of Technology",
    "section": "GitHub",
    "text": "GitHub\n\nOur course lectures and labs are posted on GitHub.\nPlease bookmark the repository: GitHub for STA6349.\nYou will want to look at my .qmd files for formatting / \\LaTeX purposes.\nFeel free to poke around my GitHub to see materials for other classes."
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#rrstudio",
    "href": "files/lecture/W01-L1-intro-to-R.html#rrstudio",
    "title": "Review of Technology",
    "section": "R/RStudio",
    "text": "R/RStudio\n\nWe will be using R in this course.\n\nI use the RStudio IDE, however, if you would like to use another IDE, that is fine.\n\nIt is okay if you have not used R before!\nFull disclosure: I am a biostatistician first, programmer second.\n\nThis means that I focus on the application of statistical methods and not on “understanding” the innerworkings of R.\n\nR is a tool that we use, like how SAS, JMP, Stata, SPSS, Excel, etc. are tools.\n\nSometimes my code is not elegant/efficient, and that’s okay! Because our focus is on the application of methods, we are interested in the code working.\nI have learned so much from my students since implementing R in the classroom.\n\nDo not be afraid to teach me new things!\n\n\nThis is an applied class."
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#rrstudio-1",
    "href": "files/lecture/W01-L1-intro-to-R.html#rrstudio-1",
    "title": "Review of Technology",
    "section": "R/RStudio",
    "text": "R/RStudio\n\nYou can install R and RStudio on your computer for free.\n\nR from CRAN\nRStudio from Posit\n\nAlternative to installing: RStudio Server hosted by UWF HMCSE\nDo not use Citrix.\nI encourage you to install R on your own machine if you are able.\n\nIn the “real world,” you will not have access to the server.\nInstalling on your own machine will help your future self troubleshoot issues."
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#tidy-data",
    "href": "files/lecture/W01-L1-intro-to-R.html#tidy-data",
    "title": "Review of Technology",
    "section": "Tidy Data",
    "text": "Tidy Data\nJournal article: Tidy Data by Wickham (2014, Journal of Statistical Software)\nBook chapter: Data Tidying by Wickham, Çetinkaya-Rundel, and Grolemund\n\nThere are three interrelated rules that make a dataset tidy:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value."
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#tidyverse",
    "href": "files/lecture/W01-L1-intro-to-R.html#tidyverse",
    "title": "Review of Technology",
    "section": "Tidyverse",
    "text": "Tidyverse"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#tidyverse-1",
    "href": "files/lecture/W01-L1-intro-to-R.html#tidyverse-1",
    "title": "Review of Technology",
    "section": "Tidyverse",
    "text": "Tidyverse\n\ntibble for modern data frames.\nreadr and haven for data import.\n\nreadr is pulled in with tidyverse\nhaven needs to be called in on its own\n\ntidyr for data tidying.\n\ndplyr for data manipulation.\nggplot2 for data visualization.\nIt is not possible for me to teach you everything you will ever need to know about programming in R.\n\nGood resource for tidyverse: data science in a box"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#tidyverse-2",
    "href": "files/lecture/W01-L1-intro-to-R.html#tidyverse-2",
    "title": "Review of Technology",
    "section": "Tidyverse",
    "text": "Tidyverse\n\nA major advantage of using tidyverse is the common “language” between the functions.\nAnother advantage: the pipe operator, %&gt;%.\n\nYes, there is a pipe operator now included in base R. No, I do not use it.\n\nHere is a discussion of similarities and differences from Hadley himself.\n\nBy default, %&gt;% deposits everything that came before into the first argument of the next function.\n\nIf we want to insert it elsewhere, we can indicate that with a “.” in the function.\n\n\n\n\nlm(body_mass_g ~ flipper_length_mm, data = penguins)\n\npenguins %&gt;% lm(body_mass_g ~ flipper_length_mm, data = .)"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#tidyverse-3",
    "href": "files/lecture/W01-L1-intro-to-R.html#tidyverse-3",
    "title": "Review of Technology",
    "section": "Tidyverse",
    "text": "Tidyverse\n\nIf we try to use a function before calling its package in, we will see an error.\n\n\nsw &lt;- tibble(starwars) %&gt;% filter(mass &lt; 100)\n\nError in tibble(starwars) %&gt;% filter(mass &lt; 100): could not find function \"%&gt;%\"\n\n\n\nWe are good to go after calling in tidyverse.\n\n\nlibrary(tidyverse)\nsw &lt;- tibble(starwars) %&gt;% filter(mass &lt; 100)\nhead(sw, n=3)"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#importing-data",
    "href": "files/lecture/W01-L1-intro-to-R.html#importing-data",
    "title": "Review of Technology",
    "section": "Importing Data",
    "text": "Importing Data\n\nLet’s import data from the Jackson Heart Study.\n\n\njhs_csv &lt;- read_csv(\"/path/to/folder/analysislong.csv\")\nhead(jhs_csv)"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#importing-data-1",
    "href": "files/lecture/W01-L1-intro-to-R.html#importing-data-1",
    "title": "Review of Technology",
    "section": "Importing Data",
    "text": "Importing Data\n\nBe comfortable with Googling for help with code to import data.\nAs a collaborative statistician, I have received the following file types:\n\n.sas7bdat\n.sav\n.dat\n.csv\n.xls\n.xlsx\n.txt\nGoogle Sheet\nhand written"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#importing-data-2",
    "href": "files/lecture/W01-L1-intro-to-R.html#importing-data-2",
    "title": "Review of Technology",
    "section": "Importing Data",
    "text": "Importing Data\n\nThere have been times where I have received data as a .xlsx, but I can’t get it to import properly.\n\nUsually, the issue is that there is a character variable with too much text.\nSometimes, it’s that the variable type changes mid-dataset.\n\ni.e., both a number and a character stored in the same vector.\n\n\nSometimes the solution is saving it as a different file type (I default to .csv).\nGet comfortable Googling error messages.\n\nI am still consulting Dr. Google for assistance on a daily basis!\n\nTry not to do any data management within the original file type!\n\nWe want to be able to retrace our steps.\nReproducible research!"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#data-manipulation",
    "href": "files/lecture/W01-L1-intro-to-R.html#data-manipulation",
    "title": "Review of Technology",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nFunctions:\n\nselect(): Selecting columns.\nfilter(): Filtering the observations.\nmutate(): Adding or transforming columns.\nsummarise(): Summarizing data.\ngroup_by(): Grouping data for summary operations.\n%&gt;%: Pipelines."
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#data-manipulation-1",
    "href": "files/lecture/W01-L1-intro-to-R.html#data-manipulation-1",
    "title": "Review of Technology",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nselect(): Selecting columns.\n\n\njhs_csv %&gt;% \n  select(subjid, visit, age, sex) %&gt;% \n  head(n=4)"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#data-manipulation-2",
    "href": "files/lecture/W01-L1-intro-to-R.html#data-manipulation-2",
    "title": "Review of Technology",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nfilter(): Filtering rows.\n\n\njhs_csv %&gt;% \n  filter(visit == 1) %&gt;% \n  head(n=3)"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#data-manipulation-3",
    "href": "files/lecture/W01-L1-intro-to-R.html#data-manipulation-3",
    "title": "Review of Technology",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nmutate(): Adding or transforming columns.\n\n\njhs_csv %&gt;% \n  filter(visit == 1) %&gt;%\n  select(subjid, sex) %&gt;%\n  mutate(male = if_else(sex == \"Male\", 1, 0)) %&gt;%\n  head(n=3)"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#data-manipulation-4",
    "href": "files/lecture/W01-L1-intro-to-R.html#data-manipulation-4",
    "title": "Review of Technology",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nsummarise(): Summarizing data.\n\n\njhs_csv %&gt;% \n  filter(visit == 1) %&gt;%\n  summarize(n = n(),\n            mean_BMI = round(mean(BMI, na.rm = TRUE),2),\n            sd_BMI = round(sd(BMI, na.rm = TRUE),2),\n            n_female = sum(sex == \"Female\", na.rm = TRUE),\n            pct_female = round(sum(sex == \"Female\", na.rm = TRUE)*100/n(),2))"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#data-manipulation-5",
    "href": "files/lecture/W01-L1-intro-to-R.html#data-manipulation-5",
    "title": "Review of Technology",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\ngroup_by(): Grouping data for summary operations.\n\n\njhs_csv %&gt;% \n  filter(visit == 1) %&gt;%\n  group_by(HTN) %&gt;%\n  summarize(n = n(),\n            mean_BMI = round(mean(BMI, na.rm = TRUE),2),\n            sd_BMI = round(sd(BMI, na.rm = TRUE),2),\n            n_female = sum(sex == \"Female\", na.rm = TRUE),\n            pct_female = round(sum(sex == \"Female\", na.rm = TRUE)*100/n(),2))"
  },
  {
    "objectID": "files/lecture/W01-L1-intro-to-R.html#wrap-up",
    "href": "files/lecture/W01-L1-intro-to-R.html#wrap-up",
    "title": "Review of Technology",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have gently introduced data management in R.\nI do not expect you to become an expert R programmer, but the more you practice, the easier it becomes.\nToday’s activity: Assignment 0"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#introduction-gamma-poisson",
    "href": "files/lecture/W10-L1-conjugate-families.html#introduction-gamma-poisson",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Introduction: Gamma-Poisson",
    "text": "Introduction: Gamma-Poisson\n\nRecall the Beta-Binomial from our previous lecture,\n\ny \\sim \\text{Bin}(n, \\pi) (data distribution)\n\\pi \\sim \\text{Beta}(\\alpha, \\beta) (prior distribution)\n\\pi|y \\sim \\text{Beta}(\\alpha+y, \\beta+n-y) (posterior distribution)\n\nBeta-Binomial is from a conjugate family (i.e., the posterior is from the same model family as the prior).\nToday, we will learn about other conjugate families, the Gamma-Poisson and the Normal-Normal."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model",
    "href": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nSuppose we are now interested in modeling the number of spam calls we receive.\n\nThis means that we are modeling the rate, \\lambda.\n\nWe take a guess and say that the value of \\lambda that is most likely is around 5,\n\n… but reasonably ranges between 2 and 7 calls per day.\n\nWhy can’t we use the Beta distribution as our prior distribution?"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-1",
    "href": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-1",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nSuppose we are now interested in modeling the number of spam calls we receive.\n\nThis means that we are modeling the rate, \\lambda.\n\nWe take a guess and say that the value of \\lambda that is most likely is around 5,\n\n… but reasonably ranges between 2 and 7 calls per day.\n\nWhy can’t we use the Beta distribution as our prior distribution?\n\n\\lambda is the mean of a count \\to \\lambda \\in \\mathbb{R}^+ \\to \\lambda is not limited to [0, 1] \\to broken assumption for Beta distribution."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-2",
    "href": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-2",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nSuppose we are now interested in modeling the number of spam calls we receive.\n\nThis means that we are modeling the rate, \\lambda.\n\nWe take a guess and say that the value of \\lambda that is most likely is around 5,\n\n… but reasonably ranges between 2 and 7 calls per day.\n\nWhy can’t we use the Beta distribution as our prior distribution?\n\n\\lambda is the mean of a count \\to \\lambda \\in \\mathbb{R}^+ \\to \\lambda is not limited to [0, 1] \\to broken assumption for Beta distribution.\n\nWhy can’t we use the binomial distribution as our data distribution?"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-3",
    "href": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-3",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nSuppose we are now interested in modeling the number of spam calls we receive.\n\nThis means that we are modeling the rate, \\lambda.\n\nWe take a guess and say that the value of \\lambda that is most likely is around 5,\n\n… but reasonably ranges between 2 and 7 calls per day.\n\nWhy can’t we use the Beta distribution as our prior distribution?\n\n\\lambda is the mean of a count \\to \\lambda \\in \\mathbb{R}^+ \\to \\lambda is not limited to [0, 1] \\to broken assumption for Beta distribution.\n\nWhy can’t we use the binomial distribution as our data distribution?\n\nY_i is a count \\to Y_i \\in \\mathbb{N}^+ \\to Y_i is not limited to \\{0, 1\\} \\to broken assumption for Binomial distribution."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-4",
    "href": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-4",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nWe will use the Poisson distribution to model the number of spam calls – Y \\in \\{0, 1, 2, ...\\}.\n\nY is the number of independent events that occur in a fixed amount of time or space.\n\\lambda &gt; 0 is the rate at which these events occur.\n\nMathematically,\n\n Y | \\lambda \\sim \\text{Pois}(\\lambda),\n\nwith pmf,\n\nf(y|\\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}, \\ \\ \\ y \\in \\{0,1, 2, ... \\}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-5",
    "href": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-5",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\n\\lambda defines the mean and the variance\n\nThe shape of the Poisson pmf depends on \\lambda."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-6",
    "href": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-6",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\n\\lambda defines the mean and the variance\n\nThe shape of the Poisson pmf depends on \\lambda."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-7",
    "href": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-7",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nWe will be taking samples from different days.\n\nWe assume that the daily number of calls may different from day to day.\nOn each day i,\n\n\nY_i|\\lambda \\sim \\text{Pois}(\\lambda)\n\nThis has a unique pmf for each day (i),\n\nf(y_i|\\lambda) = \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\n\nBut really, we are interested in the joint information in our sample of n observations.\n\nThe joint pmf gives us this information."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-8",
    "href": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-8",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nThe joint pmf for the Poisson,\n\n\n\\begin{align*}\nf\\left(\\overset{\\to}{y_i}|\\lambda\\right) &= \\prod_{i=1}^n f(y_i|\\lambda) \\\\\n&= f(y_1|\\lambda) \\times f(y_2|\\lambda) \\times ... \\times f(y_n|\\lambda) \\\\\n&= \\frac{\\lambda^{y_1}e^{-\\lambda}}{y_1!} \\times \\frac{\\lambda^{y_2}e^{-\\lambda}}{y_2!} \\times ... \\times \\frac{\\lambda^{y_n}e^{-\\lambda}}{y_n!} \\\\\n&= \\frac{\\left( \\lambda^{y_1} \\lambda^{y_2} \\cdot \\cdot \\cdot \\ \\lambda^{y_n}  \\right) \\left( e^{-\\lambda} e^{-\\lambda} \\cdot \\cdot \\cdot e^{-\\lambda}\\right)}{y_1! y_2! \\cdot \\cdot \\cdot y_n!} \\\\\n&= \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !}\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-9",
    "href": "files/lecture/W10-L1-conjugate-families.html#poisson-data-model-9",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nIf the joint pmf for the Poisson is\n\nf\\left(\\overset{\\to}{y_i}|\\lambda\\right) = \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !}\n\nthen the likelihood function for \\lambda &gt; 0 is\n\n\n\\begin{align*}\nL\\left(\\lambda|\\overset{\\to}{y_i}\\right) &= \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !} \\\\\n& \\propto \\lambda^{\\sum y_i} e^{-n\\lambda}\n\\end{align*}\n\n\nPease see page 102 in the textbook for full derivations."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-prior",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-prior",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma Prior",
    "text": "Gamma Prior\n\nIf \\lambda is a continuous random variable that can take on any positive value (\\lambda &gt; 0), then the variability may be modeled with the Gamma distribution with\n\nshape hyperparameter s&gt;0\nrate hyperparameter r&gt;0.\n\nThus,\n\n\\lambda \\sim \\text{Gamma}(s, r)\n\nand the Gamma pdf is given by\n\nf(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1} e^{-r\\lambda}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-prior-1",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-prior-1",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma Prior",
    "text": "Gamma Prior\n\n…then the variability may be modeled with the Gamma distribution with shape s&gt;0 and rate r&gt;0."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-prior-2",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-prior-2",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma Prior",
    "text": "Gamma Prior\n\nLet’s now tune our prior.\nWe are assuming \\lambda \\approx 5, somewhere between 2 and 7.\nWe know the mean of the gamma distribution,\n\nE(\\lambda) = \\frac{s}{r} \\approx 5 \\to 5r \\approx s\n\nYour turn! Use the plot_gamma() function to figure out what value of s and r we need."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-prior-3",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-prior-3",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma Prior",
    "text": "Gamma Prior\n\nLooking at different values:"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nLet \\lambda &gt; 0 be an unknown rate parameter and (Y_1, Y_2, ... , Y_n) be an independent sample from the Poisson distribution.\nThe Gamma-Poisson Bayesian model is as follows:\n\n\n\\begin{align*}\nY_i | \\lambda &\\overset{ind}\\sim \\text{Pois}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(s, r) \\\\\n\\lambda | \\overset{\\to}y &\\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right)\n\\end{align*}\n\n\nThe proof can be seen in section 5.2.4 of the textbook."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-1",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-1",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nSuppose we use Gamma(10, 2) as the prior for \\lambda, the daily rate of calls.\nOn four separate days in the second week of August (i.e., independent days), we received \\overset{\\to}y = (6, 2, 2, 1) calls.\nYour turn! Use the plot_poisson_likelihood() function:\n\n\nplot_poisson_likelihood(y = c(6, 2, 2, 1), lambda_upper_bound = 10)\n\n\nNotes:\n\nlambda_upper_bound limits the x axis – recall that \\lambda \\in (0, \\infty)!\nlambda_upper_bound’s default value is 10."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-2",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-2",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\n\nplot_poisson_likelihood(y = c(6, 2, 2, 1), lambda_upper_bound = 10) + theme_bw()"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-3",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-3",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nWe can see that the average is around 2.75.\nWe can also verify that –\n\n\nmean(c(6, 2, 2, 1))\n\n[1] 2.75\n\n\n\nWe know our prior distribution is Gamma(10, 2) and the data distribution is Poi(2.75).\nThus, the posterior is as follows,\n\n\n\\begin{align*}\n\\lambda | \\overset{\\to}y &\\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right) \\\\\n&\\sim \\text{Gamma}\\left(10 + 11, 2 + 4 \\right) \\\\\n&\\sim \\text{Gamma}\\left(21, 6 \\right)\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-4",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-4",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nYour turn! Use the plot_gamma_poisson() function:\n\n\nplot_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4) + theme_bw()"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-5",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-5",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nYour turn! Use the plot_gamma_poisson() function:\n\n\n\nplot_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4) + theme_bw()"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-6",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-6",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nYour turn! What is different if we had used one of the other priors?"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-7",
    "href": "files/lecture/W10-L1-conjugate-families.html#gamma-poisson-conjugacy-7",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nYour turn! What is different if we had used Gamma(15, 3) as our prior?"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#introduction-normal-normal",
    "href": "files/lecture/W10-L1-conjugate-families.html#introduction-normal-normal",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Introduction: Normal-Normal",
    "text": "Introduction: Normal-Normal\n\nSo far, we have learned two conjugate families:\n\nBeta-Binomial (binary outcomes)\n\ny \\sim \\text{Bin}(n, \\pi) (data distribution)\n\\pi \\sim \\text{Beta}(\\alpha, \\beta) (prior distribution)\n\\pi|y \\sim \\text{Beta}(\\alpha+y, \\beta+n-y) (posterior distribution)\n\nGamma-Poisson (count outcomes)\n\nY_i | \\lambda \\overset{ind}\\sim \\text{Pois}(\\lambda) (data distribution)\n\\lambda \\sim \\text{Gamma}(s, r) (prior distribution)\n\\lambda | \\overset{\\to}y \\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right) (posterior distribution)\n\n\nNow, we will learn about another conjugate family, the Normal-Normal, for continuous outcomes."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#introduction",
    "href": "files/lecture/W10-L1-conjugate-families.html#introduction",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Introduction",
    "text": "Introduction\n\nAs scientists learn more about brain health, the dangers of concussions are gaining greater attention.\nWe are interested in \\mu, the average volume (cm3) of a specific part of the brain: the hippocampus.\nWikipedia tells us that among the general population of human adults, each half of the hippocampus has volume between 3.0 and 3.5 cm3.\n\nTotal hippocampal volume of both sides of the brain is between 6 and 7 cm3.\nLet’s assume that the mean hippocampal volume among people with a history of concussions is also somewhere between 6 and 7 cm3.\n\nWe will take a sample of n=25 participants and update our belief."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#the-normal-model",
    "href": "files/lecture/W10-L1-conjugate-families.html#the-normal-model",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nLet Y \\in \\mathbb{R} be a continuous random variable.\n\nThe variability in Y may be represented with a Normal model with mean parameter \\mu \\in \\mathbb{R} and standard deviation parameter \\sigma &gt; 0.\n\nThe Normal model’s pdf is as follows,\n\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left\\{ \\frac{-(y-\\mu)^2}{2\\sigma^2} \\right\\}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#the-normal-model-1",
    "href": "files/lecture/W10-L1-conjugate-families.html#the-normal-model-1",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nUse the plot_normal() function to plot the following:\n\nVary \\mu:\n\nN(-1, 1)\nN(0, 1)\nN(1, 1)\n\nVary \\sigma:\n\nN(0, 1)\nN(0, 2)\nN(0, 3)"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#the-normal-model-2",
    "href": "files/lecture/W10-L1-conjugate-families.html#the-normal-model-2",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nIf we vary \\mu,"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#the-normal-model-3",
    "href": "files/lecture/W10-L1-conjugate-families.html#the-normal-model-3",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nIf we vary \\sigma,"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#the-normal-model-4",
    "href": "files/lecture/W10-L1-conjugate-families.html#the-normal-model-4",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nOur data model is as follows,\n\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\nThe joint pdf is as follows,\n\n\nf(\\overset{\\to}y | \\mu) = \\prod_{i=1}^n f(y_i | \\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-(y_i-\\mu)^2}{2\\sigma^2} \\right\\}\n\n\nMeaning the likelihood is as follows,\n\n\nL(\\mu|\\overset{\\to}y) \\propto \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-(y_i-\\mu)^2}{2\\sigma^2} \\right\\} = \\exp \\left\\{ \\frac{- \\sum_{i=1}^n(y_i-\\mu)^2}{2\\sigma^2} \\right\\}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#the-normal-model-5",
    "href": "files/lecture/W10-L1-conjugate-families.html#the-normal-model-5",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nOur data model is as follows,\n\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\nReturning to our brain analysis, we will assume that the hippocampal volumes of our n = 25 subjects have a normal distribution with mean \\mu and standard deviation \\sigma.\n\nRight now, we are only interested in \\mu, so we assume \\sigma = 0.5 cm3\nThis choice suggests that most people have hippocampal volumes within 2 \\sigma = 1 cm3."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#normal-prior",
    "href": "files/lecture/W10-L1-conjugate-families.html#normal-prior",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Normal Prior",
    "text": "Normal Prior\n\nWe know that with Y_i | \\mu \\sim N(\\mu, \\sigma^2), \\mu \\in \\mathbb{R}.\n\nWe think a normal prior for \\mu is reasonable.\n\nThus, we assume that \\mu has a normal distribution around some mean, \\theta, with standard deviation, \\tau.\n\n\\mu \\sim N(\\theta, \\tau^2),\n\nmeaning that \\mu has prior pdf\n\nf(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\tau^2}} \\exp \\left\\{ \\frac{-(\\mu - \\theta)^2}{2 \\tau^2} \\right\\}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#normal-prior-1",
    "href": "files/lecture/W10-L1-conjugate-families.html#normal-prior-1",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Normal Prior",
    "text": "Normal Prior\n\nWe can tune the hyperparameters \\theta and \\tau to reflect our understanding and uncertainty about the average hippocampal volume (\\mu) among people with a history of concussions.\nWikipedia showed us that hippocampal volumes tend to be between 6 and 7 cm3 \\to \\theta=6.5.\nWhen we set the standard deviation we can check the plausible range of values of \\mu:\n\nFollow up: why 2?\n\n\n\\theta \\pm 2 \\times \\tau\n\nIf we assume \\tau=0.4,\n\n(6.5 \\pm 2 \\times 0.4) = (5.7, 7.3)"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#normal-prior-2",
    "href": "files/lecture/W10-L1-conjugate-families.html#normal-prior-2",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Normal Prior",
    "text": "Normal Prior\n\nThus, our tuned prior is \\mu \\sim N(6.5, 0.4^2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis range incorporates our uncertainty - it is wider than the Wikipedia range."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#normal-normal-conjugacy",
    "href": "files/lecture/W10-L1-conjugate-families.html#normal-normal-conjugacy",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet \\mu \\in \\mathbb{R} be an unknown mean parameter and (Y_1, Y_2, ..., Y_n) be an independent N(\\mu, \\sigma^2) sample where \\sigma is assumed to be known.\nThe Normal-Normal Bayesian model is as follows:\n\n\n\\begin{align*}\nY_i | \\mu &\\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2) \\\\\n\\mu &\\sim N(\\theta, \\tau^2) \\\\\n\\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#normal-normal-conjugacy-1",
    "href": "files/lecture/W10-L1-conjugate-families.html#normal-normal-conjugacy-1",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nWhat happens as n increases?"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#normal-normal-conjugacy-2",
    "href": "files/lecture/W10-L1-conjugate-families.html#normal-normal-conjugacy-2",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nWhat happens as n increases?\n\n\n\\begin{align*}\n\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} &\\to 0 \\\\\n\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2} &\\to 1 \\\\\n\\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} &\\to 0\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#normal-normal-conjugacy-3",
    "href": "files/lecture/W10-L1-conjugate-families.html#normal-normal-conjugacy-3",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\n\\begin{align*}\n\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} &\\to 0 \\\\\n\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2} &\\to 1 \\\\\n\\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} &\\to 0\n\\end{align*}\n\n\nThe posterior mean places less weight on the prior mean and more weight on the sample mean \\bar{y}.\nThe posterior certainty about \\mu increases and becomes more in sync with the data."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#example",
    "href": "files/lecture/W10-L1-conjugate-families.html#example",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Example",
    "text": "Example\n\nLet us now apply this to our example.\nWe have our prior model, \\mu \\sim N(6.5, 0.4^2).\nLet’s look at the football dataset in the bayesrules package.\n\n\ndata(football)\nconcussion_subjects &lt;- football %&gt;% \n  filter(group == \"fb_concuss\")\n\n\nWhat is the average hippocampal volume?"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#example-1",
    "href": "files/lecture/W10-L1-conjugate-families.html#example-1",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Example",
    "text": "Example\n\nLet us now apply this to our example.\nWe have our prior model, \\mu \\sim N(6.5, 0.4^2).\nLet’s look at the football dataset in the bayesrules package.\n\n\ndata(football)\nconcussion_subjects &lt;- football %&gt;% \n  filter(group == \"fb_concuss\")\n\n\nWhat is the average hippocampal volume?\n\n\nmean(concussion_subjects$volume)\n\n[1] 5.7346"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#example-2",
    "href": "files/lecture/W10-L1-conjugate-families.html#example-2",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Example",
    "text": "Example\n\nWe can also plot the density!\n\n\n\nconcussion_subjects %&gt;% ggplot(aes(x = volume)) + geom_density() + theme_bw()"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#example-3",
    "href": "files/lecture/W10-L1-conjugate-families.html#example-3",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Example",
    "text": "Example\n\nNow, we can plug in the information we have (n = 25, \\bar{y} = 5.735, \\sigma = 0.5) into our likelihood,\n\n\nL(\\mu|\\overset{\\to}y) \\propto \\exp \\left\\{ \\frac{-(5.735 - \\mu)^2}{2(0.5^2/25)} \\right\\}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#example-4",
    "href": "files/lecture/W10-L1-conjugate-families.html#example-4",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Example",
    "text": "Example\n\nWe are now ready to put together our posterior:\n\nData distribution, Y_i | \\mu \\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2)\nPrior distribution, \\mu \\sim N(\\theta, \\tau^2)\n\nPosterior distribution, \\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nGiven our information (\\theta=6.5, \\tau=0.4, n=25, \\bar{y}=5.735, \\sigma=0.5), our posterior is\n\n\n\\begin{align*}\n\\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right) \\\\\n&\\sim N\\left( 6.5 \\frac{0.5^2}{25 \\cdot 0.4^2 + 0.5^2} + 5.735 \\frac{25 \\cdot 0.4^2}{25 \\cdot 0.4^2 + 0.5^2}, \\frac{0.4^2 \\cdot 0.5^2}{25 \\cdot 0.4^2 + 0.5^2} \\right) \\\\\n&\\sim N(6.5 \\cdot 0.0588 + 5.737 \\cdot 0.9412, 0.09^2) \\\\\n&\\sim N(5.78, 0.09^2)\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#example-5",
    "href": "files/lecture/W10-L1-conjugate-families.html#example-5",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Example",
    "text": "Example\n\nLooking at the posterior, we can see the weights\n\n\n\\begin{align*}\n\\mu | \\overset{\\to}y &\\sim N\\left( 6.5 \\frac{0.5^2}{25 \\cdot 0.4^2 + 0.5^2} + 5.735 \\frac{25 \\cdot 0.4^2}{25 \\cdot 0.4^2 + 0.5^2}, \\frac{0.4^2 \\cdot 0.5^2}{25 \\cdot 0.4^2 + 0.5^2} \\right) \\\\\n&\\sim N(6.5 \\cdot 0.0588 + 5.737 \\cdot 0.9412, 0.009^2)\n\\end{align*}\n\n\n95% on the data mean, 6% on the prior mean."
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#example-6",
    "href": "files/lecture/W10-L1-conjugate-families.html#example-6",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Example",
    "text": "Example\n\nWe can plot the distribution,"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#example-7",
    "href": "files/lecture/W10-L1-conjugate-families.html#example-7",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Example",
    "text": "Example\n\nWe can summarize the distribution,\n\n\nsummarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5, y_bar = 5.735, n = 25) \n\n      model mean mode         var         sd\n1     prior 6.50 6.50 0.160000000 0.40000000\n2 posterior 5.78 5.78 0.009411765 0.09701425"
  },
  {
    "objectID": "files/lecture/W10-L1-conjugate-families.html#homework",
    "href": "files/lecture/W10-L1-conjugate-families.html#homework",
    "title": "Gamma-Poisson and Normal-Normal Models",
    "section": "Homework",
    "text": "Homework\n\n5.3\n5.5\n5.6\n5.9\n5.10"
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#example",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#example",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Example",
    "text": "Example\n\nIn 1996, Gary Kasparov played a six-game chess match against the IBM supercomputer Deep Blue.\n\nOf the six games, Kasparov won three, drew two, and lost one.\nThus, Kasparov won the overall match.\n\nKasparov and Deep Blue were to meet again for a six-game match in 1997.\nLet \\pi denote Kasparov’s chances of winning any particular game in the re-match.\n\nThus, \\pi is a measure of his overall skill relative to Deep Blue.\nGiven the complexity of chess, machines, and humans, \\pi is unknown and can vary over time.\n\ni.e., \\pi is a random variable.\n\n\nOur first step is to start with a prior model. This model\n\nIdentifies what values \\pi can take,\nassigns a prior weight or probability to each, and\nthese probabilities sum to 1."
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#example-1",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#example-1",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Example",
    "text": "Example\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1\n\n\n\n\nNote that this is an incredibly simple model.\n\nThe win probability can technically be any number \\in [0, 1].\nHowever, this prior assumes that \\pi has a discrete set of possibilities: 20%, 50%, or 80%."
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#example-2",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#example-2",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Example",
    "text": "Example\n\nIn the second step of our analysis, we collect and process data which can inform our understanding of \\pi.\nHere, Y = the number of the six games in the 1997 re-match that Kasparov wins.\n\nAs chess match outcome isn’t predetermined, Y is a random variable that can take any value in \\{1, 2, 3, 4, 5, 6\\}.\n\nNote that Y inherently depends upon \\pi.\n\nIf \\pi = 0.80, Y would also be high (on average).\nIf \\pi = 0.20, Y would also be low (on average).\n\nThus, we must model this dependence of Y on \\pi using a conditional probability model."
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe must make two assumptions about the chess match:\n\nGames are independent (the outcome of one game does not influence the outcome of another).\nKasparov has an equal probability of winning any game in the match.\n\ni.e., probability of winning does not increase or decrease as the match goes on.\n\n\nWe will use a binomial model for this problem.\n\nRecall the binomial pmf,\n\n\nf(y|\\pi) = {n \\choose y} \\pi^y (1-\\pi)^{n-y}, \n\nIn our case,\n\nY|\\pi \\sim \\text{Bin}(6, \\pi)"
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model-1",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model-1",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win all 6 games is approximately 26%.\n\nf(y=6|\\pi=0.8) = {6 \\choose 6} 0.8^6 (1-0.8)^{6-6}, \n\ndbinom(6, 6, 0.8)\n\n[1] 0.262144\n\n\n\nThe probability that he would win none of the games is approximately 0%.\n\nf(y=0|\\pi=0.8) = {6 \\choose 0} 0.8^0 (1-0.8)^{6-0}, \n\ndbinom(0, 6, 0.8)\n\n[1] 6.4e-05"
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model-2",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model-2",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nYour turn!\nWe want to reproduce Figure 2.5 from the Bayes Rules! textbook (from Section 2.3.2).\n\n\n\nWork with your group to come up with that graph.\nPick one person to present in 15 minutes."
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model-3",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model-3",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nNote that the Binomial gives us the theoretical model of the data we might observe.\n\nIn the end, Kasparov only won one of the six games against Deep Blue in 1997 (Y=1).\n\nNext step: how compatible this particular data is with the various possible \\pi?\n\nWhat is the likelihood of Kasparov winning Y=1 game under each possible \\pi?\n\nRecall, f(y|\\pi) = L(\\pi|Y=y). When Y=1,\n\n\n\\begin{align*}\nL(\\pi | y = 1) &= f(y=1|\\pi) \\\\\n&= {6 \\choose 1} \\pi^1 (1-\\pi)^6-1 \\\\\n&= 6\\pi(1-\\pi)^5\n\\end{align*}\n\n\nNote that we do not expect all likelihoods to sum to 1."
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model-4",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model-4",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nYour turn!\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\n\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)"
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model-5",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#binomial-data-model-5",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nYour turn!\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)\n0.3932\n0.0938\n0.0015\n\n\n\n\nAs we can see, the likelihoods do not sum to 1."
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#normalizing-constant",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#normalizing-constant",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nBayes’ Rule requires three pieces of information:\n\nPrior\nLikelihood\nNormalizing constant\n\nNormalizing constant: a value that ensures that the sum of all probabilities is equal to 1.\n\nIt can be a scalar or a function.\nEvery probability distribution that does not sum to 1 will ahve a normalizing constant."
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#normalizing-constant-1",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#normalizing-constant-1",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) &= \\sum_{\\pi \\in \\{0.2, 0.5, 0.8 \\}} L(\\pi |y=1)f(\\pi) \\\\\n&= L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + L(\\pi=0.8|y=1)f(\\pi=0.8) \\\\\n&= \\ ...\n\\end{align*}\n\n\nWork with your group to find the normalizing constant."
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#normalizing-constant-2",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#normalizing-constant-2",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) &= \\sum_{\\pi \\in \\{0.2, 0.5, 0.8 \\}} L(\\pi |y=1)f(\\pi) \\\\\n&= L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + L(\\pi=0.8|y=1)f(\\pi=0.8) \\\\\n&\\approx 0.3932 \\cdot 0.10 + 0.0938 \\cdot 0.25 + 0.0015 \\cdot 0.65 \\\\\n&\\approx 0.0637\n\\end{align*}\n\n\nAcross all possible values of \\pi, there is about a 6% chance that Kasparov would have won only one game."
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#posterior-probability-model",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#posterior-probability-model",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNow recall,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}\n\nIn our example, where y = 1,\n\nf(\\pi | y=1) = \\frac{f(\\pi) L(\\pi | y = 1)}{f(y=1)} \\  \\text{for} \\ \\pi \\in \\{ 0.2, 0.5, 0.8\\}\n\nWork with your group to find the posterior probabilities.\n\nYou will have one posterior probability for each value of \\pi."
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#posterior-probability-model-1",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#posterior-probability-model-1",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNote!! We do not have to calculate the normalizing constant!\nWe can note that f(Y=y) = 1/c.\nThen, we say that\n\n\n\\begin{align*}\nf(\\pi | y) &= \\frac{f(\\pi) L(\\pi|y)}{f(y)} \\\\\n& \\propto  f(\\pi) L(\\pi|y) \\\\\n\\\\\n\\text{posterior} &\\propto \\text{prior} \\cdot \\text{likelihood}\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W07-L2-Bayes-models-rv.html#wrap-up",
    "href": "files/lecture/W07-L2-Bayes-models-rv.html#wrap-up",
    "title": "Bayesian Modeling for Random Variables",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have gone through how to find posterior probabilities using the binomial distribution.\nNext week, we will learn about the Beta-Binomial model."
  },
  {
    "objectID": "test/LICENSE.html",
    "href": "test/LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2024 Shafayet Khan Shafee\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "<b>STA6349 - Applied Bayesian Analysis - Spring 2025</b>",
    "section": "",
    "text": "Tentative Schedule\n\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides\n\n\n\n\n\nW 01/09\nIntroduction to R\n \n\n\n\nM 01/13\nProbability Theory: Part 1\n \n\n\n\nW 01/15\nProbability Theory: Part 2\n \n\n\n\nM 01/20\nMLK Holiday - campus closed\n\n\n\n\nW 01/22\nsnow week!\n\n\n\n\nM 01/27\nDr. Seals sick\n\n\n\n\nW 01/29\nDr. Seals sick\n\n\n\n\nM 02/03\nDiscrete Random Variables\n \n\n\n\nW 02/05\nContinuous Random Variables\n \n\n\n\nM 02/10\nDr. Seals out (conference)\n\n\n\n\nW 02/12\nContinuous Random Variables\n \n\n\n\nM 02/17\nBayesian Thinking and Bayes’ Rule\n \n\n\n\nW 02/19\nBayesian Modeling for Random Variables\n \n\n\n\nM 02/24\nBeta-Binomial Conjugage Family\n \n\n\n\nW 02/26\nBeta-Binomial Conjugage Family\n \n\n\n\nM 03/03\nBalance and Sequentiality\n \n\n\n\nW 03/05\nWrite a biography for a Bayesian statistician!\n\n\n\n\nM 03/10\nGamma-Poisson and Normal-Normal\n \n\n\n\nW 03/12\nAssignment 2\n\n\n\n\nM 03/17\nSpring Break!\n\n\n\n\nW 03/19\nSpring Break!\n\n\n\n\nM 03/24\nApproximating the Posterior\n \n\n\n\nW 03/26\nPosterior Inference & Prediction\n\n\n\n\nM 03/31\nProject 1 (in class work)\n\n\n\n\nW 04/02\nProject 1 (presentation)\n\n\n\n\nM 04/07\nSimple Normal Regression\n\n\n\n\nW 04/09\nEvaluating Regression Models\n\n\n\n\nM 04/14\nMultiple Regression\n\n\n\n\nW 04/16\nAssignment 3\n\n\n\n\nM 04/21\nProject 2 (in class work)\n\n\n\n\nW 04/23\nProject 2 (presentation)\n\n\n\n\nM 04/28\nFinal Exam - in class (4/404) - 6:00–8:30 pm"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction",
    "text": "Introduction\n\nOn Monday, we talked about the Beta-Binomial model for binary outcomes with an unknown probability of success, \\pi.\nWe will now discuss sequentality in Bayesian analyses.\nWorking example:\n\nIn Alison Bechdel’s 1985 comic strip The Rule, a character states that they only see a movie if it satisfies the following three rules (Bechdel 1986):\n\nthe movie has to have at least two women in it;\nthese two women talk to each other; and\nthey talk about something besides a man.\n\nThese criteria constitute the Bechdel test for the representation of women in film.\n\nThinking of movies you’ve watched, what percentage of all recent movies do you think pass the Bechdel test? Is it closer to 10%, 50%, 80%, or 100%?"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-1",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-1",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction",
    "text": "Introduction\n\nLet \\pi, a random value between 0 and 1, denote the unknown proportion of recent movies that pass the Bechdel test.\nThree friends - the feminist, the clueless, and the optimist - have some prior ideas about \\pi.\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters.\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon.\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test.\n\n\n\nplot_beta(alpha = 1, beta = 1)\nplot_beta(alpha = 5, beta = 11)\nplot_beta(alpha = 14, beta = 1)\n\n\nWhich one is which?"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-2",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-2",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction",
    "text": "Introduction\n\nplot_beta(alpha = 1, beta = 1)\nplot_beta(alpha = 5, beta = 11)\nplot_beta(alpha = 14, beta = 1)\n\n\nWhich one is which?\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters.\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon.\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-3",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-3",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction",
    "text": "Introduction\n\n\nplot_beta(alpha = 1, beta = 1) + theme_bw() + ggtitle(\"Beta(1, 1)\")\n\n\n\n\n\n\n\n\n\n\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-4",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-4",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction",
    "text": "Introduction\n\n\nplot_beta(alpha = 5, beta = 11) + theme_bw() + ggtitle(\"Beta(5, 11)\")\n\n\n\n\n\n\n\n\n\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-5",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-5",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction",
    "text": "Introduction\n\n\nplot_beta(alpha = 14, beta = 1) + theme_bw() + ggtitle(\"Beta(14, 1)\")\n\n\n\n\n\n\n\n\n\n\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-6",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-6",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction",
    "text": "Introduction\n\nThe analysts agree to review a sample of n recent movies and record Y, the number that pass the Bechdel test.\n\nBecause the outcome is yes/no, the binomial distribution is appropriate for the data distribution.\nWe aren’t sure what the population proportion, \\pi, is, so we will not restrict it to a fixed value.\n\nBecause we know \\pi \\in [0, 1], the beta distribution is appropriate for the prior distribution.\n\n\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta)\n\\end{align*}\n\n\nFrom the previous chapter, we know that this results in the following posterior distribution\n\n\n\\pi | (Y=y) \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-7",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-7",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction",
    "text": "Introduction\n\nWait!!\n\nEveryone gets their own prior?\n… is there a “correct” prior?\n…… is the Bayesian world always this subjective?"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-8",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-8",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction",
    "text": "Introduction\n\nWait!!\n\nEveryone gets their own prior?\n… is there a “correct” prior?\n…… is the Bayesian world always this subjective?\n\nMore clearly defined questions that we can actually answer:\n\nTo what extent might different priors lead the analysts to three different posterior conclusions about the Bechdel test?\n\nHow might this depend upon the sample size and outcomes of the movie data they collect?\n\nTo what extent will the analysts’ posterior understandings evolve as they collect more and more data?\nWill they ever come to agreement about the representation of women in film?!"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\n\n\n\n\n\n\n\n\n\n\nThe differing prior means show disagreement about whether \\pi is closer to 0 or 1.\nThe differing levels of prior variability show that the analysts have different degrees of certainty in their prior information.\n\nThe more certain we are about the prior information, the smaller the prior variability."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-1",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-1",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\n\n\n\n\n\n\n\n\n\n\nInformative prior: reflects specific information about the unknown variable with high certainty, i.e., low variability."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-2",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-2",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\n\n\n\n\n\n\n\n\n\n\nVague or diffuse prior: reflects little specific information about the unknown variable.\n\nA flat prior, which assigns equal prior plausibility to all possible values of the variable, is a special case.\nThis is effectively saying “🤷.”"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-3",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-3",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nOkay, great - we have different priors.\n\nHow do the different priors affect the posterior?\n\nWe have data from FiveThirtyEight, reporting results of the Bechdel test.\n\n\nset.seed(65821)\nbechdel20 &lt;- bayesrules::bechdel %&gt;% sample_n(20)\nhead(bechdel20, n = 3)\n\n# A tibble: 3 × 3\n   year title               binary\n  &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt; \n1  2013 Her                 FAIL  \n2  1997 Grosse Pointe Blank PASS  \n3  2006 Volver              PASS"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-4",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-4",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nSo how many pass the test in this sample?\n\n\nbechdel20 %&gt;% tabyl(binary) %&gt;% adorn_totals(\"row\")\n\n binary  n percent\n   FAIL 11    0.55\n   PASS  9    0.45\n  Total 20    1.00"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-5",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-5",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood.\n\n\nplot_beta_binomial(alpha = 5, beta = 11, y = 9, n = 20, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 1, beta = 1, y = 9, n = 20, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 9, n = 20, posterior = FALSE) + theme_bw()\n\n\nQuestions to think about:\n\nWhose posterior do you anticipate will look the most like the scaled likelihood?\nWhose do you anticipate will look the least like the scaled likelihood?"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-6",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-6",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-7",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-7",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-8",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-8",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-9",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-9",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nFind the posterior distributions. (i.e., What are the updated parameters?)\n\n\n\n\n\n\nAnalyst\n\n\nPrior\n\n\nPosterior\n\n\n\n\n\n\nthe feminist\n\n\nBeta(5, 11)\n\n\nBeta(14, 22)\n\n\n\n\nthe clueless\n\n\nBeta(1, 1)\n\n\nBeta(10, 12)\n\n\n\n\nthe optimist\n\n\nBeta(14, 1)\n\n\nBeta(23, 12)\n\n\n\n\n\n\nLet’s now explore what the posteriors look like.\n\n\nplot_beta_binomial(alpha = 5, beta = 11, y = 9, n = 20) + theme_bw()\nplot_beta_binomial(alpha = 1, beta = 1, y = 9, n = 20) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 9, n = 20) + theme_bw()"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-10",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-10",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-11",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-11",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-12",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-12",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-13",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-13",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nIn addition to priors affecting our posterior distributions… the data also affects it.\nLet’s now consider three new analysts: they all share the optimistic Beta(14, 1) for \\pi, however, they have access to different data.\n\nMorteza reviews n = 13 movies from the year 1991, among which Y=6 (about 46%) pass the Bechdel.\nNadide reviews n = 63 movies from the year 2001, among which Y=29 (about 46%) pass the Bechdel.\nUrsula reviews n = 99 movies from the year 2013, among which Y=46 (about 46%) pass the Bechdel.\n\n\n\nplot_beta_binomial(alpha = 14, beta = 1, y = 6, n = 13, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 29, n = 63, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 46, n = 99, posterior = FALSE) + theme_bw()\n\n\nHow will the different data affect the posterior distributions?\n\nWhich posterior will be the most in sync with their data?\nWhich posterior will be the least in sync with their data?"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-14",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-14",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-15",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-15",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-16",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-16",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-17",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-17",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nFind the posterior distributions. (i.e., What are the updated parameters?)\n\nRecall that all use the Beta(14, 1) prior.\n\n\n\n\n\n\n\n\nAnalyst\n\n\n\n\nData\n\n\n\n\nPosterior\n\n\n\n\n\n\n\nMorteza\n\n\nY=6 of n=13\n\n\nBeta(20, 8)\n\n\n\n\nNadide\n\n\nY=29 of n=63\n\n\nBeta(45, 35)\n\n\n\n\nUrsula\n\n\nY=46 of n=99\n\n\nBeta(60, 54)\n\n\n\n\n\n\nLet’s also explore what the posteriors look like.\n\n\nplot_beta_binomial(alpha = 14, beta = 1, y = 6, n = 13) + theme_bw() \nplot_beta_binomial(alpha = 14, beta = 1, y = 29, n = 63) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 46, n = 99) + theme_bw()"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-18",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-18",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-19",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-19",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-20",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-20",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-21",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#different-priors-to-different-posteriors-21",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nWhat did we observe?\n\nAs n \\to \\infty, variance in the likelihood \\to 0.\n\nIn Morteza’s small sample of 13 movies, the likelihood function is wide.\nIn Ursula’s larger sample size of 99 movies, the likelihood function is narrower.\n\nWe see that the narrower the likelihood, the more influence the data holds over the posterior."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#striking-a-balance",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#striking-a-balance",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Striking a Balance",
    "text": "Striking a Balance\n\n\nOverall message: no matter the strength of and discrepancies among their prior understanding of \\pi, analysts will come to a common posterior understanding in light of strong data."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#striking-a-balance-1",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#striking-a-balance-1",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Striking a Balance",
    "text": "Striking a Balance\n\nThe posterior can either favor the data or the prior.\n\nThe rate at which the posterior balance tips in favor of the data depends upon the prior.\n\nLeft to right on the graph, the sample size increases from n=13 to n=99 movies, while preserving the proportion that pass (\\approx 0.46).\n\nThe likelihood’s insistence and the data’s influence over the posterior increase with sample size.\nThis also means that the influence of our prior understanding diminishes as we gather new data.\n\nTop to bottom on the graph, priors move from informative (Beta(14,1)) to vague (Beta(1,1)).\n\nNaturally, the more informative the prior, the greater its influence on the posterior."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-sequentiality",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-sequentiality",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nLet’s now turn our thinking to - okay, we’ve updated our beliefs… but now we have new data!\nThe evolution in our posterior understanding happens incrementally, as we accumulate new data.\n\nScientists’ understanding of climate change has evolved over the span of decades as they gain new information.\nPresidential candidates’ understanding of their chances of winning an election evolve over months as new poll results become available."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-sequentiality-1",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-sequentiality-1",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nLet’s revisit Milgram’s behavioral study of obedience from Chapter 3. Recall, \\pi represents the proportion of people that will obey authority, even if it means bringing harm to others.\nPrior to Milgram’s experiments, our fictional psychologist expected that few people would obey authority in the face of harming another: \\pi \\sim \\text{Beta}(1,10).\nNow, suppose that the psychologist collected the data incrementally, day by day, over a three-day period.\nFind the following posterior distributions, each building off the last:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10.\nDay 2: Y=17 out of n=20.\nDay 3: Y=8 out of n=10."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-sequentiality-2",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#introduction-sequentiality-2",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nLet’s revisit Milgram’s behavioral study of obedience from Chapter 3. Recall, \\pi represents the proportion of people that will obey authority, even if it means bringing harm to others.\nPrior to Milgram’s experiments, our fictional psychologist expected that few people would obey authority in the face of harming another: \\pi \\sim \\text{Beta}(1,10).\nNow, suppose that the psychologist collected the data incrementally, day by day, over a three-day period.\nFind the following posterior distributions, each building off the last:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(2, 19).\nDay 2: Y=17 out of n=20: \\text{Beta}(2, 19) \\to \\text{Beta}(19, 22).\nDay 3: Y=8 out of n=10: \\text{Beta}(19, 22) \\to \\text{Beta}(27, 24).\n\nRecall from Chapter 3, our posterior was \\text{Beta}(27,24)!"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#sequential-bayesian-analysis-or-bayesian-learning",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#sequential-bayesian-analysis-or-bayesian-learning",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning\n\nIn a sequential Bayesian analysis, a posterior model is updated incrementally as more data come in.\n\nWith each new piece of data, the previous posterior model reflecting our understanding prior to observing this data becomes the new prior model.\n\nThis is why we love Bayesian!\n\nWe evolve our thinking as new data come in.\n\nThese types of sequential analyses also uphold two fundamental properties:\n\nThe final posterior model is data order invariant,\n\nThe final posterior only depends upon the cumulative data."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#sequential-bayesian-analysis-or-bayesian-learning-1",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#sequential-bayesian-analysis-or-bayesian-learning-1",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning\n\nIn order:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(2, 19).\nDay 2: Y=17 out of n=20: \\text{Beta}(2, 19) \\to \\text{Beta}(19, 22).\nDay 3: Y=8 out of n=10: \\text{Beta}(19, 22) \\to \\text{Beta}(27, 24).\n\nOut of order:\n\nDay 0: \\text{Beta}(1,10).\nDay 3: Y=8 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(9, 12).\nDay 2: Y=17 out of n=20: \\text{Beta}(9, 12) \\to \\text{Beta}(26, 15).\nDay 1: Y=1 out of n=10: \\text{Beta}(26, 15) \\to \\text{Beta}(27, 24)."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#sequential-bayesian-analysis-or-bayesian-learning-2",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#sequential-bayesian-analysis-or-bayesian-learning-2",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Proving Data Order Invariance",
    "text": "Proving Data Order Invariance\n\nData order invariance:\n\nLet \\theta be any parameter of interest with prior pdf f(\\theta).\nThen a sequential analysis in which we first observe a data point y_1, and then a second data point y_2 will produce the same posterior model of \\theta as if we first observe y_2 and then y_1.\n\n\nf(\\theta|y_1,y_2) = f(\\theta|y_2,y_1)\n\nSimilarly, the posterior model is invariant to whether we observe the data all at once or sequentially."
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance-1",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance-1",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Proving Data Order Invariance",
    "text": "Proving Data Order Invariance\n\nLet’s first specify the structure of posterior pdf f(\\theta|y_1,y_2), which evolves by sequentially observing data y_1, followed by y_2.\nIn step one, we construct the posterior pdf from our original prior pdf, f(\\theta), and the likelihood function of \\theta given the first data point y_1, L(\\theta|y_1).\n\n\n\\begin{align*}\nf(\\theta|y_1) &= \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\\\\n&= \\frac{f(\\theta)L(\\theta|y_1)}{f(y_1)}\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance-2",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance-2",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Proving Data Order Invariance",
    "text": "Proving Data Order Invariance\n\nIn step two, we update our model in light of observing new data, y_2.\n\nDon’t forget that we start from the prior model specified by f(\\theta|y_1).\n\n\n\n\\begin{align*}\nf(\\theta|y_2) &= \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\\\\n&= \\frac{\\frac{f(\\theta)L(\\theta|y_1)}{f(y_1)}L(\\theta|y_2)}{f(y_2)} \\\\\n&= \\frac{f(\\theta)L(\\theta|y_1)L(\\theta|y_2)}{f(y_1)f(y_2)}\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance-3",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance-3",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Proving Data Order Invariance",
    "text": "Proving Data Order Invariance\n\nWhat happens when we observe the data in the opposite order?\n\n\n\\begin{align*}\nf(\\theta|y_2) &= \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\\\\n&= \\frac{f(\\theta)L(\\theta|y_2)}{f(y_2)}\n\\end{align*}\n\n\n\\begin{align*}\nf(\\theta|y_1) &= \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\\\\n&= \\frac{\\frac{f(\\theta)L(\\theta|y_2)}{f(y_2)}L(\\theta|y_1)}{f(y_1)} \\\\\n&= \\frac{f(\\theta)L(\\theta|y_2)L(\\theta|y_1)}{f(y_2)f(y_1)}\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance-4",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance-4",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Proving Data Order Invariance",
    "text": "Proving Data Order Invariance\n\nFinally, not only does the order of the data not influence the ultimate posterior model of \\theta, but it doesn’t matter whether we observe the data all at once or sequentially.\nSuppose we start with the original f(\\theta) prior and observe data (y_1, y_2) together, not sequentially.\nFurther, assume that these data points are independent, thus,\n\nf(y_1, y_2) = f(y_1) f(y_2) \\text{ and } f(y_1,y_2|\\theta) = f(y_1|\\theta) f(y_2|\\theta)"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance-5",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#proving-data-order-invariance-5",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Proving Data Order Invariance",
    "text": "Proving Data Order Invariance\n\nThen, the posterior pdf is the same as the one resulting from sequential analysis,\n\n\n\\begin{align*}\nf(\\theta|y_1,y_2) &= \\frac{f(\\theta)L(\\theta|y_1,y_2)}{f(y_1,y_2)} \\\\\n&= \\frac{f(\\theta)f(y_1,y_2|\\theta)}{f(y_1)f(y_2)} \\\\\n&= \\frac{f(\\theta)f(y_1|\\theta)f(y_2|\\theta)}{f(y_1)f(y_2)} \\\\\n&= \\frac{f(\\theta)L(\\theta|y_1)L(\\theta|y_2)}{f(y_1)f(y_2)}\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W08-L2-balance-and-sequentiality.html#homework",
    "href": "files/lecture/W08-L2-balance-and-sequentiality.html#homework",
    "title": "Balance and Sequentiality in Bayesian Analyses",
    "section": "Homework",
    "text": "Homework\n\n4.3\n4.4\n4.6\n4.9\n4.15\n4.16\n4.17\n4.18\n4.19"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#introduction",
    "href": "files/lecture/W11-L1-approx-posterior.html#introduction",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nWe have learned how to think like a Bayesian:\n\nPrior distribution\nData distribution\nPosterior distribution\n\nWe have learned three conjugate families:\n\nBeta-Binomial (binary outcomes)\nGamma-Poisson (count outcomes)\nNormal-Normal (continuous outcomes)\n\nOnce we have a posterior model, we must be able to apply the results.\n\nPosterior estimation\nHypothesis testing\nPrediction"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#introduction-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#introduction-1",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nRecall, we have the posterior pdf,\n\nf(\\theta|y) = \\frac{f(\\theta) L(\\theta|y)}{f(y)} \\propto f(\\theta)L(\\theta|y)\n\nNow, in the denominator, we need to remember,\n\nf(y) = \\int_{\\theta_1} \\int_{\\theta_2} \\cdot \\cdot \\cdot \\int_{\\theta_k} f(\\theta) L(\\theta|y) d\\theta_k \\cdot \\cdot \\cdot d\\theta_2 d\\theta_1\n\nBecause this is … not fun … we will approximate the posterior via simulation."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#introduction-2",
    "href": "files/lecture/W11-L1-approx-posterior.html#introduction-2",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nWe are going to explore two simulation techniques:\n\ngrid approximation\nMarkov chain Monte Carlo (MCMC)\n\nEither method will produce a sample of N values for \\theta.\n\n\\left \\{ \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)} \\right \\}\n\nThese \\theta_i will have properties that reflect those of the posterior model for \\theta.\nTo help us, we will apply these simulation techniques to Beta-Binomial and Gamma-Poisson models.\n\nNote that these models do not require simulation! We know their posteriors!\nThat’s why we are starting there – we can link the concepts to what we know. :)"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#introduction-3",
    "href": "files/lecture/W11-L1-approx-posterior.html#introduction-3",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nNote: we will use the following packages that may be new to you:\n\njanitor\nrstan\nbayesplot\n\nIf you are using the server provided by HMCSE, they have been installed for you.\nIf you are working at home, please check to see if you have the libraries, then install if you do not.\n\ninstall.packages(c(\"janitor\", \"rstan\", \"bayesplot\"))"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\n\n\n\nSuppose there is an image that you can’t view in its entirety.\nWe can see snippets along a grid that sweeps from left to right across the image.\nThe finer the grid, the clearer the image; if the grid is fine enough, the result is a good approximation."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-1",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\n\n\n\nThis is the general idea behind Bayesian grid approximation.\nOur target image is the posterior pdf, f(\\theta|y).\n\nIt is not necessary to observe all possible f(\\theta|y) \\ \\forall \\theta for us to understand its structure.\nInstead, we evaluate f(\\theta|y) at a finite, discrete grid of possible \\theta values.\nThen, we take random samples from this discretized pdf to approximate the full posterior pdf."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-2",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-2",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nGrid approximation produces a sample of N independent \\theta values, \\left\\{ \\theta^{(1)}, \\theta^{(2)}, \\theta^{(N)} \\right\\}, from a discretized approximation of the posterior pdf, f(\\theta|y).\nAlgorithm:\n\nDefine a discrete grid of possible \\theta values.\nEvaluate the prior pdf, f(\\theta), and the likelihood function, L(\\theta|y) at each \\theta grid value.\nObtain a discrete approximation of the posterior pdf, f(\\theta|y) by:\n\nCalculating the product f(\\theta) L(\\theta|y) at each \\theta grid value,\nNormalize the products from (a) to sum to 1 across all \\theta.\n\nRandomly sample N \\theta grid values with respect to their corresponding normalized posterior probabilities."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation---example",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation---example",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation - Example",
    "text": "Grid Approximation - Example\n\nWe will use the following Beta-Binomial model to learn how to do grid approximation:\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(10, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(2, 2)\n\\end{align*}\n\n\nNote that\n\nY is the number of successes in 10 independent trials.\nEvery trial has probability of success, \\pi.\nOur prior understanding about \\pi is captured by a \\text{Beta}(2,2) model.\n\nIf we observe Y = 9 successes, we know that the updated posterior model for \\pi is \\text{Beta}(11, 3).\n\nY + \\alpha = 9+2\nn - Y + \\beta = 10-9+2"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-3",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-3",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nFirst step: define a discrete grid of possible \\theta values.\n\nSo, let’s consider \\pi \\in \\{0, 0.2, 0.4, 0.6, 0.8, 1\\}.\n\n\n\nlibrary(tidyverse)\ngrid_data &lt;- tibble(pi_grid = seq(from = 0, to = 1, length = 6))"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-4",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-4",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nFirst step: define a discrete grid of possible \\theta values.\n\nSo, let’s consider \\pi \\in \\{0, 0.2, 0.4, 0.6, 0.8, 1\\}.\n\n\n\nlibrary(tidyverse)\ngrid_data &lt;- tibble(pi_grid = seq(from = 0, to = 1, length = 6))\n\n\n\n# A tibble: 3 × 1\n  pi_grid\n    &lt;dbl&gt;\n1     0  \n2     0.2\n3     0.4"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-5",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-5",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nSecond step: evaluate the prior pdf, f(\\theta), and the likelihood function, L(\\theta|y) at each \\theta grid value.\n\nWe will use dbeta() and dbinom() to evaluate the \\text{Beta}(2,2) prior and \\text{Bin}(10, \\pi) likelihood with Y=9 at each \\pi in pi_grid.\n\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-6",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-6",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nSecond step: evaluate the prior pdf, f(\\theta), and the likelihood function, L(\\theta|y) at each \\theta grid value.\n\nWe will use dbeta() and dbinom() to evaluate the \\text{Beta}(2,2) prior and \\text{Bin}(10, \\pi) likelihood with Y=9 at each \\pi in pi_grid.\n\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))\n\n\n\n# A tibble: 3 × 3\n  pi_grid prior likelihood\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1     0    0    0         \n2     0.2  0.96 0.00000410\n3     0.4  1.44 0.00157"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-7",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-7",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nThird step: obtain a discrete approximation of the posterior pdf, f(\\theta|y) by calculating the product f(\\theta) L(\\theta|y) at each \\theta grid value and normalizing the products to sum to 1 across all \\theta.\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-8",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-8",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nThird step: obtain a discrete approximation of the posterior pdf, f(\\theta|y) by calculating the product f(\\theta) L(\\theta|y) at each \\theta grid value and normalizing the products to sum to 1 across all \\theta.\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\nWe can verify,\n\n\ngrid_data %&gt;%\n  summarize(sum(unnormalized), sum(posterior))\n\n# A tibble: 1 × 2\n  `sum(unnormalized)` `sum(posterior)`\n                &lt;dbl&gt;            &lt;dbl&gt;\n1               0.318                1"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-9",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-9",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nThird step: obtain a discrete approximation of the posterior pdf, f(\\theta|y) by calculating the product f(\\theta) L(\\theta|y) at each \\theta grid value and normalizing the products to sum to 1 across all \\theta.\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\n\n# A tibble: 3 × 5\n  pi_grid prior likelihood unnormalized posterior\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1     0    0    0            0          0        \n2     0.2  0.96 0.00000410   0.00000393 0.0000124\n3     0.4  1.44 0.00157      0.00226    0.00712"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-10",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-10",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nWe now have a glimpse into the actual posterior pdf.\n\nWe can plot it to see what it looks like,"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-11",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-11",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nAs we increase the number of possible \\theta values, the better we can “see” the resulting posterior.\nWhat happens if we try the following: n=50, n=100, n=500, n=1000?"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-12",
    "href": "files/lecture/W11-L1-approx-posterior.html#grid-approximation-12",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nAs we increase the number of possible \\theta values, the better we can “see” the resulting posterior.\nWhat happens if we try the following: n=50, n=100, n=500, n=1000?"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#markov-chain-monte-carlo-mcmc",
    "href": "files/lecture/W11-L1-approx-posterior.html#markov-chain-monte-carlo-mcmc",
    "title": "Approximating the Posterior",
    "section": "Markov chain Monte Carlo (MCMC)",
    "text": "Markov chain Monte Carlo (MCMC)\n\nMarkov chain Monte Carlo (MCMC) is an application of Markov chains to simulate probability models.\nMCMC samples are not taken directly from the posterior pdf, f(\\theta | y)… and they are not independent.\n\nEach subsequent value depends on the previous value.\n\nSuppose we have an N-length MCMC sample, \\left\\{ \\theta^{(1)}, \\theta^{(2)}, \\theta^{(3)}, ..., \\theta^{(N)} \\right\\}\n\n\\theta^{(2)} is drawn from a model that depends on \\theta^{(1)}.\n\\theta^{(3)} is drawn from a model that depends on \\theta^{(2)}.\netc."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#markov-chain-monte-carlo-mcmc-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#markov-chain-monte-carlo-mcmc-1",
    "title": "Approximating the Posterior",
    "section": "Markov chain Monte Carlo (MCMC)",
    "text": "Markov chain Monte Carlo (MCMC)\n\nThe (i+1)st chain value, \\theta^{(i+1)} is drawn from a model that depends on data y and the previous chain value, \\theta^{(i)}.\n\nf\\left( \\theta^{(i+1)} | \\theta^{(i)}, y \\right)\n\nIt is important for us to note that the pdf from which a Markov chain is simulated is not equivalent to the posterior pdf!\n\nf\\left( \\theta^{(i+1)} | \\theta^{(i)}, y  \\right) \\ne f\\left(\\theta^{(i+1)}|y \\right)"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#using-rstan",
    "href": "files/lecture/W11-L1-approx-posterior.html#using-rstan",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nWe will use rstan:\n\ndefine the Bayesian model structure in rstan notation\nsimulate the posterior\n\nAgain, we will use the Beta-Binomial model from earlier.\n\ndata: in our example, Y is the observed number of successes in 10 trials.\n\nWe need to tell rstan that Y is an integer between 0 and 10.\n\nparameters: in our example, our model depends on \\pi.\n\nWe need to tell rstan that \\pi can be any real number between 0 and 1.\n\nmodel: in our example, we need to specify Y \\sim \\text{Bin}(10, \\pi) and \\pi \\sim \\text{Beta}(2,2)."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#using-rstan-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#using-rstan-1",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\n# STEP 1: DEFINE the model\nbb_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 10&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  model {\n    Y ~ binomial(10, pi);\n    pi ~ beta(2, 2);\n  }\n\""
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#using-rstan-2",
    "href": "files/lecture/W11-L1-approx-posterior.html#using-rstan-2",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nThen, when we go to simulate, we first put in the model information\n\nmodel code: the character string defining the model (in our case, bb_model).\ndata: a list of the observed data.\n\nIn this example, we are using Y = 9 - a single data point.\n\n\nThen, we put in the Markov chain information,\n\nchains: how many parallel Markov chains to run.\n\nThis will be the number of distinct \\theta values we want.\n\niter: desired number of iterations, or length of Markov chain.\n\nHalf are thrown out as “burn in” samples.\n\n“burn in”? Think: pancakes!\n\n\nseed: used to set the seed of the RNG."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#using-rstan-3",
    "href": "files/lecture/W11-L1-approx-posterior.html#using-rstan-3",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\n# STEP 2: simulate the posterior\nbb_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n               chains = 4, iter = 5000*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 1:                0.013 seconds (Sampling)\nChain 1:                0.025 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 2:                0.013 seconds (Sampling)\nChain 2:                0.025 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 3:                0.012 seconds (Sampling)\nChain 3:                0.024 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 4:                0.013 seconds (Sampling)\nChain 4:                0.025 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#using-rstan-4",
    "href": "files/lecture/W11-L1-approx-posterior.html#using-rstan-4",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nNow, we need to extract the values,\n\n\nas.array(bb_sim, pars = \"pi\") %&gt;% head(4)\n\n, , parameters = pi\n\n          chains\niterations   chain:1   chain:2   chain:3   chain:4\n      [1,] 0.8278040 0.7523560 0.6386998 0.9627572\n      [2,] 0.9087546 0.8922386 0.6254761 0.9413518\n      [3,] 0.6143859 0.8682356 0.7222360 0.9489624\n      [4,] 0.8462156 0.8792812 0.8100192 0.9413812\n\n\n\nRemember, these are not a random sample from the posterior!\nThey are also not independent!\nEach chain forms a dependent 5,000 length Markov chain of \\left\\{ \\pi^{(1)}, \\pi^{(2)}, ..., \\pi^{(5000)}\\right\\}\n\nEach chain will move along the sample space of plausible values for \\pi."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#using-rstan-5",
    "href": "files/lecture/W11-L1-approx-posterior.html#using-rstan-5",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nWe will look at the trace plot (using mcmc_trace() from bayesplot package) to see what the values did longitudinally.\n\n\n\nmcmc_trace(bb_sim, pars = \"pi\", size = 0.1)"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#using-rstan-6",
    "href": "files/lecture/W11-L1-approx-posterior.html#using-rstan-6",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nWe can also look at the mcmc_hist() and mcmc_dens() functions,"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#diagnostics",
    "href": "files/lecture/W11-L1-approx-posterior.html#diagnostics",
    "title": "Approximating the Posterior",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nSimulations are not perfect…\n\nWhat does a good Markov chain look like?\nHow can we tell if the Markov chain sample produces a reasonable approximation of the posterior?\nHow big should our Markov chain sample size be?\n\nUnfortunately there is no one answer here.\n\nYou will learn through experience, much like other nuanced areas of statistics."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#diagnostics-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#diagnostics-1",
    "title": "Approximating the Posterior",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nLet’s now discuss diagnostic tools.\n\nTrace plots\nParallel chains\nEffective sample size\nAutocorrelation\n\\hat{R}"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#trace-plots",
    "href": "files/lecture/W11-L1-approx-posterior.html#trace-plots",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\n\n\nChain A has not stabilized after 5000 iterations.\n\nIt has not “found” or does not know how to explore the range of posterior plausible \\pi values.\nThe downward trend also hints against independent noise."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#trace-plots-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#trace-plots-1",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\n\n\nWe say that Chain A is mixing slowly.\n\nThe more Markov chains behave like fast mixing (noisy) independent samples, the smaller the error in the resulting posterior approximation."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#trace-plots-2",
    "href": "files/lecture/W11-L1-approx-posterior.html#trace-plots-2",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\n\n\nChain B is not great, either – it gets stuck when looking at a smaller value of \\pi."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#trace-plots-3",
    "href": "files/lecture/W11-L1-approx-posterior.html#trace-plots-3",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\nRealistically, we are only going to do simulations when we can’t specify the posterior and must approximate\n\ni.e., we won’t be able to compare the simulation results to the “true” results.\n\nIf we see bad trace plots:\n\nCheck the model (… or your code). Are the assumed prior and data models appropriate?\nRun the chain for more iterations. Sometimes we just need a longer run to iron out issues."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#parallel-chains",
    "href": "files/lecture/W11-L1-approx-posterior.html#parallel-chains",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nLet’s now consider a smaller simulation, where n=50 (recall, overall n=100, but half is for burn-in).\n\n\nbb_sim_short &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                     chains = 4, iter = 50*2, seed = 84735)"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#parallel-chains-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#parallel-chains-1",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nLet’s now consider a smaller simulation, where n=50 (recall, overall n=100, but half is for burn-in).\n\n\nbb_sim_short &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                     chains = 4, iter = 50*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: WARNING: There aren't enough warmup iterations to fit the\nChain 1:          three stages of adaptation as currently configured.\nChain 1:          Reducing each adaptation stage to 15%/75%/10% of\nChain 1:          the given number of warmup iterations:\nChain 1:            init_buffer = 7\nChain 1:            adapt_window = 38\nChain 1:            term_buffer = 5\nChain 1: \nChain 1: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 1: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 1: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 1: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 1: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 1: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 1: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 1: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 1: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 1: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 1: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 1: Iteration: 100 / 100 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0 seconds (Warm-up)\nChain 1:                0 seconds (Sampling)\nChain 1:                0 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: WARNING: There aren't enough warmup iterations to fit the\nChain 2:          three stages of adaptation as currently configured.\nChain 2:          Reducing each adaptation stage to 15%/75%/10% of\nChain 2:          the given number of warmup iterations:\nChain 2:            init_buffer = 7\nChain 2:            adapt_window = 38\nChain 2:            term_buffer = 5\nChain 2: \nChain 2: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 2: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 2: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 2: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 2: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 2: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 2: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 2: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 2: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 2: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 2: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 2: Iteration: 100 / 100 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0 seconds (Warm-up)\nChain 2:                0 seconds (Sampling)\nChain 2:                0 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: WARNING: There aren't enough warmup iterations to fit the\nChain 3:          three stages of adaptation as currently configured.\nChain 3:          Reducing each adaptation stage to 15%/75%/10% of\nChain 3:          the given number of warmup iterations:\nChain 3:            init_buffer = 7\nChain 3:            adapt_window = 38\nChain 3:            term_buffer = 5\nChain 3: \nChain 3: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 3: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 3: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 3: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 3: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 3: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 3: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 3: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 3: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 3: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 3: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 3: Iteration: 100 / 100 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0 seconds (Warm-up)\nChain 3:                0 seconds (Sampling)\nChain 3:                0 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: WARNING: There aren't enough warmup iterations to fit the\nChain 4:          three stages of adaptation as currently configured.\nChain 4:          Reducing each adaptation stage to 15%/75%/10% of\nChain 4:          the given number of warmup iterations:\nChain 4:            init_buffer = 7\nChain 4:            adapt_window = 38\nChain 4:            term_buffer = 5\nChain 4: \nChain 4: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 4: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 4: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 4: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 4: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 4: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 4: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 4: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 4: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 4: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 4: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 4: Iteration: 100 / 100 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0 seconds (Warm-up)\nChain 4:                0 seconds (Sampling)\nChain 4:                0 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#parallel-chains-2",
    "href": "files/lecture/W11-L1-approx-posterior.html#parallel-chains-2",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nLet’s now consider a smaller simulation, where n=50 (recall, overall n=100, but half is for burn-in)."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#parallel-chains-3",
    "href": "files/lecture/W11-L1-approx-posterior.html#parallel-chains-3",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nNow you try 10,000 iterations."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#parallel-chains-4",
    "href": "files/lecture/W11-L1-approx-posterior.html#parallel-chains-4",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nNow you try 10,000 iterations."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#effective-sample-size",
    "href": "files/lecture/W11-L1-approx-posterior.html#effective-sample-size",
    "title": "Approximating the Posterior",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\n\nThe more a dependent Markov chain behaves like an independent sample, the smaller the error in the resulting posterior approximation.\n\nPlots are great, but numerical assessment can provide more nuanced information.\n\nEffective sample size (N_{\\text{eff}}): the number of independent sample values it would take to produce an equivalently accurate posterior approximation.\nEffective sample size ratio:\n\n\\frac{N_{\\text{eff}}}{N}\n\nGenerally, we look for the effective sample size, N_{\\text{eff}}, to be greater than 10% of the actual sample size, N."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#effective-sample-size-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#effective-sample-size-1",
    "title": "Approximating the Posterior",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\n\nWe will use the neff_ratio() function to find this ratio.\nIn our example data,\n\n\n# Calculate the effective sample size ratio - N = 50\nneff_ratio(bb_sim, pars = c(\"pi\"))\n\n[1] 0.3462257\n\n# Calculate the effective sample size ratio - N = 10000\nneff_ratio(bb_sim_short, pars = c(\"pi\"))\n\n[1] 0.4950171\n\n\n\nBecause the N_{\\text{eff}} is over 10%, we are not concerned and can proceed."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#autocorrelation",
    "href": "files/lecture/W11-L1-approx-posterior.html#autocorrelation",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocorrelation allows us to evaluate if our Markov chain sufficiently mimics the behavior of an independent sample.\nAutocorrelation:\n\nLag 1 autocorrelation measures the correlation between pairs of Markov chain values that are one “step” apart (e.g., \\pi_i and \\pi_{(i-1)}; e.g., \\pi_4 and \\pi_3).\nLag 2 autocorrelation measures the correlation between pairs of Markov chain values that are two “steps apart (e.g., \\pi_i and \\pi_{(i-2)}; e.g., \\pi_4 and \\pi_2).\nLag k autocorrelation measures the correlation between pairs of Markov chain values that are k “steps” apart (e.g., \\pi_i and \\pi_{(i-k)}; e.g., \\pi_4 and \\pi_{(4-k)}).\n\nStrong autocorrelation or dependence is a bad thing.\n\nIt goes hand in hand with small effective sample size ratios.\nThese provide a warning sign that our resulting posterior approximations might be unreliable."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#autocorrelation-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#autocorrelation-1",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\n\n\nNo obvious patterns in the trace plot; dependence is relatively weak.\nAutocorrelation plot quickly drops off and is effectively 0 by lag 5.\nConfirmation that our Markov chain is mixing quickly.\n\ni.e., quickly moving around the range of posterior plausible \\pi values\ni.e., at least mimicking an independent sample."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#autocorrelation-2",
    "href": "files/lecture/W11-L1-approx-posterior.html#autocorrelation-2",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\n\n\nThis is an “unhealthy” Markov chain.\nTrace plot shows strong trends \\to autocorrelation in the Markov chain values.\nSlow decrease in autocorrelation plot indicates that the dependence between chain values does not quickly fade away.\n\nAt lag 20, the autocorrelation is still \\sim 90%."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#fast-vs.-slow-mixing-markov-chains",
    "href": "files/lecture/W11-L1-approx-posterior.html#fast-vs.-slow-mixing-markov-chains",
    "title": "Approximating the Posterior",
    "section": "Fast vs. Slow Mixing Markov Chains",
    "text": "Fast vs. Slow Mixing Markov Chains\n\nFast mixing chains:\n\nThe chains move “quickly” around the range of posterior plausible values\nThe autocorrelation among the chain values drops off quickly.\nThe effective sample size ratio is reasonably large.\n\nSlow mixing chains:\n\nThe chains move “slowly” around the range of posterior plauslbe values.\nThe autocorrelation among the chainv alues drops off very slowly.\nThe effective sample size ratio is small.\n\nWhat do we do if we have a slow mixing chain?\n\nIncrease the chain size :)\nThin the Markov chain :|"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#thinning-markov-chains",
    "href": "files/lecture/W11-L1-approx-posterior.html#thinning-markov-chains",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nLet’s thin our original results, bb_sim, to every tenth value using the thin argument in stan().\n\n\nthinned_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                    chains = 4, iter = 5000*2, seed = 84735, thin = 10)\n\nmcmc_trace(thinned_sim, pars = \"pi\")\nmcmc_acf(thinned_sim, pars = \"pi\")"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#thinning-markov-chains-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#thinning-markov-chains-1",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nLet’s thin our original results, bb_sim, to every tenth value using the thin argument in stan().\n\n\nthinned_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                    chains = 4, iter = 5000*2, seed = 84735, thin = 10)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 1:                0.011 seconds (Sampling)\nChain 1:                0.02 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 2:                0.011 seconds (Sampling)\nChain 2:                0.02 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 3:                0.009 seconds (Sampling)\nChain 3:                0.019 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 4:                0.011 seconds (Sampling)\nChain 4:                0.02 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#thinning-markov-chains-2",
    "href": "files/lecture/W11-L1-approx-posterior.html#thinning-markov-chains-2",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nLet’s thin our original results, bb_sim, to every tenth value using the thin argument in stan()."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#thinning-markov-chains-3",
    "href": "files/lecture/W11-L1-approx-posterior.html#thinning-markov-chains-3",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nWarning!\n\nThe benefits of reduced autocorrelation do not necessarily outweigh the loss of chain values.\ni.e., 5,000 Markov chain values with stronger autocorrelation may be a better posterior approximation than 500 chain values with weaker autocorrelation.\n\nThe effectiveness depends on the algorithm used to construct the Markov chain.\n\nFolks advise against thinning unless you need memory space on your computer."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#hatr",
    "href": "files/lecture/W11-L1-approx-posterior.html#hatr",
    "title": "Approximating the Posterior",
    "section": "\\hat{R}",
    "text": "\\hat{R}\n\\hat{R} \\approx \\sqrt{\\frac{\\text{var}_{\\text{combined}}}{\\text{var}_{\\text{within}}}}\n\nwhere\n\n\\text{var}_{\\text{combined}} is the variability in \\theta across all chains combined.\n\\text{var}_{\\text{within}} is the typical variability within any individual chain.\n\n\\hat{R} compares the variability in sampled \\theta values across all chains combined with the variability within each individual change.\n\nIdeally, \\hat{R} \\approx 1, showing stability across chains.\n\\hat{R} &gt; 1 indicates instability with the variability in the combined chains larger than that of the variability within the chains.\n\\hat{R} &gt; 1.05 raises red flags about the stability of the simulation."
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#hatr-1",
    "href": "files/lecture/W11-L1-approx-posterior.html#hatr-1",
    "title": "Approximating the Posterior",
    "section": "\\hat{R}",
    "text": "\\hat{R}\n\nWe can use the rhat() function from the bayesplot package to find \\hat{R}.\n\n\nrhat(bb_sim, pars = \"pi\")\n\n[1] 1.000245\n\n\n\nWe can see that our simulation is stable.\nIf we were to find \\hat{R} for the other (obviously bad) simulation, it would be 5.35 😱"
  },
  {
    "objectID": "files/lecture/W11-L1-approx-posterior.html#homework",
    "href": "files/lecture/W11-L1-approx-posterior.html#homework",
    "title": "Approximating the Posterior",
    "section": "Homework",
    "text": "Homework\n\n6.5\n6.6\n6.7\n6.13\n6.14\n6.15\n6.17"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#introduction",
    "href": "files/lecture/W05-L2-continuous-rv.html#introduction",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "Introduction",
    "text": "Introduction\n\nThe first few lectures come from Mathematical Statistics with Applications, by Wackerly.\n\nWe must understand the underlying probability and random variable theory before moving into the Bayesian world.\n\nWe will be covering the following chapters:\n\nChapter 2: probability theory\nChapter 3: discrete random variables\nChapter 4: continuous random variables"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv",
    "href": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nDistribution function:\n\nLet Y denote any random variable. The distribution function of Y, denoted by F(y), is such that\n\n\nF(y) = P[Y \\le y] \\text{ for } -\\infty &lt; y &lt; \\infty\n\nTheorem: Properties of a Distribution Function\n\nIf F(y) is a distribution function, then\n\nF(-\\infty)   \\equiv \\underset{y \\to -\\infty}{\\lim} F(y) = 0\nF(\\infty)    \\equiv \\underset{y \\to \\infty}{\\lim} F(y) = 1\nF(y) is a nondecreasing function of y.\n\nIf y_1 and y_2 are any values such that y_1 &lt; y_2, then F(y_1) \\le F(y_2).\n\n\n\nNote: To be rigorous, if F(y) is a valid distribution function, then F(y) also must be right continuous."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-1",
    "href": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-1",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nContinuous random variable:\n\nA random variable Y with distribution function F(y) is said to be continuous if F(y) is continuous for -\\infty &lt; y &lt; \\infty.\n\nNote: To be mathematically precise, we also need the first derivative of F(y) to exist and be continuous.\n\n\nIf Y is a continuous random variable, then for any real number y, P[Y = y] = 0.\n\ni.e., we must remember to find the probability of an interval."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-2",
    "href": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-2",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nProbability density function:\n\nLet F(y) be the cumulative density function for a continuous random variable, Y. Then\n\n\np[Y = y] = f(y) = \\frac{dF(y)}{dy} = F'(y).\n\nTheorem: Properties of a Density Function\n\nIf f(y) is a density function for a continuous random variable, then\n\nf(y) \\ge 0 \\ \\forall y, \\ -\\infty &lt; y &lt; \\infty.\n\\int_{-\\infty}^{\\infty} f(y) dy = 1."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-3",
    "href": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-3",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nCumulative density function:\n\nLet f(y) be the probability density function for a continuous random variable, Y. Then\n\n\nP[Y \\le y] = F(y) = \\int_{-\\infty}^y f(t) dt, \\text{ for } y \\in {\\rm I\\!R}."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-4",
    "href": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-4",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nSuppose that \nF(x) =\n\\begin{cases}\n0, & y &lt; 0 \\\\\ny, & 0 \\leq y \\leq 1 \\\\\n1, & y &gt; 1\n\\end{cases}\n\nFind the probability density function for Y."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-5",
    "href": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-5",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nTheorem\n\nIf the random variable Y has density function f(y) and a &lt; b, then the probability that Y falls in the interval [a, b] is\n\n\n\nP[a \\le Y \\le b] = \\int_a^b f(y) dy."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-6",
    "href": "files/lecture/W05-L2-continuous-rv.html#probability-distributions-for-continuous-rv-6",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nGiven f(y) = cy^2, where 0 \\le y \\le 2 and f(y)=0 elsewhere,\n\nFind the value of c for which f(y) is a valid density function. \nFind P[1 \\le Y \\le 2]."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#expected-values-for-continuous-rv",
    "href": "files/lecture/W05-L2-continuous-rv.html#expected-values-for-continuous-rv",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.3: Expected Values for Continuous RV",
    "text": "4.3: Expected Values for Continuous RV\n\nExpected value:\n\nThe expected value of a continuous variable Y is\n\n\n\nE[Y] = \\int_{-\\infty}^{\\infty} y f(y) \\ dy\n\n\nThis is the continuous version of the expected mean for a discrete random variable,\n\n\nE[Y] = \\sum_y y p(y)"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#expected-values-for-continuous-rv-1",
    "href": "files/lecture/W05-L2-continuous-rv.html#expected-values-for-continuous-rv-1",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.3: Expected Values for Continuous RV",
    "text": "4.3: Expected Values for Continuous RV\n\nTheorem:\n\nLet g(Y) be a function of Y; then the expected value of g(Y) is given by\n\n\n\nE\\left[ g(Y) \\right] = \\int_{-\\infty}^{\\infty} g(y) f(y) \\ dy\n\n\nTheorem:\n\nLet c be a constant and let g(Y), g_1(Y), g_2(Y), …, g_k(Y) be functions of a continuous random variable, Y. Then the following results hold:\n\nE[c] = c\nE\\left[cg(Y)] = cE[g(Y)\\right]\nE\\left[g_1(Y)+g_2(Y)+...+g_k(Y)\\right] = E\\left[ g_1(Y) \\right] + E\\left[ g_2(Y) \\right] + ... + E\\left[ g_k(Y) \\right]"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#expected-values-for-continuous-rv-2",
    "href": "files/lecture/W05-L2-continuous-rv.html#expected-values-for-continuous-rv-2",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.3: Expected Values for Continuous RV",
    "text": "4.3: Expected Values for Continuous RV\n\nIn a previous example, we determined that f(y) = \\frac{3}{8}y^2 for 0 \\le y \\le 2 and f(y) = 0 elsewhere is a valid density function.\nIf the random variable Y has this density function, find \\mu = E[y] and \\sigma^2 = V[Y]"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#uniform-probability-distribution",
    "href": "files/lecture/W05-L2-continuous-rv.html#uniform-probability-distribution",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.4: Uniform Probability Distribution",
    "text": "4.4: Uniform Probability Distribution\n\nUniform Distribution"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#uniform-probability-distribution-1",
    "href": "files/lecture/W05-L2-continuous-rv.html#uniform-probability-distribution-1",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.4: Uniform Probability Distribution",
    "text": "4.4: Uniform Probability Distribution\n\nUniform Distribution\n\nIf \\theta_1 &lt; \\theta_2, a random variable Y is said to have a uniform distribution on the interval (\\theta_1, \\theta_2) iff\n\n\n\nf(y) = \\begin{cases}\n      \\frac{1}{\\theta_2 - \\theta_1}, & \\theta_1 \\le y \\le \\theta_2 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nTheorem:\n\nIf \\theta_1 &lt; \\theta_2 and Y is a random variable uniformly distributed on the interval (\\theta_1, \\theta_2), then\n\n\n\nE[Y] = \\mu = \\frac{\\theta_1+\\theta_2}{2} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = \\frac{(\\theta_2-\\theta_1)^2}{12}\n\n\nSee Wackerly pg. 176 for derivation."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#uniform-probability-distribution-2",
    "href": "files/lecture/W05-L2-continuous-rv.html#uniform-probability-distribution-2",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.4: Uniform Probability Distribution",
    "text": "4.4: Uniform Probability Distribution\n\nThe change in depth of a river from one day to the next, measured in feet, at a specific location is a random variable, Y, with the following density function:\n\n\nf(y) = \\begin{cases}\n      k, & -2 \\le y \\le 2 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nDetermine the value of k. \nFind the distribution function for Y."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#uniform-probability-distribution-3",
    "href": "files/lecture/W05-L2-continuous-rv.html#uniform-probability-distribution-3",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.4: Uniform Probability Distribution",
    "text": "4.4: Uniform Probability Distribution\n\nThe change in depth of a river from one day to the next, measured in feet, at a specific location is a random variable, Y, with the following density function:\n\n\nf(y) = \\begin{cases}\n      k, & -2 \\le y \\le 2 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nWhat is the mean of the distribution? \nWhat is the variance of the distribution?"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#normal-probability-distribution",
    "href": "files/lecture/W05-L2-continuous-rv.html#normal-probability-distribution",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.5: Normal Probability Distribution",
    "text": "4.5: Normal Probability Distribution\n\nNormal Distribution"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#normal-probability-distribution-1",
    "href": "files/lecture/W05-L2-continuous-rv.html#normal-probability-distribution-1",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.5: Normal Probability Distribution",
    "text": "4.5: Normal Probability Distribution\n\nNormal Distribution\n\nA random variable Y is said to have a normal distribution iff, for \\sigma &gt; 0 and -\\infty &lt; \\mu &lt; \\infty,\n\n\n\nf(y) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(y-\\mu)^2/(2\\sigma^2)}\n\n\nTheorem:\n\nIf Y is a random variable normally distributed with parameters \\mu and \\sigma, then\n\n\n\nE[Y] = \\mu =  \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#normal-probability-distribution-2",
    "href": "files/lecture/W05-L2-continuous-rv.html#normal-probability-distribution-2",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.5: Normal Probability Distribution",
    "text": "4.5: Normal Probability Distribution\n\nThe weekly amount of money spent on maintenance and repairs by a company was observed over a long period of time to be approximately normally distributed with mean $400 and standard deviation $20.\n\n\nIf $450 is budgeted for next week, what is the probability that the actual costs will exceed the budgeted amount? \nHow much should be budgeted for weekly repairs and maintenance to provide that the probability the budgeted amount will be exceeded in a given week is only 0.10?"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#gamma-probability-distribution",
    "href": "files/lecture/W05-L2-continuous-rv.html#gamma-probability-distribution",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.6: Gamma Probability Distribution",
    "text": "4.6: Gamma Probability Distribution\n\nGamma Distribution"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#gamma-probability-distribution-1",
    "href": "files/lecture/W05-L2-continuous-rv.html#gamma-probability-distribution-1",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.6: Gamma Probability Distribution",
    "text": "4.6: Gamma Probability Distribution\n\nGamma Distribution\n\nA random variable Y is said to have a gamma distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\n\nf(y) = \\begin{cases}\n      \\frac{y^{\\alpha-1} e^{-y/\\beta}}{\\beta^{\\alpha} \\Gamma(\\alpha)}, & 0 \\le y &lt; \\infty \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nNote that \\Gamma(\\alpha) = \\int_{0}^{\\infty} y^{\\alpha-1} e^{-y} \\ dy.\nTheorem:\n\nIf Y has a gamma distribution with parameters \\alpha and \\beta, then\n\n\n\nE[Y] = \\mu = \\alpha\\beta \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\alpha\\beta^2\n\n\nSee Wackerly pg. 187 for derivation."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#gamma-probability-distribution-2",
    "href": "files/lecture/W05-L2-continuous-rv.html#gamma-probability-distribution-2",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.6: Gamma Probability Distribution",
    "text": "4.6: Gamma Probability Distribution\n\nAnnual incomes for heads of household in a section of a city have approximately a gamma distribution with \\alpha=20 and \\beta=1000.\n\nWhat is f(y)?\nWhat is the mean of Y?\nWhat is the variance of Y?\nWhat proportion of heads of households in this section of the city have incomes in excess of $30,000?"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#beta-probability-distribution",
    "href": "files/lecture/W05-L2-continuous-rv.html#beta-probability-distribution",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.7: Beta Probability Distribution",
    "text": "4.7: Beta Probability Distribution\n\nBeta Distribution"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#beta-probability-distribution-1",
    "href": "files/lecture/W05-L2-continuous-rv.html#beta-probability-distribution-1",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.7: Beta Probability Distribution",
    "text": "4.7: Beta Probability Distribution\n\nBeta Distribution\n\nA random variable Y is said to have a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\n\nf(y) = \\begin{cases}\n      \\frac{y^{\\alpha-1}(1-y)^{\\beta-1}}{B(\\alpha,\\beta)}, & 0 \\le y \\le 1 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nNote that B(\\alpha,\\beta) = \\int_0^1 y^{\\alpha-1}(1-y)^{\\beta-1} \\ dy = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}.\nTheorem:\n\nIf Y has a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0, then\n\n\n\nE[Y] = \\mu = \\frac{\\alpha}{\\alpha+\\beta} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}"
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#beta-probability-distribution-2",
    "href": "files/lecture/W05-L2-continuous-rv.html#beta-probability-distribution-2",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.7: Beta Probability Distribution",
    "text": "4.7: Beta Probability Distribution\n\nThe percentage of impurities per batch in a chemical product is a random variable Y with density function\n\n\nf(y) = \\begin{cases}\n      12y^2(1-y), & 0 \\le y \\le 1 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nA batch with more than 40% impurities cannot be sold.\n\n\nUse integration to determine the probability that a randomly selected batch cannot be sold because of excessive impurities. \nUse R to find the answer to part (a)."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#beta-probability-distribution-3",
    "href": "files/lecture/W05-L2-continuous-rv.html#beta-probability-distribution-3",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "4.7: Beta Probability Distribution",
    "text": "4.7: Beta Probability Distribution\n\nThe percentage of impurities per batch in a chemical product is a random variable Y with density function\n\n\nf(y) = \\begin{cases}\n      12y^2(1-y), & 0 \\le y \\le 1 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nA batch with more than 40% impurities cannot be sold.\n\n\nFind the mean of the percentage of impurities in a randomly selected batch of the chemical. \nFind the variance of the percentage of impurities in a randomly selected batch of the chemical."
  },
  {
    "objectID": "files/lecture/W05-L2-continuous-rv.html#homework",
    "href": "files/lecture/W05-L2-continuous-rv.html#homework",
    "title": "Continuous Random Variables and Their Probability Distributions",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\n\n4.11\n4.14\n4.17\n4.21\n4.28\n4.45\n4.46\n4.48\n4.68\n\n\n\n \n\n\n\n4.69\n4.70\n4.97\n4.98\n4.102\n4.124\n4.125\n4.128\n4.131"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#introduction",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#introduction",
    "title": "Introduction to Probability",
    "section": "Introduction",
    "text": "Introduction\n\nThe first few lectures come from Mathematical Statistics with Applications, by Wackerly.\n\nWe must understand the underlying probability and random variable theory before moving into the Bayesian world.\n\nWe will be covering the following chapters:\n\nChapter 2: probability theory\nChapter 3: discrete random variables\nChapter 4: continuous random variables"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#probability-and-inference",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#probability-and-inference",
    "title": "Introduction to Probability",
    "section": "2.2: Probability and Inference",
    "text": "2.2: Probability and Inference\n\nIn statistics we use probabilities to make inference, or draw conclusions.\nConsider a gambler who wishes to make an inference concerning the balance, or fairness, of a die.\n\nIf the die were perfectly balanced, one-sixth of the measurements in this population would be 1s, one-sixth would be 2s, one-sixth would be 3s, etc."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#probability-and-inference-1",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#probability-and-inference-1",
    "title": "Introduction to Probability",
    "section": "2.2: Probability and Inference",
    "text": "2.2: Probability and Inference\n\nUsing the scientific method, the gambler proposes the hypothesis that the die is balanced, and he seeks observations from nature to contradict the theory, if false.\n\nA sample of ten tosses is selected from the population by rolling the die ten times.\nAll ten tosses result in 1s. 🧐\nThe gambler looks upon this output and concludes that his hypothesis is not in agreement with nature and, thus, the die is not balanced.\n\nThe gambler rejected his hypothesis not because it is impossible to throw ten 1s in ten tosses of a balanced die, but because it is highly improbable."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nSuppose the elements a_1, a_2, and a_3 are in the set A, we will write A = \\left\\{ a_1, a_2, a_3 \\right\\}\n\nNotation: capital letters \\to sets of points.\n\nWe can denote the set of all elements under consideration with S, the universal set."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-1",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-1",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nA \\subset B:\n\nFor any two sets A and B, we say that A is a subset of B if every point in A is also in B."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-2",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-2",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nA \\cup B:\n\nThe union of A and B is the set of all points in either A or B.\ni.e., the union has all points that are in at least one of the sets."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-3",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-3",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nA \\cap B:\n\nThe intersection of A and B is the set of all points in both A and B.\ni.e., the intersection is where the two sets overlap."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-4",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-4",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\n\\bar{A} or A^c:\n\nThe complement of A is the set of points that are in S, but not in A.\nA \\cup \\bar{A} = S."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-5",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-5",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nA \\cap B = \\emptyset:\n\nA and B, are said to be disjoint or mutually exclusive when they have no points in common.\nRelated: for any set A, we know that A and \\bar{A} are mutually exclusive."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-6",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-6",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nFast forwarding through set algebra, we need to know these distributive laws:\n\n\n\\begin{align*}\nA \\cap (B \\cup C) &= (A \\cap B) \\cup (A \\cap C) \\\\\nA \\cup (B \\cap C) &= (A \\cup B) \\cap (A \\cup C) \\\\\n\\overline{(A \\cap B)} &= \\overline{A} \\cup \\overline{B} \\\\\n\\overline{(A \\cup B)} &= \\overline{A} \\cap \\overline{B}\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-7",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-7",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nSuppose two dice are tossed and the numbers on the upper faces are observed.\nLet S denote the set of all possible pairs that can be observed.\n\ne.g., (2, 3) indicates that a 2 was observed on the first die and a 3 on the second.\n\nDefine the following subsets of S and list their points:\n\nA: The number on the second die is even.\nB: The sum of the two numbers is even.\nC: At least one number in the pair is odd.\n\nList the points in the following:\n\nA \\cup B\nA \\cap \\bar{B}\n\\bar{A} \\cap C"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-8",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#review-of-set-notation-8",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nDefine the following subsets of S and list their points:\n\nA: The number on the second die is even. \nB: The sum of the two numbers is even.\nC: At least one number in the pair is odd.\n\nList the points in the following:\n\nA \\cup B\nA \\cap \\bar{B}\n\\bar{A} \\cap C"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#a-probabilistic-model-for-an-experiment",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#a-probabilistic-model-for-an-experiment",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nExperiment: the process by which an observation is made.\n\nExamples:\n\ncoin and die tossing,\nmeasuring the systolic blood pressure of an individual,\ndetermine the number of bacteria per cubic centimeter in a serving of processed food.\n\n\nEvents: the outcomes possible in an experiment.\n\nNotation: capital leters\nExamples for bacteria observation:\n\nA: Exactly 110 bacteria are present.\nB: More than 200 bacteria are present.\nC: The number of bacteria present is between 100 and 300."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#a-probabilistic-model-for-an-experiment-1",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#a-probabilistic-model-for-an-experiment-1",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nSample space, S: the set consisting of all possible sample points.\nThe following Venn diagram shows the six simple events in S,\n\nS = \\{E_1, E_2, E_3, E_4, E_5, E_6 \\}"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#a-probabilistic-model-for-an-experiment-2",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#a-probabilistic-model-for-an-experiment-2",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nCompound event: A collection of sample points in a discrete sample space, S.\n\ni.e., any subset of S.\n\ne.g., suppose we define two events, A and B,"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#a-probabilistic-model-for-an-experiment-3",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#a-probabilistic-model-for-an-experiment-3",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nSuppose S is a sample space associated with an experiment.\nTo every event A in S (i.e., A \\subset S), we assign a number, P[A], called the probability of A, such that the follow axioms hold:\n\nAxiom 1: P[A] \\ge 0.\nAxiom 2: P[S] = 1.\nAxiom 3: If A_1, A_2, ..., A_n form a sequence of pairwise mutually exclusive events in S\n\nRemember, mutually exclusive: A_i \\cap A_j = \\emptyset if i \\ne j\n\n\n\nP[A_1 \\cup A_2 \\cup \\ ... \\cup \\ A_n] = \\sum_{i=1}^n P[A_i]"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#a-probabilistic-model-for-an-experiment-4",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#a-probabilistic-model-for-an-experiment-4",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nSuppose a sample space consists of five simple events, E_1, E_2, E_3, E_4, and E_5.\n\nIf P[E_1] = P[E_2] = 0.15, P[E_3] = 0.4, and P[E_4] = 2P[E_5], find the probabilities of E_4 and E_5. \nIf P[E_1] = 3P[E_2] = 0.3, find the probabilities of the remaining simple events if you know that the remaining simple events are equally probable."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#calculating-the-probability-of-an-event",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#calculating-the-probability-of-an-event",
    "title": "Introduction to Probability",
    "section": "2.5: Calculating the Probability of an Event",
    "text": "2.5: Calculating the Probability of an Event\n\nThe following are steps used to find the probability of an event:\n\nDefine the experiment and clearly determine how to describe one simple event.\nDefine S: list the simple events associated with the experiment.\nAssign reasonable probabilities to the sample points in S; remember that P[E_i] \\ge 0 and \\sum_i P[E_i] = 1.\nDefine the event of interest, A, as a specific collection of sample points.\nFind P[A] by summing the probabilities of the sample points in A."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#calculating-the-probability-of-an-event-1",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#calculating-the-probability-of-an-event-1",
    "title": "Introduction to Probability",
    "section": "2.5: Calculating the Probability of an Event",
    "text": "2.5: Calculating the Probability of an Event\n\nSuppose we toss a balanced coin three times. Find the probability that 2/3 tosses result in heads.\n\nDefine the experiment and clearly determine how to describe one simple event.\nDefine S: list the simple events associated with the experiment.\nAssign reasonable probabilities to the sample points in S; remember that P[E_i] \\ge 0 and \\sum_i P[E_i] = 1.\nDefine the event of interest, A, as a specific collection of sample points.\nFind P[A] by summing the probabilities of the sample points in A."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#calculating-the-probability-of-an-event-2",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#calculating-the-probability-of-an-event-2",
    "title": "Introduction to Probability",
    "section": "2.5: Calculating the Probability of an Event",
    "text": "2.5: Calculating the Probability of an Event\n\nThe odds are two to one that when A and B play tennis, A wins. Suppose that A and B play two matches. Find the probability that A wins at least one match.\n\nDefine the experiment and clearly determine how to describe one simple event.\nDefine S: list the simple events associated with the experiment.\nAssign reasonable probabilities to the sample points in S; remember that P[E_i] \\ge 0 and \\sum_i P[E_i] = 1.\nDefine the event of interest, A, as a specific collection of sample points.\nFind P[A] by summing the probabilities of the sample points in A."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nLet us discuss some results from combinatorial theory.\nWe want to be able to count the total number of sample points in the sample space, S.\nSometimes we use this method to efficiently find probabilities.\n\nIf a sample space contains N equiprobable sample points and an event A contains exactly n_a sample points, then\n\n\n\nP[A] = \\frac{n_a}{N}"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-1",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-1",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nTheorem:\n\nWith m elements a_1, a_2, …, a_m and n elements b_1, b_2, …, b_n, it is possible for form mn = m \\times n pairs containing one element from each group."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-2",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-2",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nConsider an experiment that consists of recording the birthday for each of 20 randomly selected persons.\nIgnoring leap years and assuming that there are only 365 possible distinct birthdays, find the number of points in the sample space S for this experiment. \nIf we assume that each of the possible sets of birthdays is equiprobable, what is the probability that each person in the 20 has a different birthday?"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-3",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-3",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nPermutation: an ordered arrangement of r distinct objects, P_r^n.\n\n\nP_r^n = \\frac{n!}{(n-r)!}\n\n\nRemember, factorials are defined as follows:\n\n\nn! = n \\times (n-1) \\times (n-2) \\times ... \\times 2 \\times 1\n\n\nWith special factorials:\n\n\n\\begin{align*}\n1! &= 1 \\\\\n0! &= 1\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-4",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-4",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nSuppose that an assembly operation in a manufacturing plant involves four steps, which can be performed in any sequence. If the manufacturer wishes to compare the assembly time for each of the sequences, how many different sequences will be involved in the experiment?"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-5",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-5",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nCombination: the number of subsets formed from n objects taken r at a time, C_r^n or {n \\choose r}.\nTheorem:\n\nThe number of unordered subsets of size r chosen without replacmeent from n available objects is\n\n\n\n{n \\choose r} = C_r^n = \\frac{P_r^n}{r!} = \\frac{n!}{r(n-r)!}"
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-6",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#tools-for-counting-sample-points-6",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nA company orders supplies from M distributors and wishes to place n orders (n &lt; M). Assume that the company places the orders in a manner that allows every distributor an equal chance of obtaining any one order and there is no restriction on the number of orders that can be placed with any distributor. Find the probability that a particular distributor gets exactly k orders (k \\le n)."
  },
  {
    "objectID": "files/lecture/W02-L1-intro-prob-pt1.html#homework",
    "href": "files/lecture/W02-L1-intro-prob-pt1.html#homework",
    "title": "Introduction to Probability",
    "section": "Homework",
    "text": "Homework\n\n2.15\n2.18\n2.32\n2.33\n2.51\n2.54"
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian analysis involves updating beliefs based on observed data."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-1",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-1",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThis is the natural Bayesian knowledge-building process of:\n\nacknowledging your preconceptions (prior distribution),\nusing data (data distribution) to update your knowledge (posterior distribution), and\nrepeating (posterior distribution \\to new prior distribution)"
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-2",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-2",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian and frequentist analyses share a common goal: to learn from data about the world around us.\n\nBoth Bayesian and frequentist analyses use data to fit models, make predictions, and evaluate hypotheses.\nWhen working with the same data, they will typically produce a similar set of conclusions.\n\nStatisticians typically identify as either a “Bayesian” or “frequentist” …\n\n🚫 We are not going to “take sides.”\n✅ We will see these as tools in our toolbox."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-3",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-3",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian probability: the relative plausibility of an event.\n\nConsiders prior belief."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-4",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-4",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nFrequentist probability: the long-run relative frequency of a repeatable event.\n\nDoes not consider prior belief."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-5",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-5",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThe Bayesian framework depends upon prior information, data, and the balance between them.\n\nThe balance between the prior information and data is determined by the relative strength of each\n\n\n\n\nWhen we have little data, our posterior can rely more on prior knowledge.\nAs we collect more data, the prior can lose its influence."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-6",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-6",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe can also use this approach to combine analysis results."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#bayes-rule",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#bayes-rule",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\nWe will use an example to work through Bayesian logic.\nThe Collins Dictionary named “fake news” the 2017 term of the year.\n\nFake, misleading, and biased news has proliferated along with online news and social media platforms which allow users to post articles with little quality control.\n\nWe want to flag articles as “real” or “fake.”\nWe’ll examine a sample of 150 articles which were posted on Facebook and fact checked by five BuzzFeed journalists (Shu et al. 2017)."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#bayes-rule-1",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#bayes-rule-1",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\nInformation about each article is stored in the fake_news dataset in the bayesrules package.\n\n\nfake_news &lt;- bayesrules::fake_news\nprint(colnames(fake_news))\n\n [1] \"title\"                   \"text\"                   \n [3] \"url\"                     \"authors\"                \n [5] \"type\"                    \"title_words\"            \n [7] \"text_words\"              \"title_char\"             \n [9] \"text_char\"               \"title_caps\"             \n[11] \"text_caps\"               \"title_caps_percent\"     \n[13] \"text_caps_percent\"       \"title_excl\"             \n[15] \"text_excl\"               \"title_excl_percent\"     \n[17] \"text_excl_percent\"       \"title_has_excl\"         \n[19] \"anger\"                   \"anticipation\"           \n[21] \"disgust\"                 \"fear\"                   \n[23] \"joy\"                     \"sadness\"                \n[25] \"surprise\"                \"trust\"                  \n[27] \"negative\"                \"positive\"               \n[29] \"text_syllables\"          \"text_syllables_per_word\"\n\n\n\nWe could build a simple news filter which uses the following rule: since most articles are real, we should read and believe all articles.\n\nWhile this filter would solve the problem of disregarding real articles, we would read lots of fake news.\nIt also only takes into account the overall rates of, not the typical features of, real and fake news."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-7",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-7",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nSuppose that the most recent article posted to a social media platform is titled: The president has a funny secret!\n\nSome features of this title probably set off some red flags.\nFor example, the usage of an exclamation point might seem like an odd choice for a real news article.\n\nIn the dataset, what is the split of real and fake articles?\n\n\nfake_news %&gt;% tabyl(type)\n\n type  n percent\n fake 60     0.4\n real 90     0.6\n\n\n\nOur data backs up our instinct on the article,\n\n\nfake_news %&gt;% tabyl(title_has_excl, type) %&gt;%\n  adorn_percentages(\"col\") %&gt;%\n  adorn_pct_formatting(digits = 2) %&gt;%\n  adorn_ns()\n\n title_has_excl        fake        real\n          FALSE 73.33% (44) 97.78% (88)\n           TRUE 26.67% (16)  2.22%  (2)\n\n\n\nIn this dataset, 26.67% (16 of 60) of fake news titles but only 2.22% (2 of 90) of real news titles use an exclamation point."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-8",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#thinking-like-a-bayesian-8",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe now have two pieces of contradictory information.\n\nOur prior information suggested that incoming articles are most likely real.\nHowever, the exclamation point data is more consistent with fake news.\n\n\n\n\nThinking like Bayesians, we know that balancing both pieces of information is important in developing a posterior understanding of whether the article is fake."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nOur fake news analysis studies two variables:\n\nan article’s fake vs real status and\nits use of exclamation points.\n\nWe can represent the randomness in these variables using probability models.\nWe will now build:\n\na prior probability model for our prior understanding of whether the most recent article is fake;\na model for interpreting the exclamation point data; and, eventually,\na posterior probability model which summarizes the posterior plausibility that the article is fake."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-1",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-1",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nLet’s now formalize our prior understanding of whether the new article is fake.\nBased on our fake_news data, we saw that 40% of articles are fake and 60% are real.\n\nBefore reading the new article, there’s a 0.4 prior probability that it’s fake and a 0.6 prior probability it’s not.\n\n\nP\\left[B\\right] = 0.40 \\text{ and } P\\left[B\\right] = 0.40\n\nRemember that a valid probability model must:\n\naccount for all possible events (all articles must be fake or real);\nit assigns prior probabilities to each event; and\nthe probabilities sum to one."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-2",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-2",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nNow we will summarize the insights from the data we collected on the new article.\n\nWe want to formalize our observation that the exclamation point data is more compatible with fake news than with real news.\n\n\n\n\n\n\n\n\n\n\ntitle_has_excl\nfake\nreal\n\n\n\n\nFALSE\n73.3% (44)\n97.8% (88)\n\n\nTRUE\n26.7% (16)\n2.2% (2)\n\n\n\n\n\n\n\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points.\n\nLooking at the probabilities, we can see that 26.67% of fake articles vs. 2.22% of real articles use exclamation points.\n\nExclamation point usage is much more likely among fake news than real news.\nWe have evidence that the article is fake."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-3",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-3",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nNote that we know that the incoming article used exclamation points (A), but we do not actually know if the article is fake (B or B^c).\nIn this case, we compared P[A|B] and P[A|B^c] to ascertain the relative likelihood of observing A under different scenarios.\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]\n\n\n\nEvent\nB\nBc\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\n\nIt is important for us to note that the likelihood function is not a probability function.\n\nThis is a framework to compare the relative comparability of our exclamation point data with B and B^c."
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-4",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-4",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB (fake)\nBc (real)\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\n\nThe prior evidence suggested the article is most likely real,\n\nP[B] = 0.4 &lt; P[B^c] = 0.6\n\nThe data, however, is more consistent with the article being fake,\n\nL[B|A] = 0.2667 &gt; L[B^c|A] = 0.0222"
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-5",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-5",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe can summarize our probabilities in a table, but some calculations are required.\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\nThus,\n\n\n\\begin{align*}\nP[A \\cap B] &= P[A|B] \\times P[B] \\\\\n&= 0.2667 \\times 0.4 \\\\\n&= 0.1067\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-6",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-6",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\nThus,\n\n\n\\begin{align*}\nP[A^c \\cap B] &= P(A^c|B) \\times P[B]  \\\\\n&= (1-P[A|B]) \\times P[B] \\\\\n&= (1-0.2667) \\times 0.4 \\\\\n&= 0.2933\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-7",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-7",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\nThus,\n\n\n\\begin{align*}\nP[A \\cap B^c] &= P[A|B^c] \\times P[B^c]  \\\\\n&= 0.0222 \\times 0.6 \\\\\n&= 0.0133\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-8",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-8",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\nThus,\n\n\n\\begin{align*}\nP[A^c \\cap B^c] &= P[A^c|B^c] \\times P[B^c]  \\\\\n&= 0.9778 \\times 0.6 \\\\\n&= 0.5867\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-9",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-9",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n0.5867\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nFinally,\n\n\n\\begin{align*}\n&P[A] = 0.1067 + 0.0133 = 0.12 \\\\\n&P[A^c] = 0.2933 + 0.5867 = 0.88\n\\end{align*}"
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-10",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-10",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nUsing rules of probability, we have completed the table.\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n0.12\n\n\nA^c\n0.2933\n0.5867\n0.88\n\n\nTotal\n0.4\n0.6\n1"
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-11",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#building-a-bayesian-model-11",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWith more information, we can answer the question: what is the probability that the latest article is fake?\nWe will use the posterior probability, P[B|A], which is found using Bayes’ Rule.\nBayes’ Rule: For events A and B,\n\nP[B|A] = \\frac{P[A \\cap B]}{P[A]} = \\frac{P[B] \\times L[B|A]}{P[A]}\n\nBut really, we can think about it like this,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}"
  },
  {
    "objectID": "files/lecture/W07-L1-Bayesian-thinking.html#homework",
    "href": "files/lecture/W07-L1-Bayesian-thinking.html#homework",
    "title": "Bayesian Thinking and Bayes Rule",
    "section": "Homework",
    "text": "Homework\n\n1.3\n1.4\n1.8\n2.4\n2.9"
  },
  {
    "objectID": "files/assignments/Assignment0.html",
    "href": "files/assignments/Assignment0.html",
    "title": "Introductions",
    "section": "",
    "text": "What is your name as it appears on my roster in Classmate? (I am using this assignment for attendance verification.)\nHow is your name pronounced?\nIs there another name you prefer to go by?\nWhat are your pronouns?\nWhat is your major(s) and minor(s)?\nWhat statistics courses have you taken previously?\nAre you familiar with statistical programming? (e.g., R, python, SAS, etc.)\nDo you have any questions or concerns about this course? If so, please list them and I will respond when I grade this assignment.\nWhat interests do you have, outside of school and work?\nWhat are three of your favorite things?\nIf you were independently wealthy, what would you do with your time?"
  }
]