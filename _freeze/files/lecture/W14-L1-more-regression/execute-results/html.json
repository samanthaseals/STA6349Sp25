{
  "hash": "f907f5dadce963523d8c34f9ea81522e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"**Further Topics in Regression**\"\nsubtitle: \"**STA6349: Applied Bayesian Analysis** <br> Spring 2025\"\ndate-format: long\nexecute:\n  echo: true\n  warning: false\n  message: false\n  error: true\nformat: \n  revealjs:\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    slide-number: false\n    width: 1600\n    height: 900\n    html-math-method: katex\n    theme:\n      - default\n      - sp25.scss\neditor: source\n---\n\n\n\n## **Introduction**  \n\n- Last week, we learned regression for continuous outcomes using the normal distribution.\n\n- Today, we will focus on expanding to Poisson, negative binomial, and logistic regressions, so that we can model continuous, count, and categorical outcomes.\n\n## **Working Example**  \n\n- We will examine Australian weather from the `weather_WU` data in the `bayesrules` package. \n    - This data contains 100 days of weather data for each of two Australian cities: Uluru and Wollongong.\n    \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesrules)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidyverse)\nlibrary(broom.mixed)\nlibrary(tidybayes)\n\ndata(weather_WU)\nhead(weather_WU, n = 3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"location\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"mintemp\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"maxtemp\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"rainfall\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"windgustdir\"],\"name\":[5],\"type\":[\"ord\"],\"align\":[\"right\"]},{\"label\":[\"windgustspeed\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"winddir9am\"],\"name\":[7],\"type\":[\"ord\"],\"align\":[\"right\"]},{\"label\":[\"winddir3pm\"],\"name\":[8],\"type\":[\"ord\"],\"align\":[\"right\"]},{\"label\":[\"windspeed9am\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"windspeed3pm\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"humidity9am\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"humidity3pm\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"pressure9am\"],\"name\":[13],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"pressure3pm\"],\"name\":[14],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"temp9am\"],\"name\":[15],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"temp3pm\"],\"name\":[16],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"raintoday\"],\"name\":[17],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"risk_mm\"],\"name\":[18],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"raintomorrow\"],\"name\":[19],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"year\"],\"name\":[20],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"month\"],\"name\":[21],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"day_of_year\"],\"name\":[22],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Uluru\",\"2\":\"12.3\",\"3\":\"30.1\",\"4\":\"0\",\"5\":\"ENE\",\"6\":\"39\",\"7\":\"E\",\"8\":\"ENE\",\"9\":\"20\",\"10\":\"13\",\"11\":\"23\",\"12\":\"10\",\"13\":\"1023.3\",\"14\":\"1018.5\",\"15\":\"20.9\",\"16\":\"29.7\",\"17\":\"No\",\"18\":\"0.0\",\"19\":\"No\",\"20\":\"2014\",\"21\":\"9\",\"22\":\"255\"},{\"1\":\"Uluru\",\"2\":\"20.5\",\"3\":\"35.9\",\"4\":\"5\",\"5\":\"SSE\",\"6\":\"52\",\"7\":\"SE\",\"8\":\"SE\",\"9\":\"9\",\"10\":\"20\",\"11\":\"71\",\"12\":\"29\",\"13\":\"1012.9\",\"14\":\"1009.1\",\"15\":\"23.4\",\"16\":\"33.9\",\"17\":\"Yes\",\"18\":\"0.2\",\"19\":\"No\",\"20\":\"2015\",\"21\":\"2\",\"22\":\"54\"},{\"1\":\"Uluru\",\"2\":\"15.8\",\"3\":\"41.4\",\"4\":\"0\",\"5\":\"NNW\",\"6\":\"50\",\"7\":\"SSE\",\"8\":\"NW\",\"9\":\"7\",\"10\":\"24\",\"11\":\"15\",\"12\":\"4\",\"13\":\"1012.3\",\"14\":\"1008.5\",\"15\":\"24.1\",\"16\":\"39.7\",\"17\":\"No\",\"18\":\"0.0\",\"19\":\"No\",\"20\":\"2013\",\"21\":\"10\",\"22\":\"282\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## **Working Example**  \n\n- Let's keep only the variables on afternoon temperatures (`temp3pm`) and a subset of possible predictors that we'd have access to in the morning:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweather_WU <- weather_WU %>% \n  select(location, windspeed9am, humidity9am, pressure9am, temp9am, temp3pm)\nhead(weather_WU)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"location\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"windspeed9am\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"humidity9am\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"pressure9am\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"temp9am\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"temp3pm\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Uluru\",\"2\":\"20\",\"3\":\"23\",\"4\":\"1023.3\",\"5\":\"20.9\",\"6\":\"29.7\"},{\"1\":\"Uluru\",\"2\":\"9\",\"3\":\"71\",\"4\":\"1012.9\",\"5\":\"23.4\",\"6\":\"33.9\"},{\"1\":\"Uluru\",\"2\":\"7\",\"3\":\"15\",\"4\":\"1012.3\",\"5\":\"24.1\",\"6\":\"39.7\"},{\"1\":\"Uluru\",\"2\":\"28\",\"3\":\"29\",\"4\":\"1016.0\",\"5\":\"26.4\",\"6\":\"34.2\"},{\"1\":\"Uluru\",\"2\":\"24\",\"3\":\"10\",\"4\":\"1010.5\",\"5\":\"36.7\",\"6\":\"43.3\"},{\"1\":\"Uluru\",\"2\":\"22\",\"3\":\"32\",\"4\":\"1012.2\",\"5\":\"25.1\",\"6\":\"33.5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## **Working Example**  \n\n- We begin our analysis with the familiar: a simple Normal regression model of `temp3pm` with one quantitative predictor, the morning temperature `temp9am`, both measured in degrees Celsius.\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_WU, aes(x = temp9am, y = temp3pm)) +\n  geom_point(size = 0.2) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Working Example**  \n\n- Let's model the 3 pm temperature as a function of the 9 am temperature on a given day $i$.\n    - Outcome: $Y_i$ = 3 pm temp\n    - Predictor: $X_{i1}$ = 9 am temp\n    \n- Then, we can model it using the Bayesian normal regression model,\n\n$$\n\\begin{align*}\nY_i | \\beta_0, \\beta_1, \\sigma &\\overset{\\text{ind}}{\\sim} N(\\mu_i, \\sigma^2), \\text{ with } \\mu_i = \\beta_0 + \\beta_1 X_{i1} \\\\\n\\beta_{0c} &\\sim N(25, 5^2) \\\\\n\\beta_1 &\\sim N(0,3.1^2) \\\\\n\\sigma & \\sim \\text{Exp}(0.13)\n\\end{align*}\n$$\n\n- Note that we are using the centered intercept as 0 degree mornings are rare in Australia.\n\n## **Working Example**  \n\n- Simulating this model,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweather_model_1 <- stan_glm(\n  temp3pm ~ temp9am, \n  data = weather_WU, family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.072 seconds (Warm-up)\nChain 1:                0.123 seconds (Sampling)\nChain 1:                0.195 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.073 seconds (Warm-up)\nChain 2:                0.131 seconds (Sampling)\nChain 2:                0.204 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.077 seconds (Warm-up)\nChain 3:                0.131 seconds (Sampling)\nChain 3:                0.208 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.074 seconds (Warm-up)\nChain 4:                0.129 seconds (Sampling)\nChain 4:                0.203 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n## **Working Example** \n\n- Note that we asked `stan_glm()` to *autoscale* our priors. What did it change them to?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(weather_model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPriors for model 'weather_model_1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 25, scale = 5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 3.1)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.13)\n------\nSee help('prior_summary.stanreg') for more details\n```\n\n\n:::\n:::\n\n\n\n## **Working Example** \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_trace(weather_model_1, size = 0.1)\n```\n\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Working Example** \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_dens_overlay(weather_model_1)\n```\n\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Working Example** \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_interval(weather_model_1, prob = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  10%      90%\n(Intercept) 2.9498083 5.448752\ntemp9am     0.9802648 1.102423\nsigma       3.8739305 4.409474\n```\n\n\n:::\n:::\n\n\n\n- 80% credible interval for $\\beta_1$: (0.98, 1.10)\n\n- 80% credible interval for $\\sigma$: (3.87, 4.41)\n\n## **Working Example** \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(weather_model_1)\n```\n\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Categorical Predictors**\n\n- What if we look at the data by location?\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_WU, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- We should probably look at location as a predictor...\n\n## **Categorical Predictors**\n\n- Let's let $X_{i2}$ be an indicator for the location,\n\n$$\nX_{i2} =\n\\begin{cases}\n1 & \\text{Wollongong} \\\\\n0 & \\text{otherwise (i.e., Uluru).}\n\\end{cases}\n$$\n\n- We are treating \"not-Wollongong\" as our reference group -- in this case, it is Uluru.\n\n$$\n\\begin{array}{rl}\n\\text{data:} & Y_i \\mid \\beta_0, \\beta_1, \\sigma \\overset{\\text{ind}}{\\sim} N(\\mu_i, \\sigma^2) \\quad \\text{with} \\quad \\mu_i = \\beta_0 + \\beta_1 X_{i2} \\\\\n\\text{priors:} & \\beta_{0c} \\sim N(25, 5^2) \\\\\n& \\beta_1 \\sim N(0, 38^2) \\\\\n& \\sigma \\sim \\text{Exp}(0.13).\n\\end{array}\n$$\n\n## **Categorical Predictors**\n\n- Let's think about our model.\n\n$$y = \\beta_0 + \\beta_1 x_2$$\n\n- What do our coefficients mean?\n    - $\\beta_0$ is the typical 3 pm temperature in Uluru ($x_2=0$).\n    - $\\beta_1$ is the typical difference in 3 pm temperature in Wollongong ($x_2=1$) as compared to Uluru ($x_2=0$).\n    - $\\sigma$ represents the standard deviation in 3 pm temperatures in Wollongong and Uluru.\n\n- When we use a binary predictor, this results in two models, effectively. \n    - One when $x_1=0$ (Uluru) and one when $x_1=1$ (Wollongong).\n\n$$\n\\begin{align*}\ny = \\beta_0 + \\beta_1 x_2 \\to \\text{(U)} \\ y &= \\beta_0 \\\\\n\\text{(W)} \\ y&=  \\beta_0+\\beta_1\n\\end{align*}\n$$\n\n## **Categorical Predictors**\n\n- Let's simulate our posterior using weakly informative priors,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweather_model_2 <- stan_glm(\n  temp3pm ~ location,\n  data = weather_WU, family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.072 seconds (Warm-up)\nChain 1:                0.136 seconds (Sampling)\nChain 1:                0.208 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 6e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.078 seconds (Warm-up)\nChain 2:                0.13 seconds (Sampling)\nChain 2:                0.208 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.077 seconds (Warm-up)\nChain 3:                0.137 seconds (Sampling)\nChain 3:                0.214 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.073 seconds (Warm-up)\nChain 4:                0.134 seconds (Sampling)\nChain 4:                0.207 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n## **Categorical Predictors** \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_trace(weather_model_2, size = 0.1)\n```\n\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Categorical Predictors** \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_dens_overlay(weather_model_2)\n```\n\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Categorical Predictors**\n\n- Looking at the posterior summary statistics,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(weather_model_2, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80) %>% \n  select(-std.error)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"29.714695\",\"3\":\"29.01943\",\"4\":\"30.424971\"},{\"1\":\"locationWollongong\",\"2\":\"-10.322937\",\"3\":\"-11.31883\",\"4\":\"-9.301761\"},{\"1\":\"sigma\",\"2\":\"5.484104\",\"3\":\"5.14486\",\"4\":\"5.857824\"},{\"1\":\"mean_PPD\",\"2\":\"24.564554\",\"3\":\"23.85914\",\"4\":\"25.261738\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## **Categorical Predictors**\n\n- We can also look at the temperatures by location,\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.frame(weather_model_2) %>% \n  mutate(uluru = `(Intercept)`, \n         wollongong = `(Intercept)` + locationWollongong) %>% \n  mcmc_areas(pars = c(\"uluru\", \"wollongong\"))\n```\n\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Multiple Predictors**\n\n- What if we want to include multiple predictors?\n    - Notice in the code, our model now has multiple predictors (`temp9am` and `location`).\n    - Here, we are simulating the *prior* - this will allow us to graphically examine what we are claiming with the priors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweather_model_3_prior <- stan_glm(\n  temp3pm ~ temp9am + location,\n  data = weather_WU, family = gaussian, \n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735,\n  prior_PD = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 9e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.063 seconds (Warm-up)\nChain 1:                0.107 seconds (Sampling)\nChain 1:                0.17 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.061 seconds (Warm-up)\nChain 2:                0.056 seconds (Sampling)\nChain 2:                0.117 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.057 seconds (Warm-up)\nChain 3:                0.089 seconds (Sampling)\nChain 3:                0.146 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.06 seconds (Warm-up)\nChain 4:                0.062 seconds (Sampling)\nChain 4:                0.122 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n## **Multiple Predictors**\n\n- From the simulated priors,\n    - We can look at different sets of 3 p.m. temperature data (left graph).\n    - We can also look at our prior assumptions about the relationship between 3 p.m. and 9 a.m. temperature at each location (right graph)\n        - The graph tells us that our prior is vague - when it comes to Australian weather, we're just not sure what is going on.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Multiple Predictors**\n\n- Instead of starting the `stan_glm()` syntax from scratch, we can `update()` the `weather_model_3_prior` by setting `prior_PD = FALSE`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweather_model_3 <- update(weather_model_3_prior, prior_PD = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.088 seconds (Warm-up)\nChain 1:                0.143 seconds (Sampling)\nChain 1:                0.231 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.09 seconds (Warm-up)\nChain 2:                0.155 seconds (Sampling)\nChain 2:                0.245 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.091 seconds (Warm-up)\nChain 3:                0.14 seconds (Sampling)\nChain 3:                0.231 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.09 seconds (Warm-up)\nChain 4:                0.141 seconds (Sampling)\nChain 4:                0.231 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n## **Multiple Predictors**\n\n- The simulation results in 20,000 posterior plausible relationships between temperature and location.\n\n- You try the following code:\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweather_WU %>%\n  add_fitted_draws(weather_model_3, n = 100) %>%\n  ggplot(aes(x = temp9am, y = temp3pm, color = location)) +\n    geom_line(aes(y = .value, group = paste(location, .draw)), alpha = .1) +\n    geom_point(data = weather_WU, size = 0.1) +\n  theme_bw()\n```\n:::\n\n\n</center>\n\n## **Multiple Predictors**\n\n- The simulation results in 20,000 posterior plausible relationships between temperature and location.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- 3 p.m. temperature is positively associated with 9 a.m. temperature and tends to be higher in Uluru than in Wollongong. \n\n- Further, relative to the prior simulated relationships in Figure 11.9, these posterior relationships are very consistent \n\n## **Multiple Predictors**\n\n- Looking at the posterior summary statistics,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(weather_model_3, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80) %>% \n  select(-std.error)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"11.3391917\",\"3\":\"10.4809210\",\"4\":\"12.2016762\"},{\"1\":\"temp9am\",\"2\":\"0.8573355\",\"3\":\"0.8196281\",\"4\":\"0.8945346\"},{\"1\":\"locationWollongong\",\"2\":\"-7.0599280\",\"3\":\"-7.5067716\",\"4\":\"-6.5999057\"},{\"1\":\"sigma\",\"2\":\"2.3793095\",\"3\":\"2.2338336\",\"4\":\"2.5420914\"},{\"1\":\"mean_PPD\",\"2\":\"24.5593686\",\"3\":\"24.2584132\",\"4\":\"24.8609334\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## **Multiple Predictions**\n\n- You try! Run the following code to look at our posterior predictive models.\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate a set of predictions\nset.seed(84735)\ntemp3pm_prediction <- posterior_predict(\n  weather_model_3,\n  newdata = data.frame(temp9am = c(10, 10), \n                       location = c(\"Uluru\", \"Wollongong\")))\n\n# Plot the posterior predictive models\nmcmc_areas(temp3pm_prediction) +\n  ggplot2::scale_y_discrete(labels = c(\"Uluru\", \"Wollongong\")) + \n  xlab(\"temp3pm\")\n```\n:::\n\n\n</center>\n\n## **Multiple Predictions**\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W14-L1-more-regression_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- Roughly speaking, we can anticipate 3 p.m. temperatures between 15 and 25 degrees in Uluru, and cooler temperatures between 8 and 18 in Wollongong.\n\n## **Poisson Regression**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate the prior distribution\nequality_model_prior <- stan_glm(laws ~ percent_urban + historical, \n                                 data = equality, \n                                 family = poisson,\n                                 prior_intercept = normal(2, 0.5),\n                                 prior = normal(0, 2.5, autoscale = TRUE), \n                                 chains = 4, iter = 5000*2, seed = 84735, \n                                 prior_PD = TRUE)\n\n# Update to simulate the posterior distribution\nequality_model <- update(equality_model_prior, prior_PD = FALSE)\n```\n:::\n\n\n\n## **Negative Binomial Regression**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate the prior distribution\nbooks_negbin_sim <- stan_glm(\n  books ~ age + wise_unwise, \n  data = pulse, family = neg_binomial_2,\n  prior_intercept = normal(0, 2.5, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735)\n\n# Update to simulate the posterior distribution\nequality_model <- update(equality_model_prior, prior_PD = FALSE)\n```\n:::\n\n\n\n## **Logistic Regression**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate the prior distribution\nrain_model_prior <- stan_glm(raintomorrow ~ humidity9am,\n                             data = weather, family = binomial,\n                             prior_intercept = normal(-1.4, 0.7),\n                             prior = normal(0.07, 0.035),\n                             chains = 4, iter = 5000*2, seed = 84735,\n                             prior_PD = TRUE)\n\n# Update to simulate the posterior distribution\nrain_model_1 <- update(rain_model_prior, prior_PD = FALSE)\n```\n:::\n\n\n\n## **Wrap Up**\n\n- Today, we have expanded to multiple predictors in our model.\n    \n- Wednesday next week:\n    - Assignment to practice regression.\n    \n- Monday & Wednesday next week:\n    - Project 2! (We will build regression models. Surprise!)\n    - You will present on Wednesday.\n    - We will have smaller groups ($n=2$) this time.",
    "supporting": [
      "W14-L1-more-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}