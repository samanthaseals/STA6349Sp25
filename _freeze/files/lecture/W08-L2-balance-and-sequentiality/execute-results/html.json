{
  "hash": "6cdb81d3ca6f5ae7eb9fe77e1f7cfb7f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"**Balance and Sequentiality in Bayesian Analyses**\"\nsubtitle: \"**STA6349: Applied Bayesian Analysis** <br> Spring 2025\"\ndate-format: long\nexecute:\n  echo: true\n  warning: false\n  message: false\n  error: true\nformat: \n  revealjs:\n    code-overflow: wrap\n    embed-resources: true\n    slide-number: false\n    width: 1600\n    height: 900\n    html-math-method: katex\n    theme:\n      - default\n      - sp25.scss\neditor: source\n---\n\n\n\n## **Introduction**\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n- On Monday, we talked about the Beta-Binomial model for binary outcomes with an unknown probability of success, $\\pi$.\n\n- We will now discuss sequentality in Bayesian analyses.\n\n- Working example: \n    - In Alison Bechdel's 1985 comic strip The Rule, a character states that they only see a movie if it satisfies the following three rules (Bechdel 1986):\n        - the movie has to have at least two women in it;\n        - these two women talk to each other; and\n        - they talk about something besides a man.\n\n    - These criteria constitute the Bechdel test for the representation of women in film. \n    \n- Thinking of movies you've watched, what percentage of all recent movies do you think pass the Bechdel test? Is it closer to 10%, 50%, 80%, or 100%?\n\n## **Introduction**  \n\n- Let  $\\pi$, a random value between 0 and 1, denote the unknown proportion of recent movies that pass the Bechdel test. \n\n- Three friends - the feminist, the clueless, and the optimist - have some prior ideas about $\\pi$.\n    - Reflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters. \n    - The clueless doesnâ€™t really recall the movies they've seen, and so are unsure whether passing the Bechdel test is common or uncommon. \n    - Lastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test. \n    \n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta(alpha = 1, beta = 1)\nplot_beta(alpha = 5, beta = 11)\nplot_beta(alpha = 14, beta = 1)\n```\n:::\n\n\n    \n- Which one is which?\n    \n## **Introduction**      \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta(alpha = 1, beta = 1)\nplot_beta(alpha = 5, beta = 11)\nplot_beta(alpha = 14, beta = 1)\n```\n:::\n\n\n    \n- Which one is which?\n    - Reflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters. \n    - The clueless doesnâ€™t really recall the movies they've seen, and so are unsure whether passing the Bechdel test is common or uncommon. \n    - Lastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test. \n\n## **Introduction**  \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta(alpha = 1, beta = 1) + theme_bw() + ggtitle(\"Beta(1, 1)\")\n```\n\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- The clueless doesnâ€™t really recall the movies they've seen, and so are unsure whether passing the Bechdel test is common or uncommon. \n\n## **Introduction**  \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta(alpha = 5, beta = 11) + theme_bw() + ggtitle(\"Beta(5, 11)\")\n```\n\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- Reflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters. \n\n## **Introduction**  \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta(alpha = 14, beta = 1) + theme_bw() + ggtitle(\"Beta(14, 1)\")\n```\n\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- Lastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test.   \n\n## **Introduction** \n\n- The analysts agree to review a sample of $n$ recent movies and record $Y$, the number that pass the Bechdel test. \n    - Because the outcome is yes/no, the binomial distribution is appropriate for the data distribution.\n    - We aren't sure what the population proportion, $\\pi$, is, so we will not restrict it to a fixed value. \n        - Because we know $\\pi \\in [0, 1]$, the beta distribution is appropriate for the prior distribution.\n    \n$$\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta)\n\\end{align*}\n$$\n\n- From the previous chapter, we know that this results in the following posterior distribution\n\n$$\n\\pi | (Y=y) \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n$$\n\n## **Introduction** \n\n- Wait!!\n    - Everyone gets their own prior?\n    - ... is there a \"correct\" prior?\n    - ...... is the Bayesian world always this subjective?\n    \n## **Introduction** \n\n- Wait!!\n    - Everyone gets their own prior?\n    - ... is there a \"correct\" prior?\n    - ...... is the Bayesian world always this subjective?    \n    \n- More clearly defined questions that we *can* actually answer:\n    - To what extent might different priors lead the analysts to three different posterior conclusions about the Bechdel test? \n        - How might this depend upon the sample size and outcomes of the movie data they collect? \n    - To what extent will the analysts' posterior understandings evolve as they collect more and more data? \n    - Will they ever come to agreement about the representation of women in film?! \n    \n## **Different Priors $\\to$ Different Posteriors**\n    \n<center>    \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- The differing prior means show disagreement about whether $\\pi$ is closer to 0 or 1.\n\n- The differing levels of prior variability show that the analysts have different degrees of certainty in their prior information. \n\n    - The more certain we are about the prior information, the smaller the prior variability. \n\n## **Different Priors $\\to$ Different Posteriors**  \n\n<center>    \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- **Informative prior**: reflects specific information about the unknown variable with high certainty, i.e., low variability.\n    \n## **Different Priors $\\to$ Different Posteriors**  \n\n<center>    \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- **Vague or diffuse prior**:  reflects little specific information about the unknown variable. \n    - A **flat prior**, which assigns equal prior plausibility to all possible values of the variable, is a special case.\n    - This is effectively saying \"ðŸ¤·.\"\n    \n## **Different Priors $\\to$ Different Posteriors**  \n\n- Okay, great - we have different priors.\n    - How do the different priors affect the posterior?\n    \n- We have data from FiveThirtyEight, reporting results of the Bechdel test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(65821)\nbechdel20 <- bayesrules::bechdel %>% sample_n(20)\nhead(bechdel20, n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n   year title               binary\n  <dbl> <chr>               <chr> \n1  2013 Her                 FAIL  \n2  1997 Grosse Pointe Blank PASS  \n3  2006 Volver              PASS  \n```\n\n\n:::\n:::\n\n\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- So how many pass the test in this sample?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbechdel20 %>% tabyl(binary) %>% adorn_totals(\"row\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n binary  n percent\n   FAIL 11    0.55\n   PASS  9    0.45\n  Total 20    1.00\n```\n\n\n:::\n:::\n\n\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Let's look at the graphs of just the prior and likelihood.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta_binomial(alpha = 5, beta = 11, y = 9, n = 20, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 1, beta = 1, y = 9, n = 20, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 9, n = 20, posterior = FALSE) + theme_bw()\n```\n:::\n\n\n\n- Questions to think about:\n\n    - Whose posterior do you anticipate will look the most like the scaled likelihood? \n    - Whose do you anticipate will look the least like the scaled likelihood?\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Let's look at the graphs of just the prior and likelihood.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Let's look at the graphs of just the prior and likelihood.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Let's look at the graphs of just the prior and likelihood.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Find the posterior distributions. (i.e., What are the updated parameters?)\n\n<center>\n<table><thead>\n  <tr>\n    <th>Analyst</th>\n    <th>Prior</th>\n    <th>Posterior</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td>the feminist</td>\n    <td>Beta(5, 11)</td>\n    <td>Beta(14, 22)</td>\n  </tr>\n  <tr>\n    <td>the clueless</td>\n    <td>Beta(1, 1)</td>\n    <td>Beta(10, 12)</td>\n  </tr>\n  <tr>\n    <td>the optimist</td>\n    <td>Beta(14, 1)</td>\n    <td>Beta(23, 12)</td>\n  </tr>\n</tbody>\n</table>\n</center>\n\n- Let's now explore what the posteriors look like.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta_binomial(alpha = 5, beta = 11, y = 9, n = 20) + theme_bw()\nplot_beta_binomial(alpha = 1, beta = 1, y = 9, n = 20) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 9, n = 20) + theme_bw()\n```\n:::\n\n\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Let's now explore what the posteriors look like.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Let's now explore what the posteriors look like.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Let's now explore what the posteriors look like.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- In addition to priors affecting our posterior distributions... the data also affects it.\n\n- Let's now consider three new analysts: they all share the optimistic Beta(14, 1) for $\\pi$, however, they have access to different data.\n    - Morteza reviews $n = 13$ movies from the year 1991, among which  $Y=6$ (about 46%) pass the Bechdel.\n    - Nadide reviews $n = 63$ movies from the year 2001, among which  $Y=29$ (about 46%) pass the Bechdel.\n    - Ursula reviews $n = 99$ movies from the year 2013, among which  $Y=46$ (about 46%) pass the Bechdel.\n    \n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta_binomial(alpha = 14, beta = 1, y = 6, n = 13, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 29, n = 63, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 46, n = 99, posterior = FALSE) + theme_bw()\n```\n:::\n\n\n    \n- How will the different data affect the posterior distributions?   \n\n    - Which posterior will be the most in sync with their data?\n    - Which posterior will be the least in sync with their data?\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- How will the different data affect the posterior distributions?   \n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- How will the different data affect the posterior distributions?   \n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- How will the different data affect the posterior distributions?   \n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n</center>\n    \n## **Different Priors $\\to$ Different Posteriors**  \n\n- Find the posterior distributions. (i.e., What are the updated parameters?)\n\n    - Recall that all use the Beta(14, 1) prior.    \n\n<center>\n<table><thead>\n  <tr>\n    <th><center>Analyst</center></th>\n    <th><center>Data</center></th>\n    <th><center>Posterior</center></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td>Morteza</td>\n    <td>$Y=6$ of $n=13$</td>\n    <td>Beta(20, 8)</td>\n  </tr>\n  <tr>\n    <td>Nadide</td>\n    <td>$Y=29$ of $n=63$</td>\n    <td>Beta(45, 35)</td>\n  </tr>\n  <tr>\n    <td>Ursula</td>\n    <td>$Y=46$ of $n=99$</td>\n    <td>Beta(60, 54)</td>\n  </tr>\n</tbody>\n</table>\n</center>\n\n- Let's also explore what the posteriors look like.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta_binomial(alpha = 14, beta = 1, y = 6, n = 13) + theme_bw() \nplot_beta_binomial(alpha = 14, beta = 1, y = 29, n = 63) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 46, n = 99) + theme_bw()\n```\n:::\n\n\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Let's explore what the posteriors look like.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Let's explore what the posteriors look like.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- Let's explore what the posteriors look like.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W08-L2-balance-and-sequentiality_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Different Priors $\\to$ Different Posteriors**  \n\n- What did we observe?\n    - As $n \\to \\infty$, variance in the likelihood $\\to 0$.\n        - In Morteza's small sample of 13 movies, the likelihood function is wide.\n        - In Ursula's larger sample size of 99 movies, the likelihood function is narrower.\n    - We see that the narrower the likelihood, the more influence the data holds over the posterior. \n    \n## **Striking a Balance**  \n\n![](images/W07-L2-a.png){fig-align=\"center\"}        \n\n- Overall message: no matter the strength of and discrepancies among their prior understanding of $\\pi$, analysts will come to a common posterior understanding in light of strong data.\n\n## **Striking a Balance**  \n\n- The posterior can either favor the data or the prior.\n    - The rate at which the posterior balance tips in favor of the data depends upon the *prior*. \n    \n- Left to right on the graph, the sample size increases from $n=13$ to $n=99$ movies, while preserving the proportion that pass ($\\approx$ 0.46).\n    - The likelihood's insistence and the data's influence over the posterior increase with sample size.\n    - This also means that the influence of our prior understanding diminishes as we gather new data.\n\n- Top to bottom on the graph, priors move from informative (Beta(14,1)) to vague (Beta(1,1)).\n    - Naturally, the more informative the prior, the greater its influence on the posterior.\n  \n## **Introduction: Sequentiality**\n\n- Let's now turn our thinking to - okay, we've updated our beliefs... but now we have new data!\n\n- The evolution in our posterior understanding happens incrementally, as we accumulate new data. \n    - Scientists' understanding of climate change has evolved over the span of decades as they gain new information.\n    - Presidential candidates' understanding of their chances of winning an election evolve over months as new poll results become available. \n    \n## **Introduction: Sequentiality**\n    \n- Let's revisit Milgram's behavioral study of obedience from Chapter 3. Recall, $\\pi$ represents the proportion of people that will obey authority, even if it means bringing harm to others. \n    \n- Prior to Milgram's experiments, our fictional psychologist expected that few people would obey authority in the face of harming another: $\\pi \\sim \\text{Beta}(1,10)$.\n\n- Now, suppose that the psychologist collected the data incrementally, day by day, over a three-day period. \n\n- Find the following posterior distributions, each building off the last:\n    - Day 0: $\\text{Beta}(1,10)$.\n    - Day 1: $Y=1$ out of $n=10$.\n    - Day 2: $Y=17$ out of $n=20$.\n    - Day 3: $Y=8$ out of $n=10$.\n    \n## **Introduction: Sequentiality**\n    \n- Let's revisit Milgram's behavioral study of obedience from Chapter 3. Recall, $\\pi$ represents the proportion of people that will obey authority, even if it means bringing harm to others. \n    \n- Prior to Milgram's experiments, our fictional psychologist expected that few people would obey authority in the face of harming another: $\\pi \\sim \\text{Beta}(1,10)$.\n\n- Now, suppose that the psychologist collected the data incrementally, day by day, over a three-day period. \n\n- Find the following posterior distributions, each building off the last:\n\n    - Day 0: $\\text{Beta}(1,10)$.\n    - Day 1: $Y=1$ out of $n=10$: <font color = \"#cf63cd\">$\\text{Beta}(1,10) \\to \\text{Beta}(2, 19)$.</font>\n    - Day 2: $Y=17$ out of $n=20$: <font color = \"#cf63cd\">$\\text{Beta}(2, 19) \\to \\text{Beta}(19, 22)$.</font>\n    - Day 3: $Y=8$ out of $n=10$: <font color = \"#cf63cd\">$\\text{Beta}(19, 22) \\to \\text{Beta}(27, 24)$.</font> \n    \n- Recall from Chapter 3, our posterior was $\\text{Beta}(27,24)$!    \n\n## **Sequential Bayesian Analysis or Bayesian Learning** \n\n- In a sequential Bayesian analysis, a posterior model is updated incrementally as more data come in. \n    - With each new piece of data, the previous posterior model reflecting our understanding prior to observing this data becomes the new prior model.\n    \n- This is why we love Bayesian! \n    - We evolve our thinking as new data come in. \n    \n- These types of sequential analyses also uphold two fundamental properties:\n    1. The final posterior model is data order invariant,     \n    2. The final posterior only depends upon the cumulative data.\n\n## **Sequential Bayesian Analysis or Bayesian Learning** \n\n- In order:\n    - Day 0: $\\text{Beta}(1,10)$.\n    - Day 1: $Y=1$ out of $n=10$: <font color = \"#cf63cd\">$\\text{Beta}(1,10) \\to \\text{Beta}(2, 19)$.</font>\n    - Day 2: $Y=17$ out of $n=20$: <font color = \"#cf63cd\">$\\text{Beta}(2, 19) \\to \\text{Beta}(19, 22)$.</font>\n    - Day 3: $Y=8$ out of $n=10$: <font color = \"#cf63cd\">$\\text{Beta}(19, 22) \\to \\text{Beta}(27, 24)$.</font>  \n    \n- Out of order:\n    - Day 0: $\\text{Beta}(1,10)$.\n    - Day 3: $Y=8$ out of $n=10$: <font color = \"#cf63cd\">$\\text{Beta}(1,10) \\to \\text{Beta}(9, 12)$.</font>\n    - Day 2: $Y=17$ out of $n=20$: <font color = \"#cf63cd\">$\\text{Beta}(9, 12) \\to \\text{Beta}(26, 15)$.</font>\n    - Day 1: $Y=1$ out of $n=10$: <font color = \"#cf63cd\">$\\text{Beta}(26, 15) \\to \\text{Beta}(27, 24)$.</font>     \n    \n## **Sequential Bayesian Analysis or Bayesian Learning** \n\n![](images/W08-L1-a.png){fig-align=\"center\"}\n\n![](images/W08-L1-b.png){fig-align=\"center\"}\n\n## **Proving Data Order Invariance** \n\n- **Data order invariance**:\n    - Let $\\theta$ be any parameter of interest with prior pdf $f(\\theta)$.\n    - Then a sequential analysis in which we first observe a data point $y_1$, and then a second data point $y_2$ will produce the same posterior model of $\\theta$ as if we first observe $y_2$ and then $y_1$.\n\n$$f(\\theta|y_1,y_2) = f(\\theta|y_2,y_1)$$ \n\n- Similarly, the posterior model is invariant to whether we observe the data all at once or sequentially.\n\n## **Proving Data Order Invariance** \n\n- Let's first specify the structure of posterior pdf $f(\\theta|y_1,y_2)$, which evolves by sequentially observing data $y_1$, followed by $y_2$.\n\n- In step one, we construct the posterior pdf from our original prior pdf, $f(\\theta)$, and the likelihood function of  $\\theta$ given the first data point $y_1$, $L(\\theta|y_1)$.\n\n$$\n\\begin{align*}\nf(\\theta|y_1) &= \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\\\\n&= \\frac{f(\\theta)L(\\theta|y_1)}{f(y_1)}\n\\end{align*}\n$$\n\n## **Proving Data Order Invariance** \n\n- In step two, we update our model in light of observing new data, $y_2$.\n\n    - Don't forget that we start from the prior model specified by $f(\\theta|y_1)$.\n\n    \n$$\n\\begin{align*}\nf(\\theta|y_2) &= \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\\\\n&= \\frac{\\frac{f(\\theta)L(\\theta|y_1)}{f(y_1)}L(\\theta|y_2)}{f(y_2)} \\\\\n&= \\frac{f(\\theta)L(\\theta|y_1)L(\\theta|y_2)}{f(y_1)f(y_2)}\n\\end{align*}\n$$\n\n## **Proving Data Order Invariance** \n\n- What happens when we observe the data in the opposite order?\n\n$$\n\\begin{align*}\nf(\\theta|y_2) &= \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\\\\n&= \\frac{f(\\theta)L(\\theta|y_2)}{f(y_2)}\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\nf(\\theta|y_1) &= \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\\\\n&= \\frac{\\frac{f(\\theta)L(\\theta|y_2)}{f(y_2)}L(\\theta|y_1)}{f(y_1)} \\\\\n&= \\frac{f(\\theta)L(\\theta|y_2)L(\\theta|y_1)}{f(y_2)f(y_1)}\n\\end{align*}\n$$\n\n## **Proving Data Order Invariance** \n\n- Finally, not only does the order of the data not influence the ultimate posterior model of $\\theta$, but it doesn't matter whether we observe the data all at once or sequentially. \n\n- Suppose we start with the original $f(\\theta)$ prior and observe data $(y_1, y_2)$ together, not sequentially. \n\n- Further, assume that these data points are independent, thus,\n\n$$f(y_1, y_2) = f(y_1) f(y_2) \\text{ and } f(y_1,y_2|\\theta) = f(y_1|\\theta) f(y_2|\\theta)$$\n\n## **Proving Data Order Invariance** \n\n- Then, the posterior pdf is the same as the one resulting from sequential analysis,\n\n$$\n\\begin{align*}\nf(\\theta|y_1,y_2) &= \\frac{f(\\theta)L(\\theta|y_1,y_2)}{f(y_1,y_2)} \\\\\n&= \\frac{f(\\theta)f(y_1,y_2|\\theta)}{f(y_1)f(y_2)} \\\\\n&= \\frac{f(\\theta)f(y_1|\\theta)f(y_2|\\theta)}{f(y_1)f(y_2)} \\\\\n&= \\frac{f(\\theta)L(\\theta|y_1)L(\\theta|y_2)}{f(y_1)f(y_2)}\n\\end{align*}\n$$\n\n## **Homework** \n\n- 4.3\n\n- 4.4 \n\n- 4.6\n\n- 4.9\n\n- 4.15\n\n- 4.16\n\n- 4.17\n\n- 4.18\n\n- 4.19",
    "supporting": [
      "W08-L2-balance-and-sequentiality_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}