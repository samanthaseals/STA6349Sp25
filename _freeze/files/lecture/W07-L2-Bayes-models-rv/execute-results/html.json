{
  "hash": "1e7be59a0ad2286f2dd172f7a58e2d56",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"**Bayesian Modeling for Random Variables**\"\nsubtitle: \"**STA6349: Applied Bayesian Analysis** <br> Spring 2025\"\ndate-format: long\nexecute:\n  echo: true\n  warning: false\n  message: false\n  error: true\nformat: \n  revealjs:\n    code-overflow: wrap\n    embed-resources: true\n    slide-number: false\n    width: 1600\n    height: 900\n    html-math-method: katex\n    theme:\n      - default\n      - sp25.scss\neditor: source\n---\n\n\n\n## **Example**  \n\n- In 1996, Gary Kasparov played a six-game chess match against the IBM supercomputer Deep Blue. \n    - Of the six games, Kasparov won three, drew two, and lost one. \n    - Thus, Kasparov won the overall match. \n\n- Kasparov and Deep Blue were to meet again for a six-game match in 1997. \n\n- Let $\\pi$ denote Kasparov's chances of winning any particular game in the re-match.\n    - Thus, $\\pi$ is a measure of his overall skill relative to Deep Blue. \n    - Given the complexity of chess, machines, and humans, $\\pi$ is unknown and can vary over time.\n        - i.e., $\\pi$ is a *random variable*.\n\n- Our first step is to start with a prior model. This model\n    - Identifies what values $\\pi$ can take,\n    - assigns a prior weight or probability to each, and\n    - these probabilities sum to 1.\n\n## **Example**  \n\n- Based on what we were told, the prior model for $\\pi$ in our example,\n\n| $\\pi$    | 0.2  | 0.5  | 0.8  | Total |\n|----------|:----:|:----:|:----:|:----:|\n| $f(\\pi)$ | 0.10 | 0.25 | 0.65 | 1     |\n\n- Note that this is an incredibly simple model.\n    - The win probability can technically be any number $\\in [0, 1]$.\n    - However, this prior assumes that $\\pi$ has a discrete set of possibilities: 20%, 50%, or 80%.\n\n## **Example** \n\n- In the second step of our analysis, we collect and process data which can inform our understanding of  $\\pi$.\n \n- Here, $Y$ = the number of the six games in the 1997 re-match that Kasparov wins.\n    - As chess match outcome isnâ€™t predetermined, $Y$ is a random variable that can take any value in $\\{1, 2, 3, 4, 5, 6\\}$.\n    \n- Note that $Y$ inherently depends upon $\\pi$.\n    - If $\\pi = 0.80$, $Y$ would also be high (on average).\n    - If $\\pi = 0.20$, $Y$ would also be low (on average).\n    \n- Thus, we must model this dependence of $Y$ on $\\pi$ using a conditional probability model.\n\n## **Binomial Data Model**  \n\n- We must make two assumptions about the chess match:\n    - Games are independent (the outcome of one game does not influence the outcome of another).\n    - Kasparov has an equal probability of winning any game in the match.\n        - i.e., probability of winning does not increase or decrease as the match goes on.\n\n- We will use a binomial model for this problem.\n    - Recall the binomial pmf,\n    \n$$f(y|\\pi) = {n \\choose y} \\pi^y (1-\\pi)^{n-y}, $$    \n\n- In our case,\n\n$$Y|\\pi \\sim \\text{Bin}(6, \\pi)$$\n\n## **Binomial Data Model**  \n\n- Let's assume $\\pi = 0.8$.\n\n- The probability that he would win all 6 games is approximately 26%.\n\n$$f(y=6|\\pi=0.8) = {6 \\choose 6} 0.8^6 (1-0.8)^{6-6}, $$  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(6, 6, 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.262144\n```\n\n\n:::\n:::\n\n\n\n\n- The probability that he would win none of the games is approximately 0%.\n\n$$f(y=0|\\pi=0.8) = {6 \\choose 0} 0.8^0 (1-0.8)^{6-0}, $$  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(0, 6, 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.4e-05\n```\n\n\n:::\n:::\n\n\n\n\n## **Binomial Data Model**  \n\n- Your turn!\n\n- We want to reproduce Figure 2.5 from the [Bayes Rules! textbook (from Section 2.3.2)](https://www.bayesrulesbook.com/chapter-2#cousin-cole).\n\n![](images/fig2-5.png){fig-align=\"center\"} \n\n- Work with your group to come up with that graph.\n\n- *Pick one person to present in 15 minutes*.\n\n## **Binomial Data Model**  \n\n- Note that the Binomial gives us the *theoretical* model of the data we might observe. \n\n    - In the end, Kasparov only won one of the six games against Deep Blue in 1997 ($Y=1$).\n    \n- Next step: how compatible this particular data is with the various possible $\\pi$?\n\n    - What is the likelihood of Kasparov winning $Y=1$ game under each possible $\\pi$?\n    \n- Recall, $f(y|\\pi) = L(\\pi|Y=y)$. When $Y=1$,\n\n$$\n\\begin{align*}\nL(\\pi | y = 1) &= f(y=1|\\pi) \\\\\n&= {6 \\choose 1} \\pi^1 (1-\\pi)^6-1 \\\\\n&= 6\\pi(1-\\pi)^5\n\\end{align*}\n$$\n\n- Note that <u>we do not expect all likelihoods to sum to 1</u>.\n    \n## **Binomial Data Model** \n\n- Your turn! \n\n- Use your results from earlier to tell me the resulting likelihood values.\n\n| $\\pi$        | 0.2    | 0.5    | 0.8 |\n|--------------|:------:|:------:|:----:|\n| $L(\\pi|y=1)$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n\n## **Binomial Data Model** \n\n- Your turn! \n\n- Use your results from earlier to tell me the resulting likelihood values.\n    \n| $\\pi$        | 0.2    | 0.5    | 0.8    |\n|--------------|:------:|:------:|:----:|\n| $L(\\pi|y=1)$ | 0.3932 | 0.0938 | 0.0015 |\n\n- As we can see, the likelihoods do not sum to 1.\n\n## **Normalizing Constant** \n\n- Bayes' Rule requires three pieces of information:\n    - Prior\n    - Likelihood\n    - Normalizing constant\n    \n- **Normalizing constant**: a value that ensures that the sum of all probabilities is equal to 1.\n    - It can be a scalar or a function.\n    - Every probability distribution that does not sum to 1 will ahve a normalizing constant.\n    \n## **Normalizing Constant**     \n    \n- We now must determine the total probability that Kasparov would win $Y=1$ games across all possible win probabilities $\\pi$, $f(y=1)$.\n\n$$\n\\begin{align*}\nf(y=1) &= \\sum_{\\pi \\in \\{0.2, 0.5, 0.8 \\}} L(\\pi |y=1)f(\\pi) \\\\\n&= L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + L(\\pi=0.8|y=1)f(\\pi=0.8) \\\\\n&= \\ ...\n\\end{align*}\n$$\n\n- Work with your group to find the normalizing constant.\n\n## **Normalizing Constant**     \n    \n- We now must determine the total probability that Kasparov would win $Y=1$ games across all possible win probabilities $\\pi$, $f(y=1)$.\n\n$$\n\\begin{align*}\nf(y=1) &= \\sum_{\\pi \\in \\{0.2, 0.5, 0.8 \\}} L(\\pi |y=1)f(\\pi) \\\\\n&= L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + L(\\pi=0.8|y=1)f(\\pi=0.8) \\\\\n&\\approx 0.3932 \\cdot 0.10 + 0.0938 \\cdot 0.25 + 0.0015 \\cdot 0.65 \\\\\n&\\approx 0.0637\n\\end{align*}\n$$\n\n- Across all possible values of $\\pi$, there is about a 6% chance that Kasparov would have won only one game.\n\n## **Posterior Probability Model**   \n\n- Now recall,\n\n$$\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}$$\n\n- In our example, where $y = 1$,\n\n$$f(\\pi | y=1) = \\frac{f(\\pi) L(\\pi | y = 1)}{f(y=1)} \\  \\text{for} \\ \\pi \\in \\{ 0.2, 0.5, 0.8\\}$$\n\n- Work with your group to find the posterior probabilities.\n\n    - You will have one posterior probability for each value of $\\pi$.\n\n## **Posterior Probability Model** \n\n- Note!! We do not have to calculate the normalizing constant!\n\n- We can note that $f(Y=y) = 1/c$.\n\n- Then, we say that \n\n$$\n\\begin{align*}\nf(\\pi | y) &= \\frac{f(\\pi) L(\\pi|y)}{f(y)} \\\\\n& \\propto  f(\\pi) L(\\pi|y) \\\\\n\\\\\n\\text{posterior} &\\propto \\text{prior} \\cdot \\text{likelihood}\n\\end{align*}\n$$\n\n## **Wrap Up** \n\n- Today we have gone through how to find posterior probabilities using the binomial distribution.\n\n- Next week, we will learn about the Beta-Binomial model.\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}