{
  "hash": "485ef2bd46ad6895292c00081cb727f2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"**Posterior Inference and Prediction**\"\nsubtitle: \"**STA6349: Applied Bayesian Analysis** <br> Spring 2025\"\ndate-format: long\nexecute:\n  echo: true\n  warning: false\n  message: false\n  error: true\nformat: \n  revealjs:\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    slide-number: false\n    width: 1600\n    height: 900\n    html-math-method: katex\n    theme:\n      - default\n      - sp25.scss\neditor: source\n---\n\n\n\n## **Introduction**  \n\n- Now that we know how to find posterior distributions, we will discuss how to use them to make inference.\n\n- Here are the packages we need today: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(bayesplot)\nlibrary(broom.mixed)\nlibrary(janitor)\ndata(\"moma_sample\")\n```\n:::\n\n\n\n## **Working Example**  \n\n- Imagine you find yourself standing at the Museum of Modern Art (MoMA) in New York City, captivated by the artwork in front of you. \n    \n- While understanding that \"modern\" art doesn’t necessarily mean \"new\" art, a question still bubbles up: what are the chances that this modern artist is Gen X or even younger, i.e., born in 1965 or later? \n    \n- Let $\\pi$ denote the proportion of artists represented in major U.S. modern art museums that are Gen X or younger. \n    \n- The Beta(4,6) prior model for $\\pi$ reflects our own very vague prior assumption that major modern art museums disproportionately display artists born before 1965, i.e., $\\pi$  most likely falls below 0.5. \n    \n- After all, \"modern art\" dates back to the 1880s and it can take a while to attain such high recognition in the art world.\n\n## **Working Example**  \n\n- The Beta(4,6) prior model for $\\pi$ reflects our own very vague prior assumption\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W12-L1-posterior-analysis_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Working Example**  \n\n- Let's consider the following dataset:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(moma_sample)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"artist\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"country\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"birth\"],\"name\":[3],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"death\"],\"name\":[4],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"alive\"],\"name\":[5],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"genx\"],\"name\":[6],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"gender\"],\"name\":[7],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"count\"],\"name\":[8],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"year_acquired_min\"],\"name\":[9],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"year_acquired_max\"],\"name\":[10],\"type\":[\"fct\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Ad Gerritsen\",\"2\":\"dutch\",\"3\":\"1940\",\"4\":\"2015\",\"5\":\"FALSE\",\"6\":\"FALSE\",\"7\":\"male\",\"8\":\"1\",\"9\":\"1981\",\"10\":\"1981\",\"_rn_\":\"1\"},{\"1\":\"Kirstine Roepstorff\",\"2\":\"danish\",\"3\":\"1972\",\"4\":\"NA\",\"5\":\"TRUE\",\"6\":\"TRUE\",\"7\":\"female\",\"8\":\"3\",\"9\":\"2005\",\"10\":\"2005\",\"_rn_\":\"2\"},{\"1\":\"Lisa Baumgardner\",\"2\":\"american\",\"3\":\"1958\",\"4\":\"2015\",\"5\":\"FALSE\",\"6\":\"FALSE\",\"7\":\"female\",\"8\":\"2\",\"9\":\"2016\",\"10\":\"2016\",\"_rn_\":\"3\"},{\"1\":\"David Bates\",\"2\":\"american\",\"3\":\"1952\",\"4\":\"NA\",\"5\":\"TRUE\",\"6\":\"FALSE\",\"7\":\"male\",\"8\":\"1\",\"9\":\"2001\",\"10\":\"2001\",\"_rn_\":\"4\"},{\"1\":\"Simon Levy\",\"2\":\"american\",\"3\":\"1946\",\"4\":\"NA\",\"5\":\"TRUE\",\"6\":\"FALSE\",\"7\":\"male\",\"8\":\"1\",\"9\":\"2012\",\"10\":\"2012\",\"_rn_\":\"5\"},{\"1\":\"Pierre Mercure\",\"2\":\"canadian\",\"3\":\"1927\",\"4\":\"1966\",\"5\":\"FALSE\",\"6\":\"FALSE\",\"7\":\"male\",\"8\":\"8\",\"9\":\"2008\",\"10\":\"2008\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## **Working Example**  \n\n- Counting the number of Generation X and younger,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoma_sample %>% \n  group_by(genx) %>% \n  tally()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"genx\"],\"name\":[1],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"FALSE\",\"2\":\"86\"},{\"1\":\"TRUE\",\"2\":\"14\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## **Working Example**  \n    \n- Our modeling is as follows,     \n\n$$\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(100,\\pi) \\\\\n\\pi &\\sim \\text{Beta}(4,6) \\\\\n\\pi | (Y = 14) &\\sim \\text{Beta(18,92)}\n\\end{align*}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarize_beta_binomial(alpha = 4, beta = 6,\n                        y = 14, n = nrow(moma_sample))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"model\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"alpha\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"beta\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mode\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"var\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sd\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"prior\",\"2\":\"4\",\"3\":\"6\",\"4\":\"0.4000000\",\"5\":\"0.3750000\",\"6\":\"0.021818182\",\"7\":\"0.14770979\"},{\"1\":\"posterior\",\"2\":\"18\",\"3\":\"92\",\"4\":\"0.1636364\",\"5\":\"0.1574074\",\"6\":\"0.001232969\",\"7\":\"0.03511365\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## **Working Example**  \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta_binomial(alpha = 4, beta = 6,\n                   y = 14, n = nrow(moma_sample)) +\n  theme_bw() \n```\n\n::: {.cell-output-display}\n![](W12-L1-posterior-analysis_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Working Example**  \n\n- We must be able to utilize this posterior to perform a rigorous posterior analysis of  $\\pi$.\n\n- There are three common tasks in posterior analysis: \n    - estimation, \n    - hypothesis testing, and \n    - prediction. \n    \n- For example, \n    - What’s our estimate of  $\\pi$?\n    - Does our model support the claim that fewer than 20% of museum artists are Gen X or younger? \n    - If we sample 20 more museum artists, how many do we predict will be Gen X or younger?\n\n## **Posterior Estimation**  \n\n- We can construct **posterior credible intervals**.\n    - Let $\\theta$ have posterior pmf $f(\\theta|y)$.\n    - A posterior credible interval (CI) provides a range of posterior plausible values of $\\theta$, and thus a summary of both posterior central tendency and variability. \n    - A middle 95% CI is constructed by the 2.5th and 97.5th posterior percentiles, $$\\left( \\theta_{0.025}, \\theta_{0.975} \\right),$$ and there is a 95% posterior probability that $\\theta$ is in this range,\n    \n$$P\\left[ \\theta \\in (\\theta_{0.025}, \\theta_{0.975})|Y=y \\right] = \\int_{\\theta_{0.025}}^{\\theta_{0.975}} f(\\theta|y) d\\theta = 0.95$$    \n\n## **Posterior Estimation**  \n\n- Recall the Beta(18, 92) posterior model for $\\pi$, the proportion of modern art museum artists that are Gen X or younger. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqbeta(c(0.025, 0.975), 18, 92) # 95% CI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1009084 0.2379286\n```\n\n\n:::\n:::\n\n\n\n- There is a 95% posterior probability that somewhere between 10% and 24% of museum artists are Gen X or younger.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqbeta(c(0.25, 0.75), 18, 92) # 50% CI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1388414 0.1862197\n```\n\n\n:::\n:::\n\n\n\n- There is a 50% posterior probability that somewhere between 14% and 19% of museum artists are Gen X or younger.\n\n## **Posterior Estimation**  \n\n- 95% is a common choice, however, note that it is somewhat arbitrary and used because of decades of tradition.\n    \n<center><img src = \"images/W13-L1-a.png\" width=1000></center>    \n\n- There is no one right credible interval.\n    - It will just depend on the context of the analysis.\n\n## **Posterior Hypothesis Testing**  \n\n- Suppose we read an article claiming that fewer than 20% of museum artists are Gen X or younger. \n\n<center><img src = \"images/W13-L1-b.png\"></center>\n\n- How plausible is it that $\\pi < 0.2$?\n\n$$\nP\\left[ \\pi < 0.2 | Y = 14 \\right] = \\int_0^{0.2} f(\\pi|y = 14) d\\pi\n$$\n\n## **Posterior Hypothesis Testing**  \n\n- Posterior Probability\n\n$$\nP\\left[ \\pi < 0.2 | Y = 14 \\right] = \\int_0^{0.2} f(\\pi|y = 14) d\\pi\n$$\n\n- We can find this probability by using the `pbeta()` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbeta(0.20, 18, 92)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8489856\n```\n\n\n:::\n:::\n\n\n\n- Thus, \n\n$$\nP\\left[ \\pi < 0.2 | Y = 14 \\right] = 0.849\n$$\n\n- There is approximately an 84.9% posterior chance that Gen Xers account for fewer than 20% of modern art museum artists.\n\n## **Posterior Hypothesis Testing**  \n\n- Prior Probability\n\n$$\nP\\left[ \\pi < 0.2 \\right] = \\int_0^{0.2} f(\\pi) d\\pi\n$$\n\n- We can find this probability by using the `pbeta()` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbeta(0.20, 4, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.08564173\n```\n\n\n:::\n:::\n\n\n\n- Thus, \n\n$$\nP\\left[ \\pi < 0.2 \\right] = 0.086\n$$\n\n- There is approximately an 8.6% prior chance that Gen Xers account for fewer than 20% of modern art museum artists.\n\n## **Posterior Hypothesis Testing**  \n\n- We can create a table of information we will use to make inferences:\n\n<div class=\"tg-wrap\"><table><tbody>\n  <tr>\n    <td><b><center>Hypotheses</center></b></td>\n    <td><b><center>Prior Probability</center></b></td>\n    <td><b><center>Posterior Probability</center></b></td>\n  </tr>\n  <tr>\n    <td>$H_0: \\pi \\ge 0.2$</td>\n    <td>$P[H_0] = 0.914$</td>\n    <td>$P[H_0 | Y= 14] = 0.151$</td>\n  </tr>\n  <tr>\n    <td>$H_1: \\pi < 0.2$</td>\n    <td>$P[H_1] = 0.086$</td>\n    <td>$P[H_1 | Y = 14] = 0.849$</td>\n  </tr>\n</tbody>\n</table></div>\n\n## Posterior Hypothesis Testing  \n\n- Let's find the posterior odds:\n\n<div class=\"tg-wrap\"><table><tbody>\n  <tr>\n    <td><b><center>Hypotheses</center></b></td>\n    <td><b><center>Prior Probability</center></b></td>\n    <td><b><center>Posterior Probability</center></b></td>\n  </tr>\n  <tr>\n    <td>$H_0: \\pi \\ge 0.2$</td>\n    <td>$P[H_0] = 0.914$</td>\n    <td>$P[H_0 | Y= 14] = 0.151$</td>\n  </tr>\n  <tr>\n    <td>$H_1: \\pi < 0.2$</td>\n    <td>$P[H_1] = 0.086$</td>\n    <td>$P[H_1 | Y = 14] = 0.849$</td>\n  </tr>\n</tbody>\n</table></div>\n\n$$\n\\begin{align*}\n\\text{posterior odds} &= \\frac{P\\left[ H_1 | Y = 14 \\right]}{P\\left[ H_0 | Y = 14 \\right]} \\\\\n&= \\frac{0.849}{0.151} \\\\\n&\\approx 5.62\n\\end{align*}\n$$ \n\n- Our posterior assessment suggests that $\\pi$ is 5.62 times more likely to be *below* 0.2 rather than being *above* 0.2.\n\n## **Posterior Hypothesis Testing**  \n\n- Let's find the prior odds:\n\n<div class=\"tg-wrap\"><table><tbody>\n  <tr>\n    <td><b><center>Hypotheses</center></b></td>\n    <td><b><center>Prior Probability</center></b></td>\n    <td><b><center>Posterior Probability</center></b></td>\n  </tr>\n  <tr>\n    <td>$H_0: \\pi \\ge 0.2$</td>\n    <td>$P[H_0] = 0.914$</td>\n    <td>$P[H_0 | Y= 14] = 0.151$</td>\n  </tr>\n  <tr>\n    <td>$H_1: \\pi < 0.2$</td>\n    <td>$P[H_1] = 0.086$</td>\n    <td>$P[H_1 | Y = 14] = 0.849$</td>\n  </tr>\n</tbody>\n</table></div>\n\n$$\n\\begin{align*}\n\\text{prior odds} &= \\frac{P\\left[ H_1 \\right]}{P\\left[ H_0 \\right]} \\\\\n&= \\frac{0.086}{0.914} \\\\\n&\\approx 0.093\n\\end{align*}\n$$ \n\n## **Posterior Hypothesis Testing**  \n\n- Bayes Factor\n    - When we are comparing two competing hypotheses, $H_0$ vs. $H_1$, the Bayes Factor is an odds ratio for $H_1$:\n    \n$$\n\\text{Bayes Factor} = \\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{P\\left[H_1 | Y\\right] / P\\left[H_0 | Y\\right]}{P\\left[H_1\\right] / P\\left[H_0\\right]}\n$$\n\n- We will compare this value to 1.\n    - BF = 1: The plausibility of $H_1$ did not change in light of the observed data.\n    - BF > 1: The plausibility of $H_1$ increased in light of the observed data.\n        - The greater the Bayes Factor, the more convincing the evidence for $H_1$.\n    - BF < 1: The plausibilty of $H_1$ decreased in light of the observed data.        \n  \n## **Posterior Hypothesis Testing**  \n\n- Let's now find the Bayes Factor:\n\n<div class=\"tg-wrap\"><table><tbody>\n  <tr>\n    <td><b><center>Hypotheses</center></b></td>\n    <td><b><center>Prior Probability</center></b></td>\n    <td><b><center>Posterior Probability</center></b></td>\n  </tr>\n  <tr>\n    <td>$H_0: \\pi \\ge 0.2$</td>\n    <td>$P[H_0] = 0.914$</td>\n    <td>$P[H_0 | Y= 14] = 0.151$</td>\n  </tr>\n  <tr>\n    <td>$H_1: \\pi < 0.2$</td>\n    <td>$P[H_1] = 0.086$</td>\n    <td>$P[H_1 | Y = 14] = 0.849$</td>\n  </tr>\n</tbody>\n</table></div>\n\n$$\n\\begin{align*}\n\\text{Bayes Factor} &= \\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{P\\left[H_1 | Y\\right] / P\\left[H_0 | Y\\right]}{P\\left[H_1\\right] / P\\left[H_0\\right]} \\\\ \n&= \\frac{5.62}{0.09} \\\\\n&\\approx 62.44\n\\end{align*}\n$$\n\n## **Posterior Hypothesis Testing**  \n\n- Let's now find the Bayes Factor:\n\n<div class=\"tg-wrap\"><table><tbody>\n  <tr>\n    <td><b><center>Hypotheses</center></b></td>\n    <td><b><center>Prior Probability</center></b></td>\n    <td><b><center>Posterior Probability</center></b></td>\n  </tr>\n  <tr>\n    <td>$H_0: \\pi \\ge 0.2$</td>\n    <td>$P[H_0] = 0.914$</td>\n    <td>$P[H_0 | Y= 14] = 0.151$</td>\n  </tr>\n  <tr>\n    <td>$H_1: \\pi < 0.2$</td>\n    <td>$P[H_1] = 0.086$</td>\n    <td>$P[H_1 | Y = 14] = 0.849$</td>\n  </tr>\n</tbody>\n</table></div>\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_odds <- pbeta(0.20, 4, 6)/(1-pbeta(0.20, 4, 6))\npost_odds <- pbeta(0.20, 18, 92)/(1-pbeta(0.20, 18, 92))\n(BF <- post_odds/prior_odds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 60.02232\n```\n\n\n:::\n:::\n\n\n\n## **Posterior Hypothesis Testing**  \n\n- It's time to draw a conclusion!\n    - Posterior probability: 0.85\n    - Bayes factor: 60\n    \n- We have fairly convincing evidence in factor of the claim that fewer than 20% of artists at major modern art museums are Gen X or younger.\n    \n- This gives us more information - we have a holistic measure of the level of uncertainty about the claim.\n    - This should help us inform our next steps.\n    \n## **Posterior Hypothesis Testing**  \n\n- We now want to test whether or not 30% of major museum artists are Gen X or younger.\n\n$$\n\\begin{align*}\nH_0&: \\ \\pi = 0.3 \\\\\nH_1&: \\ \\pi \\ne 0.3\n\\end{align*}\n$$\n\n- Why is this an issue?\n    \n$$\nP\\left[ \\pi =0.3 | Y = 14 \\right] = \\int_{0.3}^{0.3} f(\\pi|y = 14) d\\pi = 0\n$$\n\n- and even worse,\n\n$$\n\\text{posterior odds}  = \\frac{P\\left[ H_1 | Y = 14 \\right]}{P\\left[ H_0 | Y = 14 \\right]} = \\frac{1}{0}\n$$\n\n## **Posterior Hypothesis Testing**  \n\n- Welp.\n\n## **Posterior Hypothesis Testing**  \n\n- Welp.\n\n- Let's think about the 95% posterior credible interval for $\\pi$: (0.10, 0.24).\n    - Do we think that 0.3 is a plausible value?\n\n## **Posterior Hypothesis Testing**  \n\n- Welp.\n\n- Let's think about the 95% posterior credible interval for $\\pi$: (0.10, 0.24).\n    - Do we think that 0.3 is a plausible value?\n    \n- Let's reframe our hypotheses:\n\n$$\n\\begin{align*}\nH_0&: \\ \\pi \\in (0.25, 0.35) \\\\\nH_1&: \\ \\pi \\not\\in (0.25, 0.35)\n\\end{align*}\n$$\n\n- Now, we can more rigorously claim belief in $H_1$.\n    - The entire hypothesized range is above the 95% CI.\n    \n- This also allows us a way to construct our hypothesis test with posterior probability and Bayes Factor.\n\n## **Posterior Prediction** \n\n- In addition to estimating the posterior and using the posterior distribution for hypothesis testing, we may be interested in predicting the outcome in a new dataset.\n\n- Example: Suppose we get our hands on data for 20 more artworks displayed at the museum. Based on the posterior understanding of $\\pi$ that we've developed throughout this chapter, what number would you predict are done by artists that are Gen X or younger?\n\n## **Posterior Prediction** \n\n- Example: Suppose we get our hands on data for 20 more artworks displayed at the museum. Based on the posterior understanding of $\\pi$ that we've developed throughout this chapter, what number would you predict are done by artists that are Gen X or younger?\n    - Logical response:\n        - posterior guess was 16%\n        - $n = 20$\n        - $n \\hat{\\pi} = 20(0.16) = 3.2$\n    - ... can we have 3.2 artworks?    \n\n## **Posterior Prediction** \n\n- Example: Suppose we get our hands on data for 20 more artworks displayed at the museum. Based on the posterior understanding of $\\pi$ that we've developed throughout this chapter, what number would you predict are done by artists that are Gen X or younger?\n    - Two sources of variability in prediction:\n        - Sampling variability: if we randomly sample $n$ artists, we do not expect $n \\hat{\\pi} \\in \\{1, 2, ..., n\\}$.\n        - Posterior variability in $\\pi$: we know that 0.16 is not the singular posterior plausible value of $\\pi$.\n            - 95% credible interval for $\\pi$: $(0.10, 0.24)$\n            - What do we expect to see under each possible $\\pi$?\n            \n## **Posterior Prediction** \n\n- Let's look at combining the sampling variability in our new $Y$ and posterior variability in $\\pi$.\n    - Let $Y' = y'$ be the (unknown) number of the 20 new artwork that are done by Gen X or younger artists.\n        - $y' \\in \\{0, 1, ..., 20\\}$.\n    - Conditioned on $\\pi$, the sampling variability in $Y'$ can be modeled\n    \n$$\nY'|\\pi \\sim \\text{Bin}(20, \\pi)\n$$\n\n\n$$\nf(y'|\\pi) = P[Y' = y'|\\pi] = {20\\choose{y'}} \\pi^{y'} (1-\\pi)^{20-y'}\n$$\n\n## **Posterior Prediction** \n\n- We can weight $f(y'|\\pi)$ by the posterior pdf, $f(\\pi|y=14)$.\n    - This captures the chance of observing $Y' = y'$ Gen Xers for a given $\\pi$\n    - At the same time, this accounts for the posterior plausibility of $\\pi$.\n    \n$$f(y'|\\pi) f(\\pi|y=14)$$\n\n## **Posterior Prediction** \n\n- This leads us to the posterior predictive model of $Y'$.\n\n$$ \nf(y'|y) = \\int f(y'|\\pi) f(\\pi|y) \\ d \\pi\n$$\n\n- The overall chance of observing $Y' = y'$ weights the chance of observing this outcome under *any* possible $\\pi$ by the posterior plausibility of $\\pi$.\n    - Chance of observing this outcome under any $\\pi$: $f(y'|\\pi)$.\n    - Posterior plausibility of $\\pi$: $f(\\pi|y)$.\n    \n## **Posterior Simulation** \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 1: DEFINE the model\nart_model <- \"\n  data {\n    int<lower = 0, upper = 100> Y;\n  }\n  parameters {\n    real<lower = 0, upper = 1> pi;\n  }\n  model {\n    Y ~ binomial(100, pi);\n    pi ~ beta(4, 6);\n  }\n\"\n```\n:::\n\n\n\n## **Posterior Simulation** \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 2: SIMULATE the posterior\nart_sim <- stan(model_code = art_model, data = list(Y = 14),  chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 1:                0.013 seconds (Sampling)\nChain 1:                0.026 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.014 seconds (Sampling)\nChain 2:                0.027 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 3:                0.014 seconds (Sampling)\nChain 3:                0.027 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 4:                0.011 seconds (Sampling)\nChain 4:                0.023 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n    \n## **Posterior Simulation** \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parallel trace plot\nmcmc_trace(art_sim, pars = \"pi\", size = 0.5) + xlab(\"iteration\")\n```\n\n::: {.cell-output-display}\n![](W12-L1-posterior-analysis_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Posterior Simulation** \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Density plot\nmcmc_dens_overlay(art_sim, pars = \"pi\")\n```\n\n::: {.cell-output-display}\n![](W12-L1-posterior-analysis_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Posterior Simulation** \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Autocorrelation plot\nmcmc_acf(art_sim, pars = \"pi\")\n```\n\n::: {.cell-output-display}\n![](W12-L1-posterior-analysis_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Posterior Simulation** \n\n- As we saw previously, the posterior was Beta(18, 92).\n\n- We will use the `tidy()` function from the `broom.mixed` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom.mixed)\ntidy(art_sim, conf.int = TRUE, conf.level = 0.95)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"pi\",\"2\":\"0.1635453\",\"3\":\"0.03536733\",\"4\":\"0.1000882\",\"5\":\"0.2391218\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n- The approximate middle 95% CI for $\\pi$ is (0.100, 0.239).\n\n- Our approximation of the actual posterior median is 0.162.\n\n## **Posterior Simulation** \n\n- We can use the `mcmc_areas()` function from the `bayesrules` package to get a corresponding graph,\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_areas(art_sim, pars = \"pi\", prob = 0.95)\n```\n\n::: {.cell-output-display}\n![](W12-L1-posterior-analysis_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n\n## **Posterior Simulation** \n\n- Unfortunately, `tidy()` does not give everything we may be interested in.\n    - We can save the Markov chain values to a dataset and analyze. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Store the 4 chains in 1 data frame\nart_chains_df <- as.data.frame(art_sim, pars = \"lp__\", include = FALSE)\ndim(art_chains_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 20000     1\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(art_chains_df, n=3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"pi\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.1300701\",\"_rn_\":\"1\"},{\"1\":\"0.1755285\",\"_rn_\":\"2\"},{\"1\":\"0.2214175\",\"_rn_\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## **Posterior Simulation** \n\n- We can then summarize the Markov chain values,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nart_chains_df %>% \n  summarize(post_mean = mean(pi), \n            post_median = median(pi),\n            post_mode = sample_mode(pi),\n            lower_95 = quantile(pi, 0.025),\n            upper_95 = quantile(pi, 0.975))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"post_mean\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"post_median\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"post_mode\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lower_95\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upper_95\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.1635453\",\"2\":\"0.1615222\",\"3\":\"0.1583033\",\"4\":\"0.1000882\",\"5\":\"0.2391218\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n- We have reproduced/verified the results from `tidy()` (and then some!)\n\n## **Posterior Simulation**\n\n- Now that we have saved the Markov chain values, we can use them to answer questions about the data.\n\n- Recall, we were interested in testing the claim that fewer than 20% of major museum artists are Gen X.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nart_chains_df %>% \n  mutate(exceeds = pi < 0.20) %>% \n  tabyl(exceeds)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"exceeds\"],\"name\":[1],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"percent\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"FALSE\",\"2\":\"3061\",\"3\":\"0.15305\"},{\"1\":\"TRUE\",\"2\":\"16939\",\"3\":\"0.84695\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n- By the posterior, there is an 84.6% chance that Gen X artist representation is under 20%.\n\n## **Posterior Simulation** \n\n- Let us compare the results between using conjugate family knowledge and MCMC.\n\n<center><img src = \"images/W13-L1-d.png\"></center>\n\n- From this, we can see that MCMC gave us an accurate approximation.\n\n- We should use this as \"proof\" that the approximations are \"reliable\" for non-conjugate families.\n    - Always look at diagnostics!\n\n## **Homework** \n\n- 8.4\n\n- 8.8\n\n- 8.9\n\n- 8.10\n\n- 8.14 \n\n- 8.15 \n\n- 8.16\n",
    "supporting": [
      "W12-L1-posterior-analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}