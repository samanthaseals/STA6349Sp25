{
  "hash": "105617fed3dea01bbf0b02c8bd6b817e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"**Simple Normal Regression**\"\nsubtitle: \"**STA6349: Applied Bayesian Analysis** <br> Spring 2025\"\ndate-format: long\nexecute:\n  echo: true\n  warning: false\n  message: false\n  error: true\nformat: \n  revealjs:\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    slide-number: false\n    width: 1600\n    height: 900\n    html-math-method: katex\n    theme:\n      - default\n      - sp25.scss\neditor: source\n---\n\n\n\n## **Introduction**  \n\n- Before today, we effectively were performing one-sample tests of means, proportions, and counts.\n\n- Now, we will focus on incorporating just a single predictor into our analysis. \n    - Overall question: what is the relationship between $Y$ (outcome) and $X$ (predictor)?\n\n- We now switch to thinking about analysis in terms of regression.\n \n    \n## **Working Example**\n\n- *Capital Bikeshare* is a bike sharing service in the Washington, D.C. area. To best serve its registered members, the company must understand the demand for its service. We will analyze the number of rides taken on a random sample of $n$ days, $(Y_1, Y_2, ..., Y_n)$.\n\n- Beacuse $Y_i$ is a count variable, you might assume that ridership might need the Poisson distribution. However, past bike riding seasons have exhibited bell-shaped daily ridership with a variability in ridership that far exceeds the typical ridership.\n    - (i.e., the Poisson assumption of $\\mu = \\sigma$ does not hold here)\n    \n- We will instead assume that the number of rides varies normally around some typical ridership,  $\\mu$, with standard deviation, $\\sigma$. \n\n$$ Y_i|\\mu,\\sigma \\overset{\\text{ind}}{\\sim} N(\\mu, \\sigma^2)$$\n\n## **Working Example**\n\n- In our example, $Y$ is the number of rides and $X$ is the temperature.\n   \n- Our specific goal will be to model the relationship between ridership and temperature: \n    - Does ridership tend to increase on warmer days? \n    - If so, by how much? \n    - How strong is this relationship?\n    \n- It is reasonable to assume a positive relationship between temperature and number of rides.\n    - As it warms up outside, folks are more likely to puruse outdoor activities, including biking.    \n    \n- Today we will focus on the model with a single predictor (`y ~ x` in R).\n    \n## **Building the Regression Model**\n\n- Suppose we have *n* data pairs,\n\n$$ \\{ (Y_1, X_1), (Y_2, X_2), ..., (Y_n, X_n) \\} $$\n\n- where $Y_i$ is the number of rides and $X_i$ is the high temperature (^o^F) on day $i$.\n\n- Assuming that the relationship is linear, we can model\n\n$$\\mu_i = \\beta_0 + \\beta_1 X_i,$$\n\n- where $\\beta_0$ and $\\beta_1$ are model coefficients.\n\n## **Building the Regression Model**\n\n- What do we mean by \"model coefficients?\"\n\n$$\\mu_i = \\beta_0 + \\beta_1 X_i,$$\n\n- $\\beta_0$ is the baseline for where our model crosses the $y$-axis, i.e., when $X_i=0$.\n    - Is this meaningful when we are talking about the average ridership when it is $0$^o^F in DC?\n\n- $\\beta_1$ is the slope, or average change, in the outcome ($Y$) for a one unit increase in the predictor ($X$).\n    - Interpretation: for a [1 unit of predictor] increase in [the predictor], [the outcome] [increases or decreases] by [abs($\\beta_1$)].\n    - In our example, suppose $\\beta_1=4.5$. For a 1^o^F increase in the temperature, the ridership increases by 4.5 riders.\n\n## **Building the Regression Model**\n\n- We are now interested in the model\n\n$$Y_i|\\beta_0,\\beta_1,\\sigma\\overset{\\text{ind}}{\\sim} N(\\mu_i, \\sigma^2) \\text{ with } \\mu_i = \\beta_0 + \\beta_1 X_i$$\n\n- Note that $\\mu_i$ is the *local mean* (for a specific value of $X$).\n    - In our data, $\\mu_i$ is the mean ridership for day $i$.\n\n- The *global mean* (regardless of the value of $X$) is given by $\\mu$.\n    - In our data, $\\mu$ is the mean ridership, regardless of day.\n    \n- Under this model, $\\sigma$ is now measuring the variability from the local mean.    \n\n## **Building the Regression Model**\n\n- We know the assumptions for linear regression in the frequentist framework. \n\n- In Bayesian Normal regression,\n    - The observations are *independent*.\n    - $Y$ can be written as a *linear function* of $X$, $\\mu = \\beta_0 + \\beta_1 X$.\n    - For any value of $X$, $Y$ varies *normally* around $\\mu$ with constant standard deviation $\\sigma$.\n    \n## **Building the Regression Model**    \n    \n- We know we must set priors for our parameters.\n    - Here, we have three parameters: $\\beta_0$, $\\beta_1$, $\\sigma$.\n    - There are multiple ways to set up the priors, but we will stick to the default framework from `rstanarm`.\n    \n- First, we will assume that our prior models of $\\beta_0$, $\\beta_1$, and $\\sigma$ are independent.\n\n- It is common to use the a normal prior for $\\beta_0$ and $\\beta_1$.\n\n$$ \n\\begin{align*}\n  \\beta_0 &\\sim N(m_0, s_0^2) \\\\\n  \\beta_1 &\\sim N(m_1, s_1^2)\n\\end{align*}\n$$\n\n- Then, it is the default to use the exponential for $\\sigma$; both are restricted to positive values.\n\n$$\\sigma \\sim \\text{Exp}(l)$$\n\n- Note that $E[\\sigma] = 1/l$ and $\\text{sd}[\\sigma] = 1/l$.\n\n## **Building the Regression Model**   \n\n- Thus, our regression model has the following formulation:\n\n$$\n\\begin{align*}\n&\\text{data}: & Y_i|\\beta_0, \\beta_1, \\sigma & \\overset{\\text{ind}}{\\sim} N(\\mu_i, \\sigma^2) \\text{ with } \\mu_i = \\beta_0 + \\beta_1 X_i \\\\\n&\\text{priors:} &\\beta_0 & \\sim N(m_0, s_0^2) \\\\\n& & \\beta_1  &\\sim N(m_1, s_1^2) \\\\\n& & & \\sigma \\sim \\text{Exp}(l)\n\\end{align*}\n$$\n\n## **Tuning the Prior Models**   \n\n- Based on the past bikeshare analyses, we have the following *centered* prior understandings.\n\n1. On an average temperature day for DC (65^o^F-70^o^F), there are typically around 5000 riders, but this could vary between 3000 and 7000 riders.\n\n$$\\beta_{0\\text{c}} \\sim N(5000, 1000^2)$$\n\n2. For every one degree increase in temperature, ridership typically increases by 100 rides, but this could vary between 20 and 180 rides.\n\n$$\\beta_{1\\text{c}} \\sim N(100, 40^2)$$\n\n3. At any given temperature, the daily ridership will tend to vary with a moderate standard deviation of 1250 rides.\n\n$$ \\sigma \\sim \\text{Exp}(0.0008) $$\n\n- Recall, $E[\\sigma] = 1/l = 1250$, so $l = 1/1250 = 0.0008$.\n    \n## **Tuning the Prior Models**  \n\n$$\\beta_{0\\text{c}} \\sim N(5000, 1000^2) \\ \\ \\ \\beta_{1\\text{c}} \\sim N(100, 40^2) \\ \\ \\ \\sigma \\sim \\text{Exp}(0.0008)$$\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W13-L1-simple-regression_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Simulating the Posterior**\n\n- Let's now update what we know using the `bikes` data in the `bayesrule` package.\n\n- Looking at the basic relationship between the number of rides vs. the temperature (as it feels outside),\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W13-L1-simple-regression_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Simulating the Posterior**\n\n- Note: I am skipping the derivation / the true math behind how to find the posterior in this situation. (It involves multiple integrals!)\n\n- We will use the `stan_glm()` function from `rstanarm` -- it contains pre-defined Bayesian regression models. \n    - `stan_glm()` also applies to the wider family of GzLM (i.e., logistic & Poisson/negbin).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstanarm)\nbike_model <- stan_glm(rides ~ temp_feel, # data model\n                       data = bikes, # dataset\n                       family = gaussian, # distribution to apply\n                       prior_intercept = normal(5000, 1000), # b0_c\n                       prior = normal(100, 40), # b1_c\n                       prior_aux = exponential(0.0008), # sigma\n                       chains = 4, # 4 chains\n                       iter = 5000*2, # 10000 iterations - throw out first 5000\n                       seed = 84735) # starting place in RNG\n```\n:::\n\n\n\n## **Simulating the Posterior**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_model <- stan_glm(rides ~ temp_feel, # data model\n                       data = bikes, # dataset\n                       family = gaussian, # distribution to apply\n                       prior_intercept = normal(5000, 1000), # b0_c\n                       prior = normal(100, 40), # b1_c\n                       prior_aux = exponential(0.0008), # sigma\n                       chains = 4, # 4 chains\n                       iter = 5000*2, # 10000 iterations - throw out first 5000\n                       seed = 84735) # starting place in RNG\n```\n:::\n\n\n\n- `stan_glm()` contains three types of information:\n    - Data information:  The first three arguments specify the structure of our data. \n        - We want to model ridership by temperature (`rides ~ temp_feel`) using `data = bikes` and assuming a Normal data model, aka `family = gaussian`.\n    - Prior information: The `prior_intercept`, `prior`, and `prior_aux` arguments give the priors for  $\\beta_{0\\text{c}}$, $\\beta_{1\\text{c}}$, $\\sigma$.\n    - Markov chain information: The `chains`, `iter`, and `seed` arguments specify the number of Markov chains to run, the length or number of iterations for each chain, and the starting place of the RNG.\n    \n## **Simulating the Posterior**\n\n- Wait, how does this work when we are looking at three model parameters?\n\n- We will have three vectors -- one for each model parameter.\n\n$$\n\\begin{align*}\n  <\\beta_0^{(1)}, & \\ \\beta_0^{(2)}, ..., \\beta_0^{(5000)}> \\\\\n  <\\beta_1^{(1)}, & \\ \\beta_1^{(2)}, ..., \\beta_1^{(5000)} > \\\\\n<\\sigma^{(1)}, & \\ \\sigma^{(2)}, ..., \\sigma^{(5000)} > \\\\\n\\end{align*}\n$$\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## **Simulation Diagnostics**\n\n- Run the simulation code:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstanarm)\nbike_model <- stan_glm(rides ~ temp_feel, # data model\n                       data = bikes, # dataset\n                       family = gaussian, # distribution to apply\n                       prior_intercept = normal(5000, 1000), # b0_c\n                       prior = normal(100, 40), # b1_c\n                       prior_aux = exponential(0.0008), # sigma\n                       chains = 4, # 4 chains\n                       iter = 5000*2, # 10000 iterations - throw out first 5000\n                       seed = 84735) # starting place in RNG\n```\n:::\n\n\n\n- Then, run diagnostics:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneff_ratio(bike_model)\nrhat(bike_model)\nmcmc_trace(bike_model, size = 0.1)\nmcmc_dens_overlay(bike_model)\n```\n:::\n\n\n\n## **Simulation Diagnostics**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_model <- stan_glm(rides ~ temp_feel, # data model\n                       data = bikes, # dataset\n                       family = gaussian, # distribution to apply\n                       prior_intercept = normal(5000, 1000), # b0_c\n                       prior = normal(100, 40), # b1_c\n                       prior_aux = exponential(0.0008), # sigma\n                       chains = 4, # 4 chains\n                       iter = 5000*2, # 10000 iterations - throw out first 5000\n                       seed = 84735) # starting place in RNG\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.126 seconds (Warm-up)\nChain 1:                0.196 seconds (Sampling)\nChain 1:                0.322 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.152 seconds (Warm-up)\nChain 2:                0.197 seconds (Sampling)\nChain 2:                0.349 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.123 seconds (Warm-up)\nChain 3:                0.191 seconds (Sampling)\nChain 3:                0.314 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.11 seconds (Warm-up)\nChain 4:                0.197 seconds (Sampling)\nChain 4:                0.307 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n## **Simulation Diagnostics**\n\n- Diagnostics:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneff_ratio(bike_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   temp_feel       sigma \n    1.03285     1.03505     0.96585 \n```\n\n\n:::\n\n```{.r .cell-code}\nrhat(bike_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   temp_feel       sigma \n  0.9998873   0.9999032   0.9999642 \n```\n\n\n:::\n:::\n\n\n\n- Quick diagnostics indicate that the resulting chains are trustworthy. \n\n- The effective sample size ratios are slightly above 1 and the R-hat values are very close to 1.\n\n## **Simulation Diagnostics**\n\n- Diagnostics:\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W13-L1-simple-regression_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- We can see that the chains are stable, mixing quickly, and behaving much like an independent sample.\n\n## **Simulation Diagnostics**\n\n- Diagnostics:\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W13-L1-simple-regression_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- The density plot lets us visualize and examine the posterior models for each of our regression parameters, $\\beta_0$, $\\beta_1$, $\\sigma$.\n\n## **Interpreting the Posterior**\n\n- Okay... what does this mean, though? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(bike_model, effects = c(\"fixed\", \"aux\"), conf.int = TRUE, conf.level = 0.80)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"-2195.31399\",\"3\":\"353.755678\",\"4\":\"-2646.10435\",\"5\":\"-1736.28195\"},{\"1\":\"temp_feel\",\"2\":\"82.22065\",\"3\":\"5.071584\",\"4\":\"75.67964\",\"5\":\"88.66023\"},{\"1\":\"sigma\",\"2\":\"1282.49512\",\"3\":\"40.644447\",\"4\":\"1232.43144\",\"5\":\"1336.13254\"},{\"1\":\"mean_PPD\",\"2\":\"3487.38007\",\"3\":\"81.523838\",\"4\":\"3382.47384\",\"5\":\"3593.21895\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n- Thus, the posterior median relationship is \n\n$$y = -2194.24 + 82.16x$$\n\n- For a 1 degree increase in temperature, we expect ridership to increase by about 82 rides, with 80% credible interval (75.7, 88.7).\n\n## **Interpreting the Posterior**\n\n- We can look at alternatives by drawing from the simulated data in `bike_model`.\n    - The `add_fitted_draws()` function is from the `tidybayes` package.\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  add_fitted_draws(bike_model, n = 50) %>%\n  ggplot(aes(x = temp_feel, y = rides)) +\n    geom_line(aes(y = .value, group = .draw), alpha = 0.15) + \n    geom_point(data = bikes, size = 0.05) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw()\n```\n:::\n\n\n</center>\n\n## **Interpreting the Posterior**\n\n- We can look at alternatives by drawing from the simulated data in `bike_model`.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W13-L1-simple-regression_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- We see that the plausible models are not super variable.\n    - This means we're more confident about the relationship we're observing. \n\n## **Interpreting the Posterior**\n\n- How does this compare against the frequentist version of regression?\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W13-L1-simple-regression_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Wrap Up**\n\n- Today, we have learned basic linear regression in the Bayesian framework. \n\n- What we have learned so far still holds true re: simulation and diagnostics.\n\n- Monday next week:\n    - Multiple predictors\n    - Poisson & negative binomial regression\n    - Logistic regression\n    \n- Wednesday next week:\n    - Assignment to practice regression\n    \n- Monday & Wednesday before finals:\n    - Project 2! (We will build regression models. Surprise!)\n    - You will present on Wednesday.\n    - We will have smaller groups this time.\n\n## **Homework** \n\n- 9.9, 9.10, 9.11, 9.12\n\n- 9.16, 9.17, 9.18",
    "supporting": [
      "W13-L1-simple-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}