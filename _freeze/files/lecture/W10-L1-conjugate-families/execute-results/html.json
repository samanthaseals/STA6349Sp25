{
  "hash": "9a376d9ca07be28d717b1144c01282f1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"**Gamma-Poisson and Normal-Normal Models**\"\nsubtitle: \"**STA6349: Applied Bayesian Analysis** <br> Spring 2025\"\ndate-format: long\nexecute:\n  echo: true\n  warning: false\n  message: false\n  error: true\nformat: \n  revealjs:\n    code-overflow: wrap\n    embed-resources: true\n    slide-number: false\n    width: 1600\n    height: 900\n    html-math-method: katex\n    theme:\n      - default\n      - sp25.scss\neditor: source\n---\n\n\n\n## **Introduction: Gamma-Poisson**  \n\n- Recall the Beta-Binomial from our previous lecture,\n    - $y \\sim \\text{Bin}(n, \\pi)$ (data distribution)\n    - $\\pi \\sim \\text{Beta}(\\alpha, \\beta)$ (prior distribution)\n    - $\\pi|y \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)$ (posterior distribution)\n    \n- Beta-Binomial is from a *conjugate family* (i.e., the posterior is from the same model family as the prior).\n\n- Today, we will learn about other conjugate families, the Gamma-Poisson and the Normal-Normal.\n\n## **Poisson Data Model**  \n\n- Suppose we are now interested in modeling the number of spam calls we receive.\n\n    - This means that we are modeling the rate, $\\lambda$.\n    \n- We take a guess and say that the value of $\\lambda$ that is most likely is around 5, \n\n    - ... but reasonably ranges between 2 and 7 calls per day.\n\n- Why can't we use the Beta distribution as our prior distribution?\n\n## **Poisson Data Model**  \n\n- Suppose we are now interested in modeling the number of spam calls we receive.\n\n    - This means that we are modeling the rate, $\\lambda$.\n    \n- We take a guess and say that the value of $\\lambda$ that is most likely is around 5, \n\n    - ... but reasonably ranges between 2 and 7 calls per day.\n\n- Why can't we use the Beta distribution as our prior distribution?\n\n    - $\\lambda$ is the mean of a count $\\to$ $\\lambda \\in \\mathbb{R}^+$ $\\to$ $\\lambda$ is not limited to $[0, 1]$ $\\to$ broken assumption for Beta distribution.\n      \n## **Poisson Data Model**  \n\n- Suppose we are now interested in modeling the number of spam calls we receive.\n    - This means that we are modeling the rate, $\\lambda$.\n    \n- We take a guess and say that the value of $\\lambda$ that is most likely is around 5, \n    - ... but reasonably ranges between 2 and 7 calls per day.\n\n- Why can't we use the Beta distribution as our prior distribution?\n    - $\\lambda$ is the mean of a count $\\to$ $\\lambda \\in \\mathbb{R}^+$ $\\to$ $\\lambda$ is not limited to $[0, 1]$ $\\to$ broken assumption for Beta distribution.\n        \n- Why can't we use the binomial distribution as our data distribution?  \n    \n## **Poisson Data Model**  \n\n- Suppose we are now interested in modeling the number of spam calls we receive.\n    - This means that we are modeling the rate, $\\lambda$.\n    \n- We take a guess and say that the value of $\\lambda$ that is most likely is around 5, \n    - ... but reasonably ranges between 2 and 7 calls per day.\n\n- Why can't we use the Beta distribution as our prior distribution?\n    - $\\lambda$ is the mean of a count $\\to$ $\\lambda \\in \\mathbb{R}^+$ $\\to$ $\\lambda$ is not limited to $[0, 1]$ $\\to$ broken assumption for Beta distribution.\n        \n- Why can't we use the binomial distribution as our data distribution?    \n    - $Y_i$ is a count $\\to$ $Y_i \\in \\mathbb{N}^+$ $\\to$ $Y_i$ is not limited to $\\{0, 1\\}$ $\\to$ broken assumption for Binomial distribution.\n\n## **Poisson Data Model**  \n\n- We will use the Poisson distribution to model the number of spam calls -- $Y \\in \\{0, 1, 2, ...\\}$.\n    - $Y$ is the number of independent events that occur in a fixed amount of time or space. \n    - $\\lambda > 0$ is the rate at which these events occur.\n\n- Mathematically, \n\n$$ Y | \\lambda \\sim \\text{Pois}(\\lambda),$$\n\n- with pmf,\n\n$$f(y|\\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}, \\ \\ \\ y \\in \\{0,1, 2, ... \\}$$\n\n## **Poisson Data Model**  \n\n- $\\lambda$ defines the mean and the variance\n    - The shape of the Poisson pmf depends on $\\lambda$.\n\n<center>    \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Poisson Data Model**  \n\n- $\\lambda$ defines the mean and the variance\n\n    - The shape of the Poisson pmf depends on $\\lambda$.\n\n<center>    \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Poisson Data Model** \n\n- We will be taking samples from different days. \n    - We assume that the daily number of calls may different from day to day.\n    - On each day $i$,\n    \n$$Y_i|\\lambda \\sim \\text{Pois}(\\lambda)$$  \n\n- This has a unique pmf for each day ($i$),\n\n$$f(y_i|\\lambda) = \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}$$\n\n- But really, we are interested in the *joint* information in our sample of $n$ observations.\n    - The joint pmf gives us this information.\n    \n## **Poisson Data Model**     \n\n- The joint pmf for the Poisson,\n    \n$$\n\\begin{align*}\nf\\left(\\overset{\\to}{y_i}|\\lambda\\right) &= \\prod_{i=1}^n f(y_i|\\lambda) \\\\ \n&= f(y_1|\\lambda) \\times f(y_2|\\lambda) \\times ... \\times f(y_n|\\lambda) \\\\\n&= \\frac{\\lambda^{y_1}e^{-\\lambda}}{y_1!} \\times \\frac{\\lambda^{y_2}e^{-\\lambda}}{y_2!} \\times ... \\times \\frac{\\lambda^{y_n}e^{-\\lambda}}{y_n!} \\\\\n&= \\frac{\\left( \\lambda^{y_1} \\lambda^{y_2} \\cdot \\cdot \\cdot \\ \\lambda^{y_n}  \\right) \\left( e^{-\\lambda} e^{-\\lambda} \\cdot \\cdot \\cdot e^{-\\lambda}\\right)}{y_1! y_2! \\cdot \\cdot \\cdot y_n!} \\\\\n&= \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !}\n\\end{align*}\n$$    \n\n## **Poisson Data Model**     \n\n- If the joint pmf for the Poisson is\n\n$$f\\left(\\overset{\\to}{y_i}|\\lambda\\right) = \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !}$$\n\n- then the likelihood function for $\\lambda > 0$ is\n\n$$\n\\begin{align*}\nL\\left(\\lambda|\\overset{\\to}{y_i}\\right) &= \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !} \\\\\n& \\propto \\lambda^{\\sum y_i} e^{-n\\lambda}\n\\end{align*}\n$$\n\n- Pease see page 102 in the textbook for full derivations.\n\n## **Gamma Prior**  \n\n- If $\\lambda$ is a continuous random variable that can take on any positive value ($\\lambda > 0$), then the variability may be modeled with the Gamma distribution with\n    - shape hyperparameter $s>0$\n    - rate hyperparameter $r>0$.\n    \n- Thus,\n\n$$\\lambda \\sim \\text{Gamma}(s, r)$$\n    \n- and the Gamma pdf is given by\n\n$$f(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1} e^{-r\\lambda}$$\n\n## **Gamma Prior** \n\n- ...then the variability may be modeled with the Gamma distribution with shape $s>0$ and rate $r>0$.\n\n<center>    \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Gamma Prior** \n\n- Let's now tune our prior. \n\n- We are assuming $\\lambda \\approx 5$, somewhere between 2 and 7.\n\n- We know the mean of the gamma distribution,\n\n$$E(\\lambda) = \\frac{s}{r} \\approx 5 \\to 5r \\approx s$$\n\n- Your turn! Use the `plot_gamma()` function to figure out what value of $s$ and $r$ we need.\n\n## **Gamma Prior** \n\n- Looking at different values:\n\n<center>    \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n\n## **Gamma-Poisson Conjugacy** \n\n- Let $\\lambda > 0$ be an unknown *rate* parameter and $(Y_1, Y_2, ... , Y_n)$ be an independent sample from the Poisson distribution.\n\n- The Gamma-Poisson Bayesian model is as follows:\n\n$$\n\\begin{align*}\nY_i | \\lambda &\\overset{ind}\\sim \\text{Pois}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(s, r) \\\\\n\\lambda | \\overset{\\to}y &\\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right)\n\\end{align*}\n$$\n\n- The proof can be seen in section 5.2.4 of the textbook.\n\n## **Gamma-Poisson Conjugacy** \n\n- Suppose we use Gamma(10, 2) as the prior for $\\lambda$, the daily rate of calls.\n\n- On four separate days in the second week of August (i.e., independent days), we received $\\overset{\\to}y = (6, 2, 2, 1)$ calls.\n\n- Your turn! Use the `plot_poisson_likelihood()` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_poisson_likelihood(y = c(6, 2, 2, 1), lambda_upper_bound = 10)\n```\n:::\n\n\n\n- Notes:\n    - `lambda_upper_bound` limits the $x$ axis -- recall that $\\lambda \\in (0, \\infty)$!\n    - `lambda_upper_bound`'s default value is 10.\n    \n## **Gamma-Poisson Conjugacy** \n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_poisson_likelihood(y = c(6, 2, 2, 1), lambda_upper_bound = 10) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Gamma-Poisson Conjugacy**\n\n- We can see that the average is around 2.75.\n\n- We can also verify that --\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(c(6, 2, 2, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.75\n```\n\n\n:::\n:::\n\n\n\n- We know our prior distribution is Gamma(10, 2) and the data distribution is Poi(2.75).\n\n- Thus, the posterior is as follows,\n\n$$\n\\begin{align*}\n\\lambda | \\overset{\\to}y &\\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right) \\\\\n&\\sim \\text{Gamma}\\left(10 + 11, 2 + 4 \\right) \\\\\n&\\sim \\text{Gamma}\\left(21, 6 \\right)\n\\end{align*}\n$$\n\n## **Gamma-Poisson Conjugacy** \n\n- Your turn! Use the `plot_gamma_poisson()` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4) + theme_bw()\n```\n:::\n\n\n\n## **Gamma-Poisson Conjugacy** \n\n- Your turn! Use the `plot_gamma_poisson()` function:\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Gamma-Poisson Conjugacy** \n\n- Your turn! What is different if we had used one of the other priors?\n\n## **Gamma-Poisson Conjugacy** \n\n- Your turn! What is different if we had used Gamma(15, 3) as our prior?\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Introduction: Normal-Normal**  \n\n- So far, we have learned two conjugate families:\n    - Beta-Binomial (binary outcomes)\n        - $y \\sim \\text{Bin}(n, \\pi)$ (data distribution)\n        - $\\pi \\sim \\text{Beta}(\\alpha, \\beta)$ (prior distribution)\n        - $\\pi|y \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)$ (posterior distribution)\n    - Gamma-Poisson (count outcomes)\n        - $Y_i | \\lambda \\overset{ind}\\sim \\text{Pois}(\\lambda)$ (data distribution) \n        - $\\lambda \\sim \\text{Gamma}(s, r)$ (prior distribution)\n        - $\\lambda | \\overset{\\to}y \\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right)$ (posterior distribution)    \n        \n- Now, we will learn about another conjugate family, the Normal-Normal, for continuous outcomes.\n\n## **Introduction**  \n\n- As scientists learn more about brain health, the dangers of concussions are gaining greater attention.\n\n- We are interested in $\\mu$, the average volume (cm^3^) of a specific part of the brain: the hippocampus. \n\n- [Wikipedia](https://en.wikipedia.org/wiki/Hippocampus#Other_mammals) tells us that among the general population of human adults, each half of the hippocampus has volume between 3.0 and 3.5 cm^3^.\n    - Total hippocampal volume of both sides of the brain is between 6 and 7  cm^3^.\n    - Let's assume that the mean hippocampal volume among people with a history of concussions is also somewhere between 6 and 7 cm^3^.\n    \n- We will take a sample of $n=25$ participants and update our belief.\n\n## **The Normal Model**  \n\n- Let $Y \\in \\mathbb{R}$ be a continuous random variable.\n    - The variability in $Y$ may be represented with a Normal model with mean parameter $\\mu \\in \\mathbb{R}$ and standard deviation parameter $\\sigma > 0$.\n    \n- The Normal model's pdf is as follows,\n\n$$f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left\\{ \\frac{-(y-\\mu)^2}{2\\sigma^2} \\right\\}$$\n\n## **The Normal Model** \n\n- Use the `plot_normal()` function to plot the following:\n    - Vary $\\mu$:\n        - N(-1, 1)\n        - N(0, 1)\n        - N(1, 1)\n    - Vary $\\sigma$: \n        - N(0, 1)\n        - N(0, 2)\n        - N(0, 3)\n\n## **The Normal Model**  \n\n- If we vary $\\mu$,\n\n<center>        \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **The Normal Model**  \n\n- If we vary $\\sigma$,\n\n<center>        \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **The Normal Model**  \n\n- Our data model is as follows,\n\n$$Y_i | \\mu \\sim N(\\mu, \\sigma^2)$$\n\n- The joint pdf is as follows,\n\n$$\nf(\\overset{\\to}y | \\mu) = \\prod_{i=1}^n f(y_i | \\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-(y_i-\\mu)^2}{2\\sigma^2} \\right\\}\n$$\n\n- Meaning the likelihood is as follows,\n\n$$\nL(\\mu|\\overset{\\to}y) \\propto \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-(y_i-\\mu)^2}{2\\sigma^2} \\right\\} = \\exp \\left\\{ \\frac{- \\sum_{i=1}^n(y_i-\\mu)^2}{2\\sigma^2} \\right\\}\n$$\n\n## **The Normal Model**  \n\n- Our data model is as follows,\n\n$$Y_i | \\mu \\sim N(\\mu, \\sigma^2)$$\n\n- Returning to our brain analysis, we will assume that the hippocampal volumes of our $n = 25$ subjects have a normal distribution with mean $\\mu$ and standard deviation $\\sigma$.\n    - Right now, we are only interested in $\\mu$, so we assume $\\sigma = 0.5$ cm^3^\n    - This choice suggests that most people have hippocampal volumes within $2 \\sigma = 1$ cm^3^.\n\n## **Normal Prior**  \n\n- We know that with $Y_i | \\mu \\sim N(\\mu, \\sigma^2)$, $\\mu \\in \\mathbb{R}$.\n    - We think a normal prior for $\\mu$ is reasonable.\n    \n- Thus, we assume that $\\mu$ has a normal distribution around some mean, $\\theta$, with standard deviation, $\\tau$.\n\n$$\\mu \\sim N(\\theta, \\tau^2),$$\n\n- meaning that $\\mu$ has prior pdf\n\n$$f(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\tau^2}} \\exp \\left\\{ \\frac{-(\\mu - \\theta)^2}{2 \\tau^2} \\right\\}$$\n\n## **Normal Prior**  \n\n- We can tune the hyperparameters $\\theta$ and $\\tau$ to reflect our understanding and uncertainty about the average hippocampal volume ($\\mu$) among people with a history of concussions.\n\n- Wikipedia showed us that hippocampal volumes tend to be between 6 and 7 cm^3^ $\\to$ $\\theta=6.5$.\n    \n- When we set the standard deviation we can check the plausible range of values of $\\mu$:\n    - Follow up: why 2?\n\n$$\\theta \\pm 2 \\times \\tau$$\n\n- If we assume $\\tau=0.4$,\n\n$$(6.5 \\pm 2 \\times 0.4) = (5.7, 7.3)$$\n\n## **Normal Prior**  \n\n- Thus, our tuned prior is  $\\mu \\sim N(6.5, 0.4^2)$\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n- This range incorporates our uncertainty - it is wider than the Wikipedia range.\n\n## **Normal-Normal Conjugacy**  \n\n- Let $\\mu \\in \\mathbb{R}$ be an unknown mean parameter and $(Y_1, Y_2, ..., Y_n)$ be an independent $N(\\mu, \\sigma^2)$ sample where $\\sigma$ is assumed to be known.\n\n- The Normal-Normal Bayesian model is as follows:\n\n$$\n\\begin{align*}\nY_i | \\mu &\\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2) \\\\\n\\mu &\\sim N(\\theta, \\tau^2) \\\\\n\\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\\end{align*}\n$$\n\n## **Normal-Normal Conjugacy**  \n\n- Let's think about our posterior and some implications,\n\n$$\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)$$\n\n- What happens as $n$ increases?\n\n## **Normal-Normal Conjugacy**  \n\n- Let's think about our posterior and some implications,\n\n$$\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)$$\n\n- What happens as $n$ increases?\n\n$$\n\\begin{align*}\n\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} &\\to 0 \\\\\n\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2} &\\to 1 \\\\\n\\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} &\\to 0\n\\end{align*}\n$$\n\n## **Normal-Normal Conjugacy**  \n\n- Let's think about our posterior and some implications,\n\n$$\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)$$\n\n$$\n\\begin{align*}\n\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} &\\to 0 \\\\\n\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2} &\\to 1 \\\\\n\\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} &\\to 0\n\\end{align*}\n$$\n\n- The posterior mean places less weight on the prior mean and more weight on the sample mean $\\bar{y}$.\n\n- The posterior certainty about $\\mu$ increases and becomes more in sync with the data.\n\n## **Example** \n\n- Let us now apply this to our example.\n\n- We have our prior model, $\\mu \\sim N(6.5, 0.4^2)$.\n\n- Let's look at the `football` dataset in the `bayesrules` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(football)\nconcussion_subjects <- football %>% \n  filter(group == \"fb_concuss\")\n```\n:::\n\n\n\n- What is the average hippocampal volume?\n\n## **Example** \n\n- Let us now apply this to our example.\n\n- We have our prior model, $\\mu \\sim N(6.5, 0.4^2)$.\n\n- Let's look at the `football` dataset in the `bayesrules` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(football)\nconcussion_subjects <- football %>% \n  filter(group == \"fb_concuss\")\n```\n:::\n\n\n\n- What is the average hippocampal volume?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(concussion_subjects$volume)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.7346\n```\n\n\n:::\n:::\n\n\n\n## **Example**\n\n- We can also plot the density!\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconcussion_subjects %>% ggplot(aes(x = volume)) + geom_density() + theme_bw()\n```\n\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Example** \n\n- Now, we can plug in the information we have ($n = 25, \\bar{y} = 5.735, \\sigma = 0.5$) into our likelihood,\n\n$$\nL(\\mu|\\overset{\\to}y) \\propto \\exp \\left\\{ \\frac{-(5.735 - \\mu)^2}{2(0.5^2/25)} \\right\\}\n$$\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Example** \n\n- We are now ready to put together our posterior:\n    - Data distribution, $Y_i | \\mu \\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2)$\n    - Prior distribution, $\\mu \\sim N(\\theta, \\tau^2)$    \n    - Posterior distribution, $\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)$\n    \n- Given our information ($\\theta=6.5$, $\\tau=0.4$, $n=25$, $\\bar{y}=5.735$, $\\sigma=0.5$), our posterior is\n\n$$\n\\begin{align*}\n\\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right) \\\\\n&\\sim N\\left( 6.5 \\frac{0.5^2}{25 \\cdot 0.4^2 + 0.5^2} + 5.735 \\frac{25 \\cdot 0.4^2}{25 \\cdot 0.4^2 + 0.5^2}, \\frac{0.4^2 \\cdot 0.5^2}{25 \\cdot 0.4^2 + 0.5^2} \\right) \\\\\n&\\sim N(6.5 \\cdot 0.0588 + 5.737 \\cdot 0.9412, 0.09^2) \\\\\n&\\sim N(5.78, 0.09^2)\n\\end{align*}\n$$\n\n## **Example** \n\n- Looking at the posterior, we can see the weights\n\n$$\n\\begin{align*}\n\\mu | \\overset{\\to}y &\\sim N\\left( 6.5 \\frac{0.5^2}{25 \\cdot 0.4^2 + 0.5^2} + 5.735 \\frac{25 \\cdot 0.4^2}{25 \\cdot 0.4^2 + 0.5^2}, \\frac{0.4^2 \\cdot 0.5^2}{25 \\cdot 0.4^2 + 0.5^2} \\right) \\\\\n&\\sim N(6.5 \\cdot 0.0588 + 5.737 \\cdot 0.9412, 0.009^2) \n\\end{align*}\n$$\n\n- 95% on the data mean, 6% on the prior mean.\n\n## **Example** \n\n- We can plot the distribution,\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W10-L1-conjugate-families_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Example** \n\n- We can summarize the distribution,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5, y_bar = 5.735, n = 25) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      model mean mode         var         sd\n1     prior 6.50 6.50 0.160000000 0.40000000\n2 posterior 5.78 5.78 0.009411765 0.09701425\n```\n\n\n:::\n:::\n\n\n\n## **Homework**\n\n- 5.3\n- 5.5\n- 5.6\n- 5.9\n- 5.10\n",
    "supporting": [
      "W10-L1-conjugate-families_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}