{
  "hash": "c8a2e78cd4184d418d2a3c83eaa6b98f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"**Discrete Random Variables and Their Probability Distributions**\"\nsubtitle: \"**STA6349: Applied Bayesian Analysis** <br> Spring 2025\"\ndate-format: long\nexecute:\n  echo: true\n  warning: false\n  message: false\n  error: true\nformat: \n  revealjs:\n    code-overflow: wrap\n    embed-resources: true\n    slide-number: false\n    width: 1600\n    height: 900\n    html-math-method: katex\n    theme:\n      - default\n      - sp25.scss\neditor: source\n---\n\n\n\n## **Introduction**\n\n- The first few lectures come from [Mathematical Statistics with Applications](https://www.cengage.com/c/mathematical-statistics-with-applications-7e-wackerly/9780495110811/), by Wackerly.\n    - We must understand the underlying probability and random variable theory before moving into the Bayesian world.\n    \n- We will be covering the following chapters:\n    - Chapter 2: probability theory\n    - <font color = \"#965dc7\">Chapter 3: discrete random variables</font>\n    - Chapter 4: continuous random variables\n\n## **3.1: Basic Definitions**\n\n- *Discrete random variable*: a variable that can assume only a finite or countably infinite number of distinct values. \n\n- *Probability distribution of a random variable*: collection of probabilities for each value of the random variable.\n\n- Notation:\n    - Uppercase letter (e.g., $Y$) denotes a random variable.\n    - Lowercase letter (e.g., $y$) denotes a particular value that the random variable may assume.\n        - The specific observed value, $y$, is not random.\n\n## **3.2: Probability Distributions for Discrete RV**\n\n- *probability function for $Y$*: sum of the the probabilities of all sample points in $S$ that are assigned the value $y$\n    - $P[Y = y] = p(y)$: the probability that $Y$ takes on the value $y$.\n    \n- *probability distribution for $Y$*: a formula, table, or graph that provides $p(y) = P[Y = y]$ $\\forall$ $y$.\n\n- **Theorem**:\n    - For any discrete probability distribution, the following must be true:\n        - $0 \\le p(y) \\le 1 \\  \\forall \\ y$\n        - $\\sum_y p(y) = 1 \\ \\forall \\ p(y) > 0$.\n\n## **3.2: Probability Distributions for Discrete RV**\n\n- A supervisor in a manufacturing plant has three men and three women working for them. The supervisor wants to choose two workers for a special job. Not wishing to show any biases in their selection, they decides to select the two workers at random. \n\n- Let $Y$ denote the number of women in his selection.  Find the probability distribution for $Y$.\n\n## **3.2: Probability Distributions for Discrete RV**\n\n- When the health department tested private wells in a county for two impurities commonly found in drinking water, it found that:\n    - 20% of the wells had neither impurity, \n    - 40% had impurity A, and \n    - 50% had impurity B. \n    \n- If a well is randomly chosen from those in the county, find the probability distribution for $Y$, the number of impurities found in the well.\n    - Hint: some wells had both impurities...\n\n## **3.3: Expected Values**\n\n- *Expected value*: Let $Y$ be a discrete random variable with the probability function, $p(y)$. Then, the *expected value* of $Y$, $E[Y]$, is defined to be\n\n$$\nE(Y) = \\sum_{y} y p(y)\n$$\n\n- When $p(y)$ is an accurate characterization of the population frequency distribution, then the expected value is the population mean.\n\n$$\nE[Y] = \\mu\n$$\n  \n- **Theorem**: \n\n    - Let $Y$ be a discrete random variable with probability function $p(y)$ and $g(Y)$ be a real-valued function of $Y$ (i.e., a *transformed* variable). Then the expected value of $g(Y)$ is given by\n    \n$$\nE[g(Y)] = \\sum_{y} g(y) p(y)\n$$\n\n## **3.3: Expected Values**\n\n- *Variance*: if $Y$ is a random variable with mean $E[Y] = \\mu$, the variance of a random variable $Y$ is defined to be the expected value of $(Y-\\mu)^2$.\n\n$$\nV[Y] = E\\left[ (Y-\\mu)^2 \\right]\n$$\n\n- If $p(y)$ is an accurate characterization of the population frequency distribution, then $V(Y)$ is the population variance,\n\n$$\nV[Y] = \\sigma^2\n$$\n\n- *Standard deviation*: the positive square root of $V[Y]$.\n\n## **3.3: Expected Values** \n\n- The probability distribution for a random variable $Y$ is given below. \n\n![](images/W03-L1-a.png){fig-align=\"center\" width=\"50%\"} \n\n- Find the mean, variance, and standard deviation of $Y$.\n\n<!-- \n![](images/W03-L1-b.png){fig-align=\"center\"} \n-->\n\n## **3.3: Expected Values**\n\n- **Theorem**: \n\n    - Let $Y$ be a discrete random variable with probability function $p(y)$ and $c$ be a constant. Then, \n    \n$$E(c) = c$$\n    \n- **Theorem**:\n\n    - Let $Y$ be a discrete random variable with probability function $p(y)$, $g(Y)$ be a function of $Y$, and $c$ be a constant. Then,\n    \n$$E[cg(Y)] = cE[g(Y)]$$\n\n- **Theorem**:\n\n    - Let $Y$ be a discrete random variable with probability function $p(y)$, and $g_1(Y), g_2(Y), ..., g_k(Y)$ be $k$ functions of $Y$. Then,\n    \n$$E[g_1(Y) + g_2(Y) + ... + g_k(Y)] = E[g_1(Y)] + E[g_2(Y)] + ... + E[g_k(Y)]$$    \n\n\n## **3.3: Expected Values** \n\n- Putting the previous theorems into one:\n\n- **Theorem**:\n\n    - Let $Y$ be a discrete random variable with probability function $p(y)$ and mean $E[Y] = \\mu$. Then,\n    \n$$V[Y] = \\sigma^2 = E\\left[(Y-\\mu)^2\\right] = E\\left[Y^2\\right] - \\mu^2 $$    \n\n## **3.3: Expected Values** \n\n- The probability distribution for a random variable $Y$ is given below. \n\n![](images/W03-L1-a.png){fig-align=\"center\" width=\"40%\"} \n\n- Use the previous theorem to find $V[Y]$ and compare to our previous answer.\n\n    - Recall that $\\mu=1.75$.\n\n## **3.3: Expected Values** \n\n- Let $Y$ be a random variable with $p(y)$ in the table below.\n\n![](images/W03-L1-b.png){fig-align=\"center\"} \n\n- Find\n    - $E[Y]$ <br><br>\n    - $E[1/Y]$ <br><br>\n    - $E\\left[Y^2-1\\right]$ <br><br>\n    - $V[Y]$\n    \n## **3.3: Expected Values** \n\n- $E[Y]$ <br><br><br><br><br>\n- $E[1/Y]$ <br><br><br><br><br>\n- $E\\left[Y^2-1\\right]$ <br><br><br><br><br>\n- $V[Y]$    \n\n## **3.4: Binomial Probability Distribution**\n\n- *Binomial experiment*:\n\n    1. The experiment consists of a fixed number, $n$, of identical trials.\n    \n    2. Each trial results in one of two outcomes: success ($S$) or failure ($F$).\n    \n    3. The probability of success on a single trial is equal to some value $p$ and remains the same from trial to trial. \n    \n        - The probability of failure is equal to $q = (1-p)$.\n        \n    4. The trials are independent.\n    \n    5. The random variable of interest is $Y$, the number of successes observed during the $n$ trials.\n\n## **3.4: Binomial Probability Distribution**\n\n- **Binomial Distribution**\n\n    - A random variable $Y$ is said to have a *binomial distribution* based on $n$ trials with success probability $p$ [iff](https://en.wikipedia.org/wiki/If_and_only_if)\n    \n$$\np(y) = {n \\choose y}p^y q^{n-y}, \\text{ where } y = 0, 1, 2, ..., n, \\text{ and } 0 \\le p \\le1\n$$\n\n- **Theorem**:\n\n    - Let $Y$ be a binomial random variable based on n trials and success probability $p$. Then\n    \n$$\nE[Y] = \\mu = np \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = npq\n$$\n\n- See Wackerly pg. 107 for derivation.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## **3.4: Binomial Probability Distribution** \n\n$p=0.10$\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W05-L1-discrete-rv_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **3.4: Binomial Probability Distribution**\n\n$p=0.25$\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W05-L1-discrete-rv_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **3.4: Binomial Probability Distribution**\n\n$p=0.50$\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W05-L1-discrete-rv_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **3.4: Binomial Probability Distribution**\n\n$p=0.75$\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W05-L1-discrete-rv_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **3.4: Binomial Probability Distribution**\n\n$p=0.90$\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W05-L1-discrete-rv_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **3.4: Binomial Probability Distribution**\n\n- What do you notice when comparing distributions under $p$ vs. $1-p$?\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W05-L1-discrete-rv_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **3.4: Binomial Probability Distribution**\n\n- What do you notice as $n$ increases?\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W05-L1-discrete-rv_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **3.4: Binomial Probability Distribution**\n\n- The manufacturer of a low-calorie dairy drink wishes to compare the taste appeal of a new formula (formula $B$) with that of the standard formula (formula $A$). Each of four judges is given three glasses in random order, two containing formula $A$ and the other containing formula $B$. Each judge is asked to state which glass he or she most enjoyed. Suppose that the two formulas are equally attractive. Let $Y$ be the number of judges stating a preference for the new formula.\n\n    a. Find the probability function for $Y$. <br><br><br>\n    \n    b. What is the probability that at least three of the four judges state a preference for the new formula?<br><br><br>\n    \n    c. Find the expected value of $Y$.<br><br><br>\n    \n    d. Find the variance of $Y$.\n  \n<!-- \n![](images/W03-L1-b.png){fig-align=\"center\"} \n-->\n\n## **3.8: Poisson Probability Distribution**\n\n- The Poisson probability distribution often provides a good model for the probability distribution of the number $Y$ of rare events that occur in space, time, volume, or any other dimension. \n\n- **Poisson Distribution**:\n\n    - A random variable $Y$ is said to have a *Poisson probability distribution* [iff](https://en.wikipedia.org/wiki/If_and_only_if)\n    \n$$\np(y) = \\frac{\\lambda^y}{y!}e^{-\\lambda}, \\text{ where } y=0,1,2,..., \\text{ and } \\lambda > 0\n$$\n\n- **Theorem**\n\n  - If $Y$ is a random variable with a Poisson distribution with parameter $\\lambda$, then\n  \n$$\nE[Y] = \\mu = \\lambda \\text{ and } V[Y] = \\sigma^2 = \\lambda\n$$\n\n- See Wackerly pg. 134 for derivation.\n  \n<!-- \n![](images/W03-L1-b.png){fig-align=\"center\"} \n-->\n\n## **3.8: Poisson Probability Distribution**\n\n- Customers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. During a given hour, what are the probabilities that\n\n    a. no more than three customers arrive? <br><br><br><br><br>\n\n    b. at least two customers arrive? <br><br><br><br><br>\n\n    c. exactly five customers arrive? <br><br><br><br><br>\n\n\n## **Homework**\n\n- 3.6\n- 3.10\n- 3.15\n- 3.22\n- 3.34\n- 3.60\n- 3.128\n- 3.129\n- 3.136\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "W05-L1-discrete-rv_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}