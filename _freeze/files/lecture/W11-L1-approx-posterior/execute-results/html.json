{
  "hash": "1716d0f70a5d21c6df8b03430681923f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"**Approximating the Posterior**\"\nsubtitle: \"**STA6349: Applied Bayesian Analysis** <br> Spring 2025\"\ndate-format: long\nexecute:\n  echo: true\n  warning: false\n  message: false\n  error: true\nformat: \n  revealjs:\n    code-overflow: wrap\n    embed-resources: true\n    slide-number: false\n    width: 1600\n    height: 900\n    html-math-method: katex\n    theme:\n      - default\n      - sp25.scss\neditor: source\n---\n\n\n\n## **Introduction**\n\n- We have learned how to think like a Bayesian: \n\n    - Prior distribution\n    - Data distribution\n    - Posterior distribution\n\n- We have learned three conjugate families:\n\n    - Beta-Binomial (binary outcomes)\n    - Gamma-Poisson (count outcomes)\n    - Normal-Normal (continuous outcomes)\n    \n- Once we have a posterior model, we must be able to apply the results.\n\n    - Posterior estimation\n    - Hypothesis testing\n    - Prediction  \n\n## **Introduction**\n\n- Recall, we have the posterior pdf,\n\n$$f(\\theta|y) = \\frac{f(\\theta) L(\\theta|y)}{f(y)} \\propto f(\\theta)L(\\theta|y)$$\n    \n- Now, in the denominator, we need to remember,\n\n$$f(y) = \\int_{\\theta_1} \\int_{\\theta_2} \\cdot \\cdot \\cdot \\int_{\\theta_k} f(\\theta) L(\\theta|y) d\\theta_k \\cdot \\cdot \\cdot d\\theta_2 d\\theta_1$$\n\n- Because this is ... not fun ... we will approximate the posterior via simulation.\n\n## **Introduction**\n\n- We are going to explore two simulation techniques:\n    - grid approximation\n    - Markov chain Monte Carlo (MCMC)\n    \n- Either method will produce a **sample** of $N$ values for $\\theta$. \n\n$$\\left \\{ \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)} \\right \\}$$\n\n- These $\\theta_i$ will have properties that reflect those of the posterior model for $\\theta$.\n\n- To help us, we will apply these simulation techniques to Beta-Binomial and Gamma-Poisson models.\n    - Note that these models do not require simulation! We know their posteriors!\n    - That's why we are starting there -- we can link the concepts to what we know. :)\n\n## **Introduction**\n\n- Note: we will use the following packages that may be new to you:\n    - `janitor`\n    - `rstan`\n    - `bayesplot`\n    \n- If you are using the server provided by HMCSE, *they have been installed for you*.\n\n- If you are working at home, please check to see if you have the libraries, then install if you do not.\n    - `install.packages(c(\"janitor\", \"rstan\", \"bayesplot\"))`\n\n## **Grid Approximation**\n\n<center><img src = \"images/W12-L1-a.png\"></center>\n\n- Suppose there is an image that you can't view in its entirety.\n\n- We can see snippets along a grid that sweeps from left to right across the image. \n\n- The finer the grid, the clearer the image; if the grid is fine enough, the result is a good approximation.\n\n## **Grid Approximation**\n\n<center><img src = \"images/W12-L1-a.png\"></center>\n\n- This is the general idea behind Bayesian grid approximation.\n\n- Our target image is the posterior pdf, $f(\\theta|y)$.\n    - It is not necessary to observe all possible $f(\\theta|y) \\ \\forall \\theta$ for us to understand its structure.\n    - Instead, we evaluate $f(\\theta|y)$ at a finite, discrete grid of possible $\\theta$ values.\n    - Then, we take random samples from this discretized pdf to approximate the full posterior pdf.\n    \n## **Grid Approximation**\n\n- Grid approximation produces a sample of $N$ independent $\\theta$ values, $$\\left\\{ \\theta^{(1)}, \\theta^{(2)}, \\theta^{(N)} \\right\\},$$ from a discretized approximation of the posterior pdf, $f(\\theta|y)$.\n\n- **Algorithm:**\n    1. Define a discrete grid of possible $\\theta$ values.\n    2. Evaluate the prior pdf, $f(\\theta)$, and the likelihood function, $L(\\theta|y)$ at each $\\theta$ grid value.\n    3. Obtain a discrete approximation of the posterior pdf, $f(\\theta|y)$ by:\n        a. Calculating the product $f(\\theta) L(\\theta|y)$ at each $\\theta$ grid value,\n        b. Normalize the products from (a) to sum to 1 across all $\\theta$.\n    4. Randomly sample $N$ $\\theta$ grid values with respect to their corresponding normalized posterior probabilities.        \n\n## **Grid Approximation - Example**\n\n- We will use the following Beta-Binomial model to learn how to do grid approximation:\n\n$$\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(10, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(2, 2)\n\\end{align*}\n$$\n\n- Note that \n    - $Y$ is the number of successes in 10 independent trials.\n    - Every trial has probability of success, $\\pi$.\n    - Our prior understanding about $\\pi$ is captured by a $\\text{Beta}(2,2)$ model.\n    \n- If we observe $Y = 9$ successes, we know that the updated posterior model for $\\pi$ is $\\text{Beta}(11, 3)$.\n    - $Y + \\alpha = 9+2$\n    - $n - Y + \\beta = 10-9+2$\n\n## **Grid Approximation**\n\n- Instead of using the posterior we know, let's approximate it using grid approximation.\n\n- **First step:** define a discrete grid of possible $\\theta$ values.\n    - So, let's consider $\\pi \\in \\{0, 0.2, 0.4, 0.6, 0.8, 1\\}$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ngrid_data <- tibble(pi_grid = seq(from = 0, to = 1, length = 6))\n```\n:::\n\n\n\n## **Grid Approximation** \n\n- Instead of using the posterior we know, let's approximate it using grid approximation.\n\n- **First step:** define a discrete grid of possible $\\theta$ values.\n    - So, let's consider $\\pi \\in \\{0, 0.2, 0.4, 0.6, 0.8, 1\\}$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ngrid_data <- tibble(pi_grid = seq(from = 0, to = 1, length = 6))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 1\n  pi_grid\n    <dbl>\n1     0  \n2     0.2\n3     0.4\n```\n\n\n:::\n:::\n\n\n\n## **Grid Approximation** \n\n- Instead of using the posterior we know, let's approximate it using grid approximation.\n\n- **Second step:** evaluate the prior pdf, $f(\\theta)$, and the likelihood function, $L(\\theta|y)$ at each $\\theta$ grid value.\n    - We will use `dbeta()` and `dbinom()` to evaluate the $\\text{Beta}(2,2)$ prior and $\\text{Bin}(10, \\pi)$ likelihood with $Y=9$ at each $\\pi$ in `pi_grid`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_data <- grid_data %>%\n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))\n```\n:::\n\n\n\n## **Grid Approximation**\n\n- Instead of using the posterior we know, let's approximate it using grid approximation.\n\n- **Second step:** evaluate the prior pdf, $f(\\theta)$, and the likelihood function, $L(\\theta|y)$ at each $\\theta$ grid value.\n    - We will use `dbeta()` and `dbinom()` to evaluate the $\\text{Beta}(2,2)$ prior and $\\text{Bin}(10, \\pi)$ likelihood with $Y=9$ at each $\\pi$ in `pi_grid`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_data <- grid_data %>%\n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  pi_grid prior likelihood\n    <dbl> <dbl>      <dbl>\n1     0    0    0         \n2     0.2  0.96 0.00000410\n3     0.4  1.44 0.00157   \n```\n\n\n:::\n:::\n\n\n\n## **Grid Approximation**\n\n- Instead of using the posterior we know, let's approximate it using grid approximation.\n\n- **Third step:** obtain a discrete approximation of the posterior pdf, $f(\\theta|y)$ by calculating the product $f(\\theta) L(\\theta|y)$ at each $\\theta$ grid value and normalizing the products to sum to 1 across all $\\theta$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_data <- grid_data %>%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))\n```\n:::\n\n\n\n## **Grid Approximation**\n\n- Instead of using the posterior we know, let's approximate it using grid approximation.\n\n- **Third step:** obtain a discrete approximation of the posterior pdf, $f(\\theta|y)$ by calculating the product $f(\\theta) L(\\theta|y)$ at each $\\theta$ grid value and normalizing the products to sum to 1 across all $\\theta$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_data <- grid_data %>%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))\n```\n:::\n\n\n\n- We can verify,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_data %>%\n  summarize(sum(unnormalized), sum(posterior))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  `sum(unnormalized)` `sum(posterior)`\n                <dbl>            <dbl>\n1               0.318                1\n```\n\n\n:::\n:::\n\n\n\n## **Grid Approximation**\n\n- Instead of using the posterior we know, let's approximate it using grid approximation.\n\n- **Third step:** obtain a discrete approximation of the posterior pdf, $f(\\theta|y)$ by calculating the product $f(\\theta) L(\\theta|y)$ at each $\\theta$ grid value and normalizing the products to sum to 1 across all $\\theta$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_data <- grid_data %>%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  pi_grid prior likelihood unnormalized posterior\n    <dbl> <dbl>      <dbl>        <dbl>     <dbl>\n1     0    0    0            0          0        \n2     0.2  0.96 0.00000410   0.00000393 0.0000124\n3     0.4  1.44 0.00157      0.00226    0.00712  \n```\n\n\n:::\n:::\n\n\n\n## **Grid Approximation**\n\n- Instead of using the posterior we know, let's approximate it using grid approximation.\n\n- We now have a *glimpse* into the actual posterior pdf.\n    - We can plot it to see what it looks like,\n    \n<center>    \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W11-L1-approx-posterior_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Grid Approximation**\n\n- As we increase the number of possible $\\theta$ values, the better we can \"see\" the resulting posterior.\n\n- What happens if we try the following: $n=50$, $n=100$, $n=500$, $n=1000$?\n\n## **Grid Approximation**\n\n- As we increase the number of possible $\\theta$ values, the better we can \"see\" the resulting posterior.\n\n- What happens if we try the following: $n=50$, $n=100$, $n=500$, $n=1000$?\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W11-L1-approx-posterior_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n\n<!-- ## Grid Approximation - Example -->\n\n<!-- - Your turn! -->\n\n<!-- - Let's now apply this to a Gamma-Poisson model. -->\n\n<!-- - $Y$ is the number of events that occur in a one-hour period, where events occur at an average rate of $\\lambda$ per hour. -->\n\n<!-- $$ -->\n<!-- \\begin{align*} -->\n<!-- Y_i | \\lambda &\\overset{\\text{iid}}\\sim \\text{Pois}(\\lambda) \\\\ -->\n<!-- \\lambda &\\sim \\text{Gamma}(3,1) -->\n<!-- \\end{align*} -->\n<!-- $$ -->\n\n<!-- - We collect two data points,  $(Y_1, Y_2) = (2, 3)$ -->\n\n<!-- - What is the posterior model? -->\n\n<!-- - Simulate this posterior using grid approximation. -->\n\n<!--     - You will need `dgamma()` and `dpois()`. -->\n<!--     - Remember to set the seed. -->\n\n## **Markov chain Monte Carlo (MCMC)**\n\n- Markov chain Monte Carlo (MCMC) is an application of [Markov chains](https://en.wikipedia.org/wiki/Markov_chain) to simulate probability models.\n\n- MCMC samples are not taken directly from the posterior pdf, $f(\\theta | y)$... and they are not independent.\n    - Each subsequent value depends on the previous value.\n    \n- Suppose we have an $N$-length MCMC sample, $$\\left\\{ \\theta^{(1)}, \\theta^{(2)}, \\theta^{(3)}, ..., \\theta^{(N)} \\right\\}$$\n    - $\\theta^{(2)}$ is drawn from a model that depends on $\\theta^{(1)}$.\n    - $\\theta^{(3)}$ is drawn from a model that depends on $\\theta^{(2)}$.\n    - etc.\n\n## **Markov chain Monte Carlo (MCMC)**\n\n- The $(i+1)$^st^ chain value, $\\theta^{(i+1)}$ is drawn from a model that depends on data $y$ and the previous chain value, $\\theta^{(i)}$.\n\n$$f\\left( \\theta^{(i+1)} | \\theta^{(i)}, y \\right)$$ \n    \n- It is important for us to note that the pdf from which a Markov chain is simulated is not equivalent to the posterior pdf!\n\n$$f\\left( \\theta^{(i+1)} | \\theta^{(i)}, y  \\right) \\ne f\\left(\\theta^{(i+1)}|y \\right)$$\n\n## **Using `rstan`**\n\n- We will use `rstan`:\n    - define the Bayesian model structure in `rstan` notation\n    - simulate the posterior\n    \n- Again, we will use the Beta-Binomial model from earlier.\n    - `data`: in our example, $Y$ is the observed number of successes in 10 trials.\n        - We need to tell `rstan` that $Y$ is an *integer* between 0 and 10.\n    - `parameters`: in our example, our model depends on $\\pi$.\n        - We need to tell `rstan` that $\\pi$ can be any *real* number between 0 and 1.\n    - `model`: in our example, we need to specify $Y \\sim \\text{Bin}(10, \\pi)$ and $\\pi \\sim \\text{Beta}(2,2)$.\n\n## **Using `rstan`** \n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 1: DEFINE the model\nbb_model <- \"\n  data {\n    int<lower = 0, upper = 10> Y;\n  }\n  parameters {\n    real<lower = 0, upper = 1> pi;\n  }\n  model {\n    Y ~ binomial(10, pi);\n    pi ~ beta(2, 2);\n  }\n\"\n```\n:::\n\n\n\n## **Using `rstan`** \n\n- Then, when we go to simulate, we first put in the model information\n    - `model code`: the character string defining the model (in our case, `bb_model`).\n    - `data`: a list of the observed data.\n        - In this example, we are using $Y = 9$ - a single data point.\n        \n- Then, we put in the Markov chain information,\n    - `chains`: how many parallel Markov chains to run.\n        - This will be the number of distinct $\\theta$ values we want.\n    - `iter`: desired number of iterations, or length of Markov chain.\n        - Half are thrown out as \"burn in\" samples. \n            - \"burn in\"? Think: pancakes!\n    - `seed`: used to set the seed of the RNG.        \n\n## **Using `rstan`**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 2: simulate the posterior\nbb_sim <- stan(model_code = bb_model, data = list(Y = 9), \n               chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 1:                0.013 seconds (Sampling)\nChain 1:                0.025 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 2:                0.013 seconds (Sampling)\nChain 2:                0.025 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 3:                0.012 seconds (Sampling)\nChain 3:                0.024 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 4:                0.013 seconds (Sampling)\nChain 4:                0.025 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n## **Using `rstan`** \n\n- Now, we need to extract the values,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.array(bb_sim, pars = \"pi\") %>% head(4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n, , parameters = pi\n\n          chains\niterations   chain:1   chain:2   chain:3   chain:4\n      [1,] 0.8278040 0.7523560 0.6386998 0.9627572\n      [2,] 0.9087546 0.8922386 0.6254761 0.9413518\n      [3,] 0.6143859 0.8682356 0.7222360 0.9489624\n      [4,] 0.8462156 0.8792812 0.8100192 0.9413812\n```\n\n\n:::\n:::\n\n\n\n- Remember, these are *not* a random sample from the posterior!\n\n- They are also *not* independent!\n\n- Each chain forms a dependent 5,000 length Markov chain of $\\left\\{ \\pi^{(1)}, \\pi^{(2)}, ..., \\pi^{(5000)}\\right\\}$\n\n    - Each chain will move along the sample space of plausible values for $\\pi$.\n    \n## **Using `rstan`** \n\n- We will look at the trace plot (using `mcmc_trace()` from `bayesplot` package) to see what the values did longitudinally.\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_trace(bb_sim, pars = \"pi\", size = 0.1)\n```\n\n::: {.cell-output-display}\n![](W11-L1-approx-posterior_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Using `rstan`** \n\n- We can also look at the `mcmc_hist()` and `mcmc_dens()` functions,\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W11-L1-approx-posterior_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n<!-- ## MCMC - Example -->\n\n<!-- - Your turn! -->\n\n<!-- - Let's now apply this to a Gamma-Poisson model. -->\n\n<!-- - $Y$ is the number of events that occur in a one-hour period, where events occur at an average rate of $\\lambda$ per hour. -->\n\n<!-- $$ -->\n<!-- \\begin{align*} -->\n<!-- Y_i | \\lambda &\\overset{\\text{iid}}\\sim \\text{Pois}(\\lambda) \\\\ -->\n<!-- \\lambda &\\sim \\text{Gamma}(3,1) -->\n<!-- \\end{align*} -->\n<!-- $$ -->\n\n<!-- - We collect two data points, $(Y_1, Y_2) = (2, 8)$ -->\n\n<!-- - What is the posterior model? -->\n\n<!-- - Simulate this posterior using MCMC in `rstan`. -->\n\n<!-- - If you struggle, ask Dr. Seals for a hint :) -->\n    \n## **Diagnostics**\n\n- Simulations are not perfect...\n    - What does a good Markov chain look like?\n    - How can we tell if the Markov chain sample produces a reasonable approximation of the posterior?\n    - How big should our Markov chain sample size be?\n    \n- Unfortunately there is no one answer here.\n    - You will learn through experience, much like other nuanced areas of statistics.\n    \n## **Diagnostics**     \n    \n- Let's now discuss diagnostic tools.\n    - Trace plots\n    - Parallel chains\n    - Effective sample size\n    - Autocorrelation\n    - $\\hat{R}$\n\n## **Trace Plots** \n\n<center><img src = \"images/W12-L1-b.png\" width = 1200></center>\n\n- Chain A has not stabilized after 5000 iterations.\n    - It has not \"found\" or does not know how to explore the range of posterior plausible $\\pi$ values.\n    - The downward trend also hints against independent noise. \n    \n## **Trace Plots** \n\n<center><img src = \"images/W12-L1-b.png\" width = 1200></center>    \n- We say that Chain A is mixing slowly. \n    - The more Markov chains behave like fast mixing (noisy) independent samples, the smaller the error in the resulting posterior approximation.\n\n## **Trace Plots** \n\n<center><img src = \"images/W12-L1-c.png\" width = 1200></center>\n\n- Chain B is not great, either -- it gets stuck when looking at a smaller value of $\\pi$.\n\n## **Trace Plots** \n\n- Realistically, we are only going to do simulations when we can't specify the posterior and must approximate\n    - i.e., we won't be able to compare the simulation results to the \"true\" results.\n    \n- If we see bad trace plots:\n    - Check the model (... or your code). Are the assumed prior and data models appropriate? \n    - Run the chain for more iterations. Sometimes we just need a longer run to iron out issues.\n\n## **Parallel Chains**\n\n- Let's now consider a smaller simulation, where $n=50$ (recall, overall $n=100$, but half is for burn-in).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbb_sim_short <- stan(model_code = bb_model, data = list(Y = 9), \n                     chains = 4, iter = 50*2, seed = 84735)\n```\n:::\n\n\n\n\n## **Parallel Chains** \n\n- Let's now consider a smaller simulation, where $n=50$ (recall, overall $n=100$, but half is for burn-in).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbb_sim_short <- stan(model_code = bb_model, data = list(Y = 9), \n                     chains = 4, iter = 50*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: WARNING: There aren't enough warmup iterations to fit the\nChain 1:          three stages of adaptation as currently configured.\nChain 1:          Reducing each adaptation stage to 15%/75%/10% of\nChain 1:          the given number of warmup iterations:\nChain 1:            init_buffer = 7\nChain 1:            adapt_window = 38\nChain 1:            term_buffer = 5\nChain 1: \nChain 1: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 1: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 1: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 1: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 1: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 1: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 1: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 1: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 1: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 1: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 1: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 1: Iteration: 100 / 100 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0 seconds (Warm-up)\nChain 1:                0 seconds (Sampling)\nChain 1:                0 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: WARNING: There aren't enough warmup iterations to fit the\nChain 2:          three stages of adaptation as currently configured.\nChain 2:          Reducing each adaptation stage to 15%/75%/10% of\nChain 2:          the given number of warmup iterations:\nChain 2:            init_buffer = 7\nChain 2:            adapt_window = 38\nChain 2:            term_buffer = 5\nChain 2: \nChain 2: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 2: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 2: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 2: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 2: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 2: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 2: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 2: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 2: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 2: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 2: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 2: Iteration: 100 / 100 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0 seconds (Warm-up)\nChain 2:                0 seconds (Sampling)\nChain 2:                0 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: WARNING: There aren't enough warmup iterations to fit the\nChain 3:          three stages of adaptation as currently configured.\nChain 3:          Reducing each adaptation stage to 15%/75%/10% of\nChain 3:          the given number of warmup iterations:\nChain 3:            init_buffer = 7\nChain 3:            adapt_window = 38\nChain 3:            term_buffer = 5\nChain 3: \nChain 3: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 3: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 3: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 3: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 3: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 3: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 3: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 3: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 3: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 3: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 3: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 3: Iteration: 100 / 100 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0 seconds (Warm-up)\nChain 3:                0 seconds (Sampling)\nChain 3:                0 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: WARNING: There aren't enough warmup iterations to fit the\nChain 4:          three stages of adaptation as currently configured.\nChain 4:          Reducing each adaptation stage to 15%/75%/10% of\nChain 4:          the given number of warmup iterations:\nChain 4:            init_buffer = 7\nChain 4:            adapt_window = 38\nChain 4:            term_buffer = 5\nChain 4: \nChain 4: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 4: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 4: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 4: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 4: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 4: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 4: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 4: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 4: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 4: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 4: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 4: Iteration: 100 / 100 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0 seconds (Warm-up)\nChain 4:                0 seconds (Sampling)\nChain 4:                0 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n## **Parallel Chains**\n\n- Let's now consider a smaller simulation, where $n=50$ (recall, overall $n=100$, but half is for burn-in).\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W11-L1-approx-posterior_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n\n## **Parallel Chains** \n\n- Now you try 10,000 iterations.\n\n## **Parallel Chains** \n\n- Now you try 10,000 iterations.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W11-L1-approx-posterior_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Effective Sample Size**  \n\n- The more a dependent Markov chain behaves like an independent sample, the smaller the error in the resulting posterior approximation. \n    - Plots are great, but numerical assessment can provide more nuanced information.\n\n- **Effective sample size** ($N_{\\text{eff}}$): the number of independent sample values it would take to produce an equivalently accurate posterior approximation.\n\n- **Effective sample size ratio**: \n\n$$\\frac{N_{\\text{eff}}}{N}$$\n\n- Generally, we look for the effective sample size, $N_{\\text{eff}}$, to be greater than 10% of the actual sample size, $N$. \n\n## **Effective Sample Size**  \n\n- We will use the `neff_ratio()` function to find this ratio.\n\n- In our example data,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the effective sample size ratio - N = 50\nneff_ratio(bb_sim, pars = c(\"pi\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3462257\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the effective sample size ratio - N = 10000\nneff_ratio(bb_sim_short, pars = c(\"pi\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4950171\n```\n\n\n:::\n:::\n\n\n\n- Because the $N_{\\text{eff}}$ is over 10%, we are not concerned and can proceed.\n\n## **Autocorrelation**  \n\n- Autocorrelation allows us to evaluate if our Markov chain sufficiently mimics the behavior of an independent sample. \n\n- **Autocorrelation**:\n    - *Lag 1* autocorrelation measures the correlation between pairs of Markov chain values that are one \"step\" apart (e.g.,  $\\pi_i$ and $\\pi_{(i-1)}$; e.g., $\\pi_4$ and $\\pi_3$).\n    - *Lag 2* autocorrelation measures the correlation between pairs of Markov chain values that are two \"steps apart (e.g.,  $\\pi_i$ and $\\pi_{(i-2)}$; e.g., $\\pi_4$ and $\\pi_2$).\n    - *Lag $k$* autocorrelation measures the correlation between pairs of Markov chain values that are $k$ \"steps\" apart (e.g.,  $\\pi_i$ and $\\pi_{(i-k)}$; e.g., $\\pi_4$ and $\\pi_{(4-k)}$).\n\n- Strong autocorrelation or dependence is a bad thing.\n    - It goes hand in hand with small effective sample size ratios.\n    - These provide a warning sign that our resulting posterior approximations might be unreliable. \n    \n## **Autocorrelation** \n\n<center>\n<img src = \"images/W12-L1-d.png\" width = 1200>\n</center>\n\n- No obvious patterns in the trace plot; dependence is relatively weak.\n\n- Autocorrelation plot quickly drops off and is effectively 0 by lag 5.\n\n- Confirmation that our Markov chain is mixing quickly.\n    - i.e., quickly moving around the range of posterior plausible $\\pi$ values\n    - i.e., at least mimicking an independent sample.\n\n## **Autocorrelation** \n\n<center>\n<img src = \"images/W12-L1-e.png\" width = 1200>\n</center>\n\n- This is an \"unhealthy\" Markov chain.\n\n- Trace plot shows strong trends $\\to$ autocorrelation in the Markov chain values.\n\n- Slow decrease in autocorrelation plot indicates that the dependence between chain values does not quickly fade away.\n\n    - At lag 20, the autocorrelation is still $\\sim$ 90%.\n\n## **Fast vs. Slow Mixing Markov Chains** \n\n- **Fast mixing** chains: \n    - The chains move \"quickly\" around the range of posterior plausible values\n    - The autocorrelation among the chain values drops off quickly.\n    - The effective sample size ratio is reasonably large.\n    \n- **Slow mixing** chains:\n    - The chains move \"slowly\" around the range of posterior plauslbe values.\n    - The autocorrelation among the chainv alues drops off very slowly.\n    - The effective sample size ratio is small.\n    \n- What do we do if we have a slow mixing chain?\n    - Increase the chain size :)\n    - Thin the Markov chain :|\n\n## **Thinning Markov Chains**\n\n- Let's thin our original results, `bb_sim`, to every tenth value using the `thin` argument in `stan()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthinned_sim <- stan(model_code = bb_model, data = list(Y = 9), \n                    chains = 4, iter = 5000*2, seed = 84735, thin = 10)\n\nmcmc_trace(thinned_sim, pars = \"pi\")\nmcmc_acf(thinned_sim, pars = \"pi\")\n```\n:::\n\n\n\n## **Thinning Markov Chains**\n\n- Let's thin our original results, `bb_sim`, to every tenth value using the `thin` argument in `stan()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthinned_sim <- stan(model_code = bb_model, data = list(Y = 9), \n                    chains = 4, iter = 5000*2, seed = 84735, thin = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 1:                0.011 seconds (Sampling)\nChain 1:                0.02 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 2:                0.011 seconds (Sampling)\nChain 2:                0.02 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 3:                0.009 seconds (Sampling)\nChain 3:                0.019 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 4:                0.011 seconds (Sampling)\nChain 4:                0.02 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n## **Thinning Markov Chains**\n\n- Let's thin our original results, `bb_sim`, to every tenth value using the `thin` argument in `stan()`.\n\n<center>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](W11-L1-approx-posterior_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## **Thinning Markov Chains**\n\n- Warning!\n    - The benefits of reduced autocorrelation do not necessarily outweigh the loss of chain values.\n    - i.e., 5,000 Markov chain values with stronger autocorrelation may be a better posterior approximation than 500 chain values with weaker autocorrelation.\n    \n- The effectiveness depends on the algorithm used to construct the Markov chain.\n    - Folks advise against thinning unless you need memory space on your computer.\n\n## **$\\hat{R}$**\n\n$$\\hat{R} \\approx \\sqrt{\\frac{\\text{var}_{\\text{combined}}}{\\text{var}_{\\text{within}}}}$$\n\n- where\n    - $\\text{var}_{\\text{combined}}$ is the variability in $\\theta$ across all chains combined.\n    - $\\text{var}_{\\text{within}}$ is the typical variability within any individual chain.\n- $\\hat{R}$ compares the variability in sampled $\\theta$ values across all chains combined with the variability within each individual change.\n    - Ideally, $\\hat{R} \\approx 1$, showing stability across chains.\n    - $\\hat{R} > 1$ indicates instability with the variability in the combined chains larger than that of the variability within the chains.\n    - $\\hat{R} > 1.05$ raises red flags about the stability of the simulation.\n\n## **$\\hat{R}$**\n\n- We can use the `rhat()` function from the `bayesplot` package to find $\\hat{R}$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrhat(bb_sim, pars = \"pi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.000245\n```\n\n\n:::\n:::\n\n\n\n- We can see that our simulation is stable.\n\n- If we were to find $\\hat{R}$ for the other (obviously bad) simulation, it would be 5.35 😱\n\n## **Homework**\n\n- 6.5\n\n- 6.6\n\n- 6.7\n\n- 6.13\n\n- 6.14\n\n- 6.15\n\n- 6.17\n\n\n\n\n\n\n\n",
    "supporting": [
      "W11-L1-approx-posterior_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}